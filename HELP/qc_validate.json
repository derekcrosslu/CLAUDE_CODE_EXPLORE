{
  "tool": "qc_validate",
  "version": "2.3.2",
  "description": "QuantConnect walk-forward validation and Monte Carlo robustness testing for Phase 5",
  "sections": [
    {
      "id": "cli_commands",
      "title": "CLI Commands Reference",
      "content": "**Top-Level Commands**\n\nAll commands use: `venv/bin/python SCRIPTS/qc_validate.py <command> [options]`\n\n---\n\n## run - Execute Walk-Forward Validation\n\n```bash\nqc_validate run --strategy <file> [options]\n```\n\n**Required**:\n- `--strategy PATH`: Strategy file to validate (required)\n\n**Optional**:\n- `--state PATH`: iteration_state.json path (default: `iteration_state.json`)\n- `--output PATH`: Output file path (default: `PROJECT_LOGS/validation_result.json`)\n- `--split FLOAT`: Train/test split ratio (default: `0.80` for 80/20 split)\n  - `0.80`: 80% in-sample, 20% out-of-sample (standard)\n  - `0.70`: 70% in-sample, 30% out-of-sample (conservative)\n  - `0.60`: 60% in-sample, 40% out-of-sample (very conservative)\n\n**Examples**:\n```bash\n# Run validation (standard 80/20 split)\nqc_validate run --strategy strategy.py\n\n# Conservative 70/30 split\nqc_validate run --strategy strategy.py --split 0.70\n\n# Custom output location\nqc_validate run --strategy strategy.py --output results/validation_20251113.json\n\n# Full specification\nqc_validate run --strategy strategy.py --state iteration_state.json --split 0.80 --output validation.json\n```\n\n**How It Works**:\n1. Reads strategy code from `--strategy` file\n2. Extracts `SetStartDate()` and `SetEndDate()` from strategy\n3. Calculates split point based on `--split` ratio\n4. Modifies strategy for in-sample period (80% of data)\n5. Uploads to existing QC project (from iteration_state.json)\n6. Runs in-sample backtest\n7. Modifies strategy for out-of-sample period (20% of data)\n8. Runs out-of-sample backtest\n9. Calculates degradation and robustness metrics\n10. Makes automatic decision: DEPLOY, CAUTION, ABANDON, or ESCALATE\n\n**Output** (`PROJECT_LOGS/validation_result.json`):\n```json\n{\n  \"in_sample\": {\n    \"sharpe_ratio\": 0.95,\n    \"max_drawdown\": 0.18,\n    \"total_return\": 0.42,\n    \"total_trades\": 125,\n    \"win_rate\": 0.48\n  },\n  \"out_of_sample\": {\n    \"sharpe_ratio\": 0.88,\n    \"max_drawdown\": 0.21,\n    \"total_return\": 0.19,\n    \"total_trades\": 34,\n    \"win_rate\": 0.46\n  },\n  \"degradation_pct\": 0.074,\n  \"robustness_score\": 0.93,\n  \"split_ratio\": 0.80,\n  \"in_sample_period\": \"2019-2022\",\n  \"out_of_sample_period\": \"2023-2023\",\n  \"timestamp\": \"2025-11-13T15:00:00Z\"\n}\n```\n\n**Console Output**:\n```\nüî¨ QuantConnect Walk-Forward Validation\n============================================================\n\nüìä Data Split (80/20):\n   Full Period: 2019-01-01 to 2023-12-31\n   In-Sample (Training): 2019-01-01 to 2022-12-31\n   Out-of-Sample (Testing): 2023-01-01 to 2023-12-31\n\nüîÑ Running in-sample backtest...\n   Backtest ID: abc123\n   ‚úì In-sample Sharpe: 0.950\n\nüîÑ Running out-of-sample backtest...\n   Backtest ID: xyz789\n   ‚úì Out-of-sample Sharpe: 0.880\n\n============================================================\n‚úÖ VALIDATION COMPLETE\n============================================================\n\nüìä Results:\n   In-Sample Sharpe: 0.950\n   Out-of-Sample Sharpe: 0.880\n   Degradation: 7.4%\n   Robustness Score: 0.93\n\n‚úÖ Excellent robustness (<15% degradation)\n\nüíæ Results saved to: PROJECT_LOGS/validation_result.json\n```\n\n**Return Codes**:\n- `0`: Success\n- `1`: Error (missing prerequisites, API failure, timeout, insufficient trades, etc.)\n\n**Runtime**: 10-20 minutes typical (runs 2 backtests sequentially)\n\n---\n\n## analyze - Analyze Validation Results\n\n```bash\nqc_validate analyze --results <file>\n```\n\n**Required**:\n- `--results PATH`: Validation results JSON file (required)\n\n**Examples**:\n```bash\n# Analyze validation results\nqc_validate analyze --results PROJECT_LOGS/validation_result.json\n\n# Analyze custom results file\nqc_validate analyze --results my_validation.json\n```\n\n**Purpose**: Detailed analysis of validation results without re-running backtests\n\n**Output**:\n```\nüìä Validation Analysis\n============================================================\n\nIn-Sample Performance:\n  Sharpe Ratio: 0.950\n  Max Drawdown: 18.0%\n  Total Return: 42.0%\n  Total Trades: 125\n  Win Rate: 48.0%\n\nOut-of-Sample Performance:\n  Sharpe Ratio: 0.880\n  Max Drawdown: 21.0%\n  Total Return: 19.0%\n  Total Trades: 34\n  Win Rate: 46.0%\n\nRobustness Metrics:\n  Degradation: 7.4%\n  Robustness Score: 0.93 (maintains 93% of IS performance)\n  Trade Rate Change: +6.7% (consistent)\n\n‚úÖ Decision: DEPLOY_STRATEGY\n\nReasoning:\n  ‚úÖ Degradation 7.4% < 15% threshold (excellent)\n  ‚úÖ Robustness 0.93 > 0.75 threshold (high)\n  ‚úÖ OOS Sharpe 0.88 > 0.7 threshold (good)\n  ‚úÖ OOS Trades 34 > 10 threshold (sufficient)\n\nNext Steps:\n  - Paper trade for 6-12 months\n  - Initial position size: 50%\n  - Monitor closely first 3 months\n```\n\n**Use Case**: Review validation results from previous run without re-running expensive backtests\n\n---\n\n## Complete Workflow Example\n\n```bash\n# 1. Run walk-forward validation (80/20 split)\nqc_validate run --strategy strategy.py\n# Output: Runs IS and OOS backtests, saves to validation_result.json\n# Runtime: 10-20 minutes\n\n# 2. Analyze results in detail\nqc_validate analyze --results PROJECT_LOGS/validation_result.json\n# Output: Detailed analysis, decision, next steps\n\n# 3. If needed, try different split ratio\nqc_validate run --strategy strategy.py --split 0.70\n# Output: More conservative 70/30 split\n```",
      "tags": [
        "cli",
        "commands",
        "reference"
      ],
      "priority": 1,
      "related_sections": [
        "api_methods",
        "workflow_overview"
      ]
    },
    {
      "id": "api_methods",
      "title": "API Methods Reference (qc_api.py)",
      "content": "**Low-Level API Methods Used by qc_validate.py**\n\nThese are the `qc_api.py` methods that `qc_validate.py` calls under the hood.\n\n---\n\n## read_files()\n\n**Purpose**: Read files from QC project (e.g., current strategy code)\n\n**Signature**:\n```python\napi.read_files(\n    project_id: int,\n    filename: str = None\n) -> dict\n```\n\n**Parameters**:\n- `project_id`: QC project ID (from iteration_state.json)\n- `filename`: Specific file to read (optional, if None returns all files)\n\n**Returns**:\n```python\n{\n    'success': True,\n    'files': [\n        {\n            'name': 'main.py',\n            'content': '# Strategy code here...'\n        }\n    ]\n}\n```\n\n**Example**:\n```python\nfrom qc_api import QuantConnectAPI\nimport json\n\napi = QuantConnectAPI()\n\n# Read project_id from iteration_state.json\nwith open('iteration_state.json') as f:\n    state = json.load(f)\nproject_id = state['project']['project_id']\n\n# Read strategy file\nresult = api.read_files(project_id, \"main.py\")\nif result['success']:\n    strategy_code = result['files'][0]['content']\n    print(f\"Strategy loaded ({len(strategy_code)} chars)\")\n```\n\n**HTTP Endpoint**: `GET /files/read`\n\n---\n\n## upload_file()\n\n**Purpose**: Upload or update file in QC project (handles create/update automatically)\n\n**Signature**:\n```python\napi.upload_file(\n    project_id: int,\n    name: str,\n    content: str\n) -> dict\n```\n\n**Parameters**:\n- `project_id`: QC project ID (from iteration_state.json)\n- `name`: File name (e.g., `\"main.py\"`)\n- `content`: File content (modified strategy code)\n\n**Returns**:\n```python\n{\n    'success': True,\n    'files': [\n        {'name': 'main.py', 'content': '...'}\n    ]\n}\n```\n\n**Example**:\n```python\n# Modify strategy for in-sample period\nimport re\n\nis_code = re.sub(\n    r'SetEndDate\\((\\d+),\\s*(\\d+),\\s*(\\d+)\\)',\n    'SetEndDate(2022, 12, 31)',  # End IS at 2022\n    strategy_code\n)\n\n# Upload modified strategy\nresult = api.upload_file(\n    project_id=project_id,\n    name=\"main.py\",\n    content=is_code\n)\n\nif result['success']:\n    print(\"IS strategy uploaded\")\n```\n\n**HTTP Endpoint**: `POST /files/update` or `POST /files/create` (automatic)\n\n---\n\n## create_backtest()\n\n**Purpose**: Submit backtest job to QuantConnect\n\n**Signature**:\n```python\napi.create_backtest(\n    project_id: int,\n    compile_id: str = None,\n    name: str = None\n) -> dict\n```\n\n**Parameters**:\n- `project_id`: QC project ID\n- `compile_id`: Compile ID (optional, will compile if not provided)\n- `name`: Backtest name (optional, auto-generated if not provided)\n\n**Returns**:\n```python\n{\n    'success': True,\n    'backtests': [\n        {\n            'backtestId': 'abc123def456',\n            'name': 'InSample_Validation',\n            'status': 'Running'\n        }\n    ]\n}\n```\n\n**Example**:\n```python\n# Create in-sample backtest\nresult = api.create_backtest(\n    project_id=project_id,\n    name=\"Validation_IS_2019-2022\"\n)\n\nif result['success']:\n    backtest_id_is = result['backtests'][0]['backtestId']\n    print(f\"IS backtest started: {backtest_id_is}\")\n```\n\n**HTTP Endpoint**: `POST /backtests/create`\n\n---\n\n## wait_for_backtest()\n\n**Purpose**: Poll backtest status until completion or timeout\n\n**Signature**:\n```python\napi.wait_for_backtest(\n    project_id: int,\n    backtest_id: str,\n    timeout: int = 300,\n    poll_interval: int = 5\n) -> dict\n```\n\n**Parameters**:\n- `project_id`: QC project ID\n- `backtest_id`: Backtest ID from `create_backtest()`\n- `timeout`: Maximum wait time in seconds (default: 300 = 5 min)\n- `poll_interval`: Seconds between status checks (default: 5)\n\n**Returns**:\n```python\n{\n    'success': True,\n    'backtest': {\n        'backtestId': 'abc123def456',\n        'status': 'Completed',\n        'result': { /* full backtest results */ },\n        'statistics': { /* performance metrics */ }\n    }\n}\n```\n\n**Behavior**:\n- Polls every `poll_interval` seconds\n- Returns when status == `'Completed'`\n- Raises timeout error if exceeds `timeout`\n- Displays progress updates to console\n\n**Example**:\n```python\n# Wait for IS backtest to complete\nresult_is = api.wait_for_backtest(\n    project_id=project_id,\n    backtest_id=backtest_id_is,\n    timeout=600  # 10 minutes\n)\n\nif result_is['success']:\n    print(\"IS backtest completed\")\nelse:\n    print(f\"IS backtest failed: {result_is.get('error')}\")\n```\n\n**HTTP Endpoint**: `GET /backtests/read` (called repeatedly)\n\n---\n\n## read_backtest()\n\n**Purpose**: Get backtest results (single check, non-blocking)\n\n**Signature**:\n```python\napi.read_backtest(\n    project_id: int,\n    backtest_id: str\n) -> dict\n```\n\n**Parameters**:\n- `project_id`: QC project ID\n- `backtest_id`: Backtest ID\n\n**Returns**:\n```python\n{\n    'success': True,\n    'backtest': {\n        'backtestId': 'abc123def456',\n        'status': 'Completed',\n        'result': { /* full results */ },\n        'statistics': {\n            'sharpeRatio': 0.88,\n            'drawdown': 0.21,\n            'totalNetProfit': 0.19,\n            /* ... more metrics */\n        }\n    }\n}\n```\n\n**Example**:\n```python\n# Read backtest results\nresult = api.read_backtest(project_id, backtest_id_is)\nif result['success']:\n    stats = result['backtest']['statistics']\n    sharpe = stats.get('sharpeRatio', 0)\n    print(f\"Sharpe: {sharpe}\")\n```\n\n**HTTP Endpoint**: `GET /backtests/read`\n\n---\n\n## parse_backtest_results()\n\n**Purpose**: Extract key metrics from backtest data\n\n**Signature**:\n```python\napi.parse_backtest_results(\n    backtest_data: dict\n) -> dict\n```\n\n**Parameters**:\n- `backtest_data`: Full backtest result from `read_backtest()` or `wait_for_backtest()`\n\n**Returns**:\n```python\n{\n    'sharpe_ratio': 0.88,\n    'max_drawdown': 0.21,\n    'total_return': 0.19,\n    'total_trades': 34,\n    'win_rate': 0.46,\n    'avg_win': 0.025,\n    'avg_loss': -0.018,\n    'profit_factor': 1.35\n}\n```\n\n**Example**:\n```python\n# Parse IS results\nperf_is = api.parse_backtest_results(result_is)\nprint(f\"IS Sharpe: {perf_is['sharpe_ratio']:.3f}\")\nprint(f\"IS Trades: {perf_is['total_trades']}\")\n\n# Parse OOS results\nperf_oos = api.parse_backtest_results(result_oos)\nprint(f\"OOS Sharpe: {perf_oos['sharpe_ratio']:.3f}\")\nprint(f\"OOS Trades: {perf_oos['total_trades']}\")\n```\n\n**Note**: Pure Python function, no HTTP call\n\n---\n\n## Complete API Workflow\n\n```python\nfrom qc_api import QuantConnectAPI\nimport json\nimport re\n\n# Initialize API\napi = QuantConnectAPI()\n\n# Step 1: Read project_id from iteration_state.json\nwith open('iteration_state.json') as f:\n    state = json.load(f)\nproject_id = state['project']['project_id']\n\n# Step 2: Read strategy from project\nfiles = api.read_files(project_id, \"main.py\")\nstrategy_code = files['files'][0]['content']\n\n# Step 3: Modify for in-sample (2019-2022)\nis_code = re.sub(\n    r'SetEndDate\\((\\d+),\\s*(\\d+),\\s*(\\d+)\\)',\n    'SetEndDate(2022, 12, 31)',\n    strategy_code\n)\n\n# Step 4: Upload IS strategy\napi.upload_file(project_id, \"main.py\", is_code)\n\n# Step 5: Run IS backtest\nbacktest_is = api.create_backtest(project_id, name=\"Validation_IS\")\nbacktest_id_is = backtest_is['backtests'][0]['backtestId']\n\n# Step 6: Wait for IS completion\nresult_is = api.wait_for_backtest(project_id, backtest_id_is, timeout=600)\nperf_is = api.parse_backtest_results(result_is)\nprint(f\"IS Sharpe: {perf_is['sharpe_ratio']:.3f}\")\n\n# Step 7: Modify for out-of-sample (2023)\noos_code = re.sub(\n    r'SetStartDate\\((\\d+),\\s*(\\d+),\\s*(\\d+)\\)',\n    'SetStartDate(2023, 1, 1)',\n    strategy_code\n)\n\n# Step 8: Upload OOS strategy\napi.upload_file(project_id, \"main.py\", oos_code)\n\n# Step 9: Run OOS backtest\nbacktest_oos = api.create_backtest(project_id, name=\"Validation_OOS\")\nbacktest_id_oos = backtest_oos['backtests'][0]['backtestId']\n\n# Step 10: Wait for OOS completion\nresult_oos = api.wait_for_backtest(project_id, backtest_id_oos, timeout=600)\nperf_oos = api.parse_backtest_results(result_oos)\nprint(f\"OOS Sharpe: {perf_oos['sharpe_ratio']:.3f}\")\n\n# Step 11: Calculate metrics\nsharpe_is = perf_is['sharpe_ratio']\nsharpe_oos = perf_oos['sharpe_ratio']\n\ndegradation = (sharpe_is - sharpe_oos) / sharpe_is if sharpe_is > 0 else 1.0\nrobustness = sharpe_oos / sharpe_is if sharpe_is > 0 else 0.0\n\nprint(f\"\\nDegradation: {degradation*100:.1f}%\")\nprint(f\"Robustness: {robustness:.3f}\")\n\n# Step 12: Make decision\nif degradation < 0.15 and robustness > 0.75 and sharpe_oos > 0.7:\n    decision = \"DEPLOY_STRATEGY\"\nelif degradation < 0.30 and robustness > 0.60 and sharpe_oos > 0.5:\n    decision = \"PROCEED_WITH_CAUTION\"\nelse:\n    decision = \"ABANDON_HYPOTHESIS\"\n\nprint(f\"Decision: {decision}\")\n```",
      "tags": [
        "api",
        "methods",
        "qc_api"
      ],
      "priority": 1,
      "related_sections": [
        "cli_commands",
        "executing_validation"
      ]
    },
    {
      "id": "workflow_overview",
      "title": "Phase 5 Workflow Overview",
      "content": "**Prerequisites**:\n- `/qc-init` completed (iteration_state.json exists)\n- `/qc-backtest` completed (project created, baseline results exist)\n- `/qc-optimize` completed OR skipped (strategy parameters final)\n\n**Validation Workflow**:\n\n1. **Read iteration_state.json** - Get project_id and final parameters\n2. **Define time splits** - 80/20 in-sample vs out-of-sample\n3. **Run in-sample backtest** - Train period (e.g., 2019-2022)\n4. **Run out-of-sample backtest** - Test period (e.g., 2023)\n5. **Calculate degradation** - Compare IS vs OOS performance\n6. **Route decision** - DEPLOY, PROCEED_WITH_CAUTION, or ABANDON\n\n**CRITICAL RULE**: MUST use project_id from iteration_state.json (created by /qc-backtest). NEVER create new project in this phase.\n\n**Why Project ID Matters**:\n- Maintains workflow continuity\n- Validates correct strategy version\n- Preserves audit trail\n- Uses same project across all phases\n\n**Workflow Location**: Phase 5 (final validation before deployment)",
      "tags": [
        "workflow",
        "overview",
        "phase5"
      ],
      "priority": 1,
      "related_sections": [
        "qc_platform_access",
        "executing_validation"
      ]
    },
    {
      "id": "qc_platform_access",
      "title": "QuantConnect Platform Access",
      "content": "**Authentication Setup**\n\n1. Get API credentials from QuantConnect:\n   - Visit: https://www.quantconnect.com/account\n   - Copy User ID and API Token\n\n2. Create `.env` file in project root:\n```bash\nQUANTCONNECT_USER_ID=your_user_id_here\nQUANTCONNECT_API_TOKEN=your_api_token_here\n```\n\n3. Verify authentication:\n```python\nfrom qc_api import QuantConnectAPI\n\napi = QuantConnectAPI()\nprojects = api.list_projects()\nprint(f\"Found {len(projects.get('projects', []))} projects\")\n```\n\n**Using Existing Project from iteration_state.json**\n\n‚ö†Ô∏è **CRITICAL: NEVER create new project in validation phase!**\n\n```python\nimport json\nfrom qc_api import QuantConnectAPI\n\napi = QuantConnectAPI()\n\n# ‚úÖ CORRECT: Read project_id from iteration_state.json\nwith open('iteration_state.json') as f:\n    state = json.load(f)\n\nproject_id = state['project']['project_id']\n\n# Validate project exists (created by /qc-backtest)\nif not project_id:\n    raise ValueError(\"No project_id found. Run /qc-backtest first.\")\n\nprint(f\"Using existing project: {project_id}\")\n\n# Get final parameters (from optimization or baseline)\nif state.get('optimization', {}).get('status') == 'completed':\n    params = state['optimization']['best_parameters']\n    print(f\"Using optimized parameters: {params}\")\nelse:\n    print(\"Using baseline parameters (no optimization)\")\n```\n\n**Reading Strategy from Project**\n\n```python\n# Read current strategy code\nfiles = api.read_files(project_id, \"main.py\")\nif files['success'] and files['files']:\n    strategy_code = files['files'][0]['content']\n    print(f\"Strategy loaded ({len(strategy_code)} chars)\")\nelse:\n    raise ValueError(\"Strategy file not found in project\")\n```\n\n**Uploading Modified Strategy for Validation**\n\n```python\n# Modify strategy for time splits\nmodified_code = modify_for_validation(\n    strategy_code,\n    is_start=\"2019-01-01\",\n    is_end=\"2022-12-31\"\n)\n\n# Upload to existing project\nresult = api.upload_file(project_id, \"main.py\", modified_code)\nif result['success']:\n    print(\"Validation strategy uploaded\")\n```\n\n**Common Errors**:\n- \"Authentication failed\": Check .env credentials\n- \"No project_id in iteration_state.json\": Run /qc-backtest first\n- \"Strategy file not found\": Ensure main.py exists in project\n- \"Project not found\": Verify project_id is correct",
      "tags": [
        "authentication",
        "platform",
        "setup"
      ],
      "priority": 1,
      "related_sections": [
        "workflow_overview",
        "executing_validation"
      ]
    },
    {
      "id": "executing_validation",
      "title": "Executing Walk-Forward Validation",
      "content": "**Step 1: Load Project and Parameters**\n\n```python\nimport json\nfrom qc_api import QuantConnectAPI\n\napi = QuantConnectAPI()\n\n# Load iteration state (has project_id from /qc-backtest)\nwith open('iteration_state.json') as f:\n    state = json.load(f)\n\nproject_id = state['project']['project_id']\n\nif not project_id:\n    raise ValueError(\"Run /qc-backtest first to create project\")\n\nprint(f\"Project ID: {project_id}\")\n\n# Get final parameters\nif state.get('optimization', {}).get('status') == 'completed':\n    params = state['optimization']['best_parameters']\n    baseline_sharpe = state['optimization']['best_sharpe']\n    print(f\"Using optimized parameters: {params}\")\nelse:\n    baseline_sharpe = state['backtest_results']['performance']['sharpe_ratio']\n    print(\"Using baseline parameters\")\n\nprint(f\"Baseline Sharpe: {baseline_sharpe:.3f}\")\n```\n\n**Step 2: Read Strategy from Project**\n\n```python\n# Read current strategy from existing project\nfiles = api.read_files(project_id, \"main.py\")\noriginal_code = files['files'][0]['content']\n\nprint(f\"Strategy loaded ({len(original_code)} characters)\")\n```\n\n**Step 3: Run In-Sample Backtest**\n\n```python\n# Modify strategy for IS period (80% of data)\nis_code = modify_dates(\n    original_code,\n    start_date=\"2019-01-01\",\n    end_date=\"2022-12-31\"  # 4 years = 80%\n)\n\n# Upload IS version to existing project\napi.upload_file(project_id, \"main.py\", is_code)\n\n# Submit IS backtest\nresult = api.create_backtest(\n    project_id,\n    name=\"Validation_IS_2019-2022\"\n)\nbacktest_id_is = result['backtests'][0]['backtestId']\n\n# Wait for completion\nfinal_is = api.wait_for_backtest(\n    project_id,\n    backtest_id_is,\n    timeout=600\n)\n\n# Extract IS metrics\nis_stats = final_is['backtest']['statistics']\nis_sharpe = is_stats.get('sharpeRatio', 0)\nis_drawdown = is_stats.get('drawdown', 0)\nis_trades = is_stats.get('totalOrders', 0)\n\nprint(f\"IS Sharpe: {is_sharpe:.3f}\")\nprint(f\"IS Drawdown: {is_drawdown:.1%}\")\nprint(f\"IS Trades: {is_trades}\")\n```\n\n**Step 4: Run Out-of-Sample Backtest**\n\n```python\n# Modify strategy for OOS period (20% of data)\noos_code = modify_dates(\n    original_code,\n    start_date=\"2023-01-01\",\n    end_date=\"2023-12-31\"  # 1 year = 20%\n)\n\n# Upload OOS version to existing project\napi.upload_file(project_id, \"main.py\", oos_code)\n\n# Submit OOS backtest\nresult = api.create_backtest(\n    project_id,\n    name=\"Validation_OOS_2023\"\n)\nbacktest_id_oos = result['backtests'][0]['backtestId']\n\n# Wait for completion\nfinal_oos = api.wait_for_backtest(\n    project_id,\n    backtest_id_oos,\n    timeout=600\n)\n\n# Extract OOS metrics\noos_stats = final_oos['backtest']['statistics']\noos_sharpe = oos_stats.get('sharpeRatio', 0)\noos_drawdown = oos_stats.get('drawdown', 0)\noos_trades = oos_stats.get('totalOrders', 0)\n\nprint(f\"OOS Sharpe: {oos_sharpe:.3f}\")\nprint(f\"OOS Drawdown: {oos_drawdown:.1%}\")\nprint(f\"OOS Trades: {oos_trades}\")\n```\n\n**Step 5: Calculate Metrics and Make Decision**\n\n```python\n# Calculate degradation\ndegradation = (is_sharpe - oos_sharpe) / is_sharpe if is_sharpe > 0 else 1.0\nrobustness = oos_sharpe / is_sharpe if is_sharpe > 0 else 0.0\n\nprint(f\"\\nValidation Results:\")\nprint(f\"  Degradation: {degradation*100:.1f}%\")\nprint(f\"  Robustness: {robustness:.3f}\")\n\n# Automatic decision thresholds\nif degradation < 0.15 and robustness > 0.75 and oos_sharpe > 0.7:\n    decision = \"DEPLOY_STRATEGY\"\n    reason = \"Minimal degradation, high robustness\"\nelif degradation < 0.30 and robustness > 0.60 and oos_sharpe > 0.5:\n    decision = \"PROCEED_WITH_CAUTION\"\n    reason = \"Acceptable degradation, deploy with monitoring\"\nelif degradation > 0.40 or robustness < 0.5 or oos_sharpe < 0:\n    decision = \"ABANDON_HYPOTHESIS\"\n    reason = \"Severe degradation or negative OOS performance\"\nelse:\n    decision = \"ESCALATE_TO_HUMAN\"\n    reason = \"Borderline results, human review needed\"\n\nprint(f\"\\nDecision: {decision}\")\nprint(f\"Reason: {reason}\")\n\n# Save results\nresults = {\n    'in_sample': {\n        'sharpe': is_sharpe,\n        'drawdown': is_drawdown,\n        'trades': is_trades,\n        'backtest_id': backtest_id_is\n    },\n    'out_of_sample': {\n        'sharpe': oos_sharpe,\n        'drawdown': oos_drawdown,\n        'trades': oos_trades,\n        'backtest_id': backtest_id_oos\n    },\n    'degradation': degradation,\n    'robustness': robustness,\n    'decision': decision,\n    'reason': reason\n}\n\nwith open('PROJECT_LOGS/validation_result.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\nResults saved to PROJECT_LOGS/validation_result.json\")\n```\n\n**Via CLI** (recommended - handles all above automatically):\n\n```bash\n# Run walk-forward validation\nvenv/bin/python SCRIPTS/qc_validate.py run --strategy strategy.py\n\n# Custom split ratio (default 80/20)\nvenv/bin/python SCRIPTS/qc_validate.py run --strategy strategy.py --split 0.70\n\n# Analyze results\nvenv/bin/python SCRIPTS/qc_validate.py analyze --results PROJECT_LOGS/validation_result.json\n```",
      "tags": [
        "execution",
        "validation",
        "walk-forward"
      ],
      "priority": 1,
      "related_sections": [
        "qc_platform_access",
        "walk_forward_basics"
      ]
    },
    {
      "id": "monte_carlo_execution",
      "title": "Monte Carlo Validation via Research Notebook",
      "content": "**Monte Carlo Purpose**: Additional robustness testing using trade randomization.\n\n**When to Use**:\n- After walk-forward validation passes\n- For institutional-grade validation\n- To calculate PSR, DSR, MinTRL metrics\n\n**Uploading Research Notebook to Existing Project**\n\n‚ö†Ô∏è **Use existing project from iteration_state.json, NOT new project**\n\n```python\nimport json\nfrom qc_api import QuantConnectAPI\n\napi = QuantConnectAPI()\n\n# ‚úÖ CORRECT: Use existing project from iteration_state.json\nwith open('iteration_state.json') as f:\n    state = json.load(f)\n\nproject_id = state['project']['project_id']\n\nif not project_id:\n    raise ValueError(\"Run /qc-backtest first to create project\")\n\n# Upload MC validation notebook to existing project\nwith open('mc_validation.ipynb') as f:\n    notebook_content = f.read()\n\nresult = api.upload_file(project_id, \"research.ipynb\", notebook_content)\nif result['success']:\n    print(f\"MC notebook uploaded to project {project_id}\")\n```\n\n**Monte Carlo Notebook Structure**\n\n```python\n# Cell 1: Import QuantBook\nfrom QuantConnect import *\nfrom QuantConnect.Data import *\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nqb = QuantBook()\n\n# Cell 2: Load backtest results\n# Get backtest_id from iteration_state.json\nproject_id = 12345  # From iteration_state.json\nbacktest_id = \"abc123\"  # OOS backtest_id\n\n# Read backtest results\nresults = qb.ReadBacktest(project_id, backtest_id)\ntrades = pd.DataFrame(results['trades'])\n\nprint(f\"Loaded {len(trades)} trades\")\nprint(f\"Original Sharpe: {results['statistics']['sharpeRatio']:.3f}\")\n\n# Cell 3: Trade Sequence Randomization (1000 runs)\nnp.random.seed(42)\nmc_results = []\n\nfor i in range(1000):\n    # Shuffle trade sequence\n    shuffled_trades = trades.sample(frac=1, replace=False)\n    \n    # Calculate cumulative returns\n    shuffled_trades['cumulative_pnl'] = shuffled_trades['profit_loss'].cumsum()\n    \n    # Calculate Sharpe ratio\n    returns = shuffled_trades['profit_loss']\n    sharpe = (returns.mean() / returns.std()) * np.sqrt(252) if returns.std() > 0 else 0\n    \n    # Calculate max drawdown\n    cumulative = shuffled_trades['cumulative_pnl']\n    running_max = cumulative.expanding().max()\n    drawdown = (cumulative - running_max).min()\n    \n    mc_results.append({\n        'run': i,\n        'sharpe': sharpe,\n        'max_dd': drawdown,\n        'total_pnl': cumulative.iloc[-1]\n    })\n\nmc_df = pd.DataFrame(mc_results)\n\n# Cell 4: Statistical Analysis\nprint(\"=\" * 60)\nprint(\"MONTE CARLO RESULTS (1000 runs)\")\nprint(\"=\" * 60)\nprint(f\"Median Sharpe: {mc_df['sharpe'].median():.3f}\")\nprint(f\"Mean Sharpe: {mc_df['sharpe'].mean():.3f}\")\nprint(f\"Std Dev: {mc_df['sharpe'].std():.3f}\")\nprint(f\"5th Percentile: {mc_df['sharpe'].quantile(0.05):.3f}\")\nprint(f\"95th Percentile: {mc_df['sharpe'].quantile(0.95):.3f}\")\nprint(f\"95th Percentile DD: ${mc_df['max_dd'].quantile(0.95):,.0f}\")\n\n# Cell 5: PSR Calculation\ndef calculate_psr(sharpe_ratios, benchmark=0.0):\n    \"\"\"Probabilistic Sharpe Ratio\"\"\"\n    n = len(sharpe_ratios)\n    sr_mean = np.mean(sharpe_ratios)\n    sr_std = np.std(sharpe_ratios)\n    \n    if sr_std == 0:\n        return 1.0 if sr_mean > benchmark else 0.0\n    \n    z_score = (sr_mean - benchmark) / (sr_std / np.sqrt(n))\n    psr = stats.norm.cdf(z_score)\n    return psr\n\npsr = calculate_psr(mc_df['sharpe'].values, benchmark=0.0)\nprint(f\"\\nPSR (vs 0.0): {psr:.3f}\")\n\nif psr > 0.95:\n    print(\"‚úÖ Very high confidence (PSR > 0.95)\")\nelif psr > 0.90:\n    print(\"‚úÖ High confidence (PSR > 0.90)\")\nelif psr > 0.75:\n    print(\"‚ö†Ô∏è Moderate confidence (PSR > 0.75)\")\nelse:\n    print(\"‚ùå Low confidence (PSR < 0.75)\")\n\n# Cell 6: Visualization\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Sharpe distribution\naxes[0,0].hist(mc_df['sharpe'], bins=50, edgecolor='black')\naxes[0,0].axvline(mc_df['sharpe'].median(), color='red', \n                   label=f\"Median: {mc_df['sharpe'].median():.3f}\")\naxes[0,0].set_title('Sharpe Ratio Distribution')\naxes[0,0].set_xlabel('Sharpe Ratio')\naxes[0,0].legend()\n\n# Drawdown distribution\naxes[0,1].hist(mc_df['max_dd'], bins=50, edgecolor='black')\naxes[0,1].axvline(mc_df['max_dd'].quantile(0.95), color='red',\n                   label=f\"95th: ${mc_df['max_dd'].quantile(0.95):,.0f}\")\naxes[0,1].set_title('Max Drawdown Distribution')\naxes[0,1].set_xlabel('Max Drawdown ($)')\naxes[0,1].legend()\n\n# PnL distribution\naxes[1,0].hist(mc_df['total_pnl'], bins=50, edgecolor='black')\naxes[1,0].set_title('Total PnL Distribution')\naxes[1,0].set_xlabel('Total PnL ($)')\n\n# Q-Q plot\nstats.probplot(mc_df['sharpe'], dist=\"norm\", plot=axes[1,1])\naxes[1,1].set_title('Q-Q Plot (Normality Check)')\n\nplt.tight_layout()\nplt.show()\n```\n\n**Accessing Research Notebook**:\n\n1. Visit: https://www.quantconnect.com/terminal\n2. Navigate to project (use project_id from iteration_state.json)\n3. Open research.ipynb\n4. Run all cells\n5. Export results and plots\n\n**Best Practices**:\n- Run MC AFTER walk-forward validation passes\n- Use 1000+ runs for statistical significance\n- Save results before deleting project\n- Don't use MC results to adjust strategy (contamination)",
      "tags": [
        "monte_carlo",
        "research",
        "validation"
      ],
      "priority": 2,
      "related_sections": [
        "qc_platform_access",
        "monte_carlo_validation"
      ]
    },
    {
      "id": "walk_forward_basics",
      "title": "Walk-Forward Validation Basics",
      "content": "**Purpose**: Detect overfitting and ensure strategy generalizes to unseen data.\n\n**Approach**:\n1. **Training (In-Sample)**: Develop/optimize on 70-80% of data\n2. **Testing (Out-of-Sample)**: Validate on remaining 20-30%\n3. **Compare**: Measure performance degradation\n\n**Example** (5-year backtest 2019-2023):\n\n**Standard 80/20 Split**:\n- In-sample: 2019-2022 (4 years) - Training period\n- Out-of-sample: 2023 (1 year) - Testing period\n\n**Conservative 70/30 Split**:\n- In-sample: 2019-2021 (3 years) - Training period\n- Out-of-sample: 2022-2023 (2 years) - Testing period\n\n**CRITICAL RULE: Never Peek at OOS Data**\n\n‚ö†Ô∏è **NEVER adjust strategy based on OOS results!**\n\n```python\n# ‚ùå WRONG: Looking at OOS before finalizing strategy\noos_sharpe = run_oos_test()\nif oos_sharpe < 0.5:\n    # Adjusting parameters based on OOS = INVALID!\n    strategy = modify_strategy()  # This contaminates validation!\n\n# ‚úÖ CORRECT: Finalize strategy, then test OOS\nstrategy = finalize_strategy()  # Lock parameters\noos_sharpe = run_oos_test()  # Test only, no changes\nif oos_sharpe < threshold:\n    decision = \"ABANDON\"  # Don't adjust, abandon or start over\n```\n\n**Why This Rule Matters**:\n- OOS is for TESTING only, not development\n- Looking at OOS and adjusting makes it in-sample\n- Defeats entire purpose of validation\n- Must treat OOS as truly unseen future data\n\n**Time Split Guidelines**:\n\n**Standard** (80/20):\n- Best for 5+ years of data\n- Balances IS/OOS sample sizes\n- Most common in practice\n\n**Conservative** (70/30):\n- More rigorous testing\n- Better for marginal strategies\n- Reduces OOS variance\n\n**Very Conservative** (60/40):\n- Maximum OOS testing\n- For institutional validation\n- Requires 7+ years data\n\n**Minimum Requirements**:\n- **In-sample**: Minimum 2 years (3+ preferred)\n- **Out-of-sample**: Minimum 6 months (1 year preferred)\n- **Total data**: Minimum 3 years (5+ preferred)\n\n**Trade Count Requirements**:\n- **In-sample**: Minimum 30 trades (50+ preferred)\n- **Out-of-sample**: Minimum 10 trades (20+ preferred)\n- Fewer trades = unreliable validation",
      "tags": [
        "walk-forward",
        "basics",
        "validation"
      ],
      "priority": 1,
      "related_sections": [
        "performance_degradation",
        "executing_validation"
      ]
    },
    {
      "id": "performance_degradation",
      "title": "Performance Degradation Analysis",
      "content": "**Degradation Formula**\n\n`degradation = (IS Sharpe - OOS Sharpe) / IS Sharpe`\n\n**Decision Thresholds**\n\n**< 15% Degradation: EXCELLENT** ‚úÖ\n- **Decision**: DEPLOY_STRATEGY with confidence\n- Strategy generalizes very well\n- Minimal overfitting\n- **Example**: IS 1.0 ‚Üí OOS 0.90 (10% degradation)\n\n**15-30% Degradation: ACCEPTABLE** ‚úÖ\n- **Decision**: PROCEED_WITH_CAUTION\n- Normal performance drop\n- Deploy but monitor closely\n- **Example**: IS 1.0 ‚Üí OOS 0.75 (25% degradation)\n\n**30-40% Degradation: CONCERNING** ‚ö†Ô∏è\n- **Decision**: ESCALATE_TO_HUMAN\n- Borderline overfitting\n- Requires careful human review\n- **Example**: IS 1.0 ‚Üí OOS 0.65 (35% degradation)\n\n**> 40% Degradation: SEVERE** ‚ùå\n- **Decision**: ABANDON_HYPOTHESIS\n- Clear overfitting\n- Will NOT work in live trading\n- **Example**: IS 1.0 ‚Üí OOS 0.50 (50% degradation)\n\n**Research Findings**\n\nFrom academic research on strategy validation:\n- **Expected degradation**: 33-50% is NORMAL\n- **IS-OOS correlation**: Very low (R¬≤ < 0.025)\n- **Implication**: IS performance explains only 1-2% of OOS performance\n\n**Why Degradation Happens**:\n\n1. **Normal market variation** (acceptable)\n   - Different market regimes between IS/OOS\n   - Random performance fluctuation\n   - Expected even for robust strategies\n\n2. **Overfitting** (problematic)\n   - Parameters fit to IS noise\n   - Strategy too complex\n   - Exploited spurious patterns\n\n**Distinguishing Normal vs Overfitting**:\n\n‚úÖ **Likely Normal Variation** (15-30%):\n```python\nIS: Sharpe 1.0, Trades 120, Win% 48%\nOOS: Sharpe 0.78, Trades 32, Win% 44%\n\nDegradation: 22% (acceptable range)\nTrades: Consistent rate (120/4yr ‚âà 30/yr, 32/1yr matches)\nWin%: Similar (48% vs 44%)\nDecision: PROCEED_WITH_CAUTION\n```\n\n‚ùå **Likely Overfitting** (> 40%):\n```python\nIS: Sharpe 1.5, Trades 150, Win% 68%\nOOS: Sharpe 0.4, Trades 8, Win% 38%\n\nDegradation: 73% (severe)\nTrades: Dropped 95% (150/4yr ‚Üí 8/1yr)\nWin%: Dropped 30% (suspicious)\nDecision: ABANDON_HYPOTHESIS\n```\n\n**Additional Checks**:\n\n```python\n# Check multiple metrics, not just Sharpe\nis_metrics = {'sharpe': 1.0, 'sortino': 1.3, 'calmar': 2.0}\noos_metrics = {'sharpe': 0.80, 'sortino': 1.05, 'calmar': 1.65}\n\n# All should degrade similarly for robust strategy\nsharpe_deg = (1.0 - 0.80) / 1.0  # 20%\nsortino_deg = (1.3 - 1.05) / 1.3  # 19%\ncalmar_deg = (2.0 - 1.65) / 2.0  # 18%\n\n# ‚úÖ Consistent degradation = robust\n# ‚ùå Wildly different degradation = overfitting\n```",
      "tags": [
        "degradation",
        "overfitting",
        "thresholds"
      ],
      "priority": 1,
      "related_sections": [
        "walk_forward_basics",
        "decision_criteria"
      ]
    },
    {
      "id": "robustness_score",
      "title": "Robustness Score (OOS/IS Ratio)",
      "content": "**Formula**\n\n`robustness = OOS Sharpe / IS Sharpe`\n\n**Interpretation**: Percentage of IS performance maintained in OOS.\n\n**Thresholds**\n\n**> 0.75: HIGH ROBUSTNESS** ‚úÖ\n- Strategy maintains 75%+ of IS performance\n- Strong generalization\n- Minimal overfitting\n- **Example**: IS 1.0 ‚Üí OOS 0.85 (85% maintained)\n- **Decision**: High confidence deployment\n\n**0.60-0.75: MODERATE ROBUSTNESS** ‚úÖ\n- Strategy maintains 60-75% of IS performance\n- Acceptable generalization\n- Normal market variation\n- **Example**: IS 1.0 ‚Üí OOS 0.68 (68% maintained)\n- **Decision**: Deploy with monitoring\n\n**0.50-0.60: LOW ROBUSTNESS** ‚ö†Ô∏è\n- Strategy maintains only 50-60% of IS performance\n- Concerning degradation\n- Possible overfitting\n- **Example**: IS 1.0 ‚Üí OOS 0.55 (55% maintained)\n- **Decision**: Human review required\n\n**< 0.50: VERY LOW ROBUSTNESS** ‚ùå\n- Strategy maintains less than 50% of IS performance\n- Clear instability\n- High overfitting risk\n- **Example**: IS 1.0 ‚Üí OOS 0.40 (40% maintained)\n- **Decision**: Abandon hypothesis\n\n**Relationship to Degradation**\n\n```python\n# These metrics are inverses\nrobustness = OOS / IS\ndegradation = (IS - OOS) / IS = 1 - robustness\n\n# Examples:\nrobustness = 0.75  # 75% maintained\ndegradation = 0.25  # 25% degradation\n\nrobustness = 0.50  # 50% maintained\ndegradation = 0.50  # 50% degradation\n```\n\n**Why Use Robustness Instead of Degradation?**\n\n1. **More intuitive**: \"Maintains 80%\" vs \"Degrades 20%\"\n2. **Positive framing**: Higher is better\n3. **Direct ratio**: Easy to compare strategies\n4. **Industry standard**: Used in institutional research\n\n**Comparing Strategies**:\n\n```python\n# Strategy A\nIS_A = 0.8\nOOS_A = 0.64\nrobustness_A = 0.64 / 0.8 = 0.80  # HIGH\n\n# Strategy B\nIS_B = 1.2\nOOS_B = 0.60\nrobustness_B = 0.60 / 1.2 = 0.50  # LOW\n\n# Choose Strategy A despite lower OOS Sharpe\n# - More robust (80% vs 50%)\n# - Better generalization\n# - Lower overfitting risk\n```",
      "tags": [
        "robustness",
        "metrics",
        "ratio"
      ],
      "priority": 1,
      "related_sections": [
        "performance_degradation",
        "decision_criteria"
      ]
    },
    {
      "id": "monte_carlo_validation",
      "title": "Monte Carlo Validation Methods",
      "content": "**Monte Carlo Purpose**: Additional robustness testing beyond walk-forward.\n\n**Three MC Methods**:\n\n**1. Trade Sequence Randomization** (Most Common)\n```python\n# Shuffle trade order, recalculate metrics\nfor i in range(1000):\n    shuffled_trades = trades.sample(frac=1, replace=False)\n    sharpe = calculate_sharpe(shuffled_trades)\n    mc_results.append(sharpe)\n\n# Analyze distribution\nmedian_sharpe = np.median(mc_results)\npsr = calculate_psr(mc_results)\n```\n\n**Use Case**: Test if performance depends on specific trade sequence\n**Good For**: Strategies with <200 trades\n\n**2. Parameter Jittering**\n```python\n# Add noise to parameters, re-run backtest\nfor i in range(100):\n    jittered_params = {\n        'rsi_period': base_rsi + np.random.uniform(-2, 2),\n        'stop_loss': base_stop + np.random.uniform(-0.01, 0.01)\n    }\n    result = run_backtest(jittered_params)\n    mc_results.append(result['sharpe'])\n```\n\n**Use Case**: Test parameter sensitivity\n**Good For**: Checking if small parameter changes destroy performance\n\n**3. Bootstrap Resampling**\n```python\n# Resample returns with replacement\nfor i in range(1000):\n    resampled_returns = np.random.choice(returns, size=len(returns), replace=True)\n    sharpe = (resampled_returns.mean() / resampled_returns.std()) * np.sqrt(252)\n    mc_results.append(sharpe)\n```\n\n**Use Case**: Estimate confidence intervals\n**Good For**: Understanding Sharpe ratio uncertainty\n\n**Key Metrics from MC**:\n\n**Probabilistic Sharpe Ratio (PSR)**:\n```python\npsr = P(Sharpe > benchmark)\n\nInterpretation:\n- PSR > 0.95: Very high confidence\n- PSR > 0.90: High confidence\n- PSR > 0.75: Moderate confidence\n- PSR < 0.75: Low confidence\n```\n\n**Deflated Sharpe Ratio (DSR)**:\n```python\n# Adjusts for multiple testing bias\ndsr = sharpe_adjusted_for_trials\n\nInterpretation:\n- DSR > 1.0: Good (after accounting for trials)\n- DSR > 0.7: Acceptable\n- DSR < 0.5: Poor (likely data-mined)\n```\n\n**Minimum Track Record Length (MinTRL)**:\n```python\n# Years needed to be confident Sharpe > benchmark\nmin_trl = years_needed\n\nInterpretation:\n- MinTRL < 1 year: High confidence quickly\n- MinTRL < 2 years: Acceptable\n- MinTRL > 3 years: Long verification needed\n- MinTRL > 5 years: Questionable strategy\n```\n\n**When to Use MC**:\n\n‚úÖ **Use MC When**:\n- Walk-forward validation passed\n- Need institutional-grade validation\n- Strategy has sufficient trades (>50)\n- Want statistical confidence metrics\n\n‚ùå **Skip MC When**:\n- Walk-forward validation failed\n- Very few trades (<30)\n- Early development stage\n- Just need quick validation\n\n**MC Workflow**:\n\n1. **First**: Run walk-forward validation\n2. **If passed**: Consider MC for additional confidence\n3. **If failed**: Don't bother with MC, fix strategy first\n\nMC is a supplement, not a replacement, for walk-forward validation.",
      "tags": [
        "monte_carlo",
        "validation",
        "psr"
      ],
      "priority": 2,
      "related_sections": [
        "monte_carlo_execution",
        "probabilistic_sharpe_ratio"
      ]
    },
    {
      "id": "probabilistic_sharpe_ratio",
      "title": "Probabilistic Sharpe Ratio (PSR)",
      "content": "**PSR Definition**: Probability that true Sharpe ratio exceeds a benchmark.\n\n**Formula**:\n```python\nPSR = Œ¶((Sharpe - Benchmark) / SE(Sharpe))\n\nwhere:\n- Œ¶ = Standard normal CDF\n- SE(Sharpe) = Standard error of Sharpe estimate\n- SE(Sharpe) ‚âà ‚àö((1 + Sharpe¬≤/2) / N)\n- N = Number of observations (trades or periods)\n```\n\n**Interpretation**:\n\n**PSR > 0.95**: Very High Confidence ‚úÖ\n- 95%+ probability Sharpe > benchmark\n- Strong statistical evidence\n- Deploy with confidence\n\n**PSR > 0.90**: High Confidence ‚úÖ\n- 90%+ probability Sharpe > benchmark\n- Good statistical evidence\n- Acceptable for deployment\n\n**PSR > 0.75**: Moderate Confidence ‚ö†Ô∏è\n- 75%+ probability Sharpe > benchmark\n- Weak statistical evidence\n- Deploy with caution\n\n**PSR < 0.75**: Low Confidence ‚ùå\n- < 75% probability Sharpe > benchmark\n- Insufficient statistical evidence\n- Do not deploy\n\n**Python Implementation**:\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef calculate_psr(sharpe_ratios, benchmark=0.0):\n    \"\"\"\n    Calculate Probabilistic Sharpe Ratio\n    \n    Args:\n        sharpe_ratios: Array of Sharpe ratios from MC runs\n        benchmark: Benchmark Sharpe (default 0.0)\n    \n    Returns:\n        PSR value between 0 and 1\n    \"\"\"\n    n = len(sharpe_ratios)\n    sr_mean = np.mean(sharpe_ratios)\n    sr_std = np.std(sharpe_ratios, ddof=1)  # Sample std\n    \n    if sr_std == 0:\n        return 1.0 if sr_mean > benchmark else 0.0\n    \n    # Calculate z-score\n    z_score = (sr_mean - benchmark) / (sr_std / np.sqrt(n))\n    \n    # PSR = P(Z < z_score)\n    psr = stats.norm.cdf(z_score)\n    \n    return psr\n\n# Example usage\nmc_sharpes = np.array([0.85, 0.92, 0.78, ...])  # From 1000 MC runs\npsr = calculate_psr(mc_sharpes, benchmark=0.0)\nprint(f\"PSR: {psr:.3f}\")\n\nif psr > 0.95:\n    print(\"‚úÖ Very high confidence\")\nelif psr > 0.90:\n    print(\"‚úÖ High confidence\")\nelif psr > 0.75:\n    print(\"‚ö†Ô∏è Moderate confidence\")\nelse:\n    print(\"‚ùå Low confidence - do not deploy\")\n```\n\n**Trade Count Impact on PSR**:\n\n```python\n# Same strategy, different sample sizes\nSharpe = 1.0\n\n# 30 trades\nPSR_30 = 0.72  # Low confidence\n\n# 100 trades\nPSR_100 = 0.89  # Moderate confidence\n\n# 300 trades\nPSR_300 = 0.97  # High confidence\n\n# More trades ‚Üí Higher PSR (with same Sharpe)\n```\n\n**PSR vs Walk-Forward**:\n\n**Walk-Forward**: Tests out-of-sample generalization\n**PSR**: Quantifies statistical confidence\n\n**Use Both**:\n1. Walk-forward: Detect overfitting\n2. PSR: Measure confidence in results\n\n**Example Decision**:\n```python\n# Strategy A\nOOS_Sharpe_A = 0.85\nPSR_A = 0.96\nDecision: DEPLOY ‚úÖ (good performance + high confidence)\n\n# Strategy B\nOOS_Sharpe_B = 0.85\nPSR_B = 0.68\nDecision: CAUTION ‚ö†Ô∏è (same performance, low confidence)\n\n# Strategy C\nOOS_Sharpe_C = 1.20\nPSR_C = 0.72\nDecision: CAUTION ‚ö†Ô∏è (high performance but low confidence = suspicious)\n```",
      "tags": [
        "psr",
        "statistics",
        "confidence"
      ],
      "priority": 2,
      "related_sections": [
        "monte_carlo_validation",
        "deflated_sharpe_ratio"
      ]
    },
    {
      "id": "deflated_sharpe_ratio",
      "title": "Deflated Sharpe Ratio (DSR)",
      "content": "**DSR Purpose**: Adjust Sharpe ratio for multiple testing bias (data mining).\n\n**Problem**: If you test 100 strategies, the best one will have inflated Sharpe by luck.\n\n**DSR Solution**: Deflate Sharpe based on number of trials tested.\n\n**Formula**:\n```python\nDSR = Œ¶((Sharpe - Sharpe_max_expected) / SE(Sharpe))\n\nwhere:\n- Sharpe_max_expected = Expected max Sharpe from N random trials\n- SE(Sharpe) = Standard error\n- Œ¶ = Standard normal CDF\n```\n\n**Interpretation**:\n\n**DSR > 1.0**: Excellent ‚úÖ\n- Sharpe exceeds what's expected from random trials\n- Strategy has real edge\n- Not due to data mining\n\n**DSR > 0.7**: Good ‚úÖ\n- Sharpe likely not from pure luck\n- Acceptable evidence\n- Deploy with confidence\n\n**DSR > 0.5**: Marginal ‚ö†Ô∏è\n- Sharpe may be inflated\n- Borderline data mining\n- Careful validation needed\n\n**DSR < 0.5**: Poor ‚ùå\n- Sharpe likely from data mining\n- Not statistically significant\n- Do not deploy\n\n**Python Implementation**:\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef calculate_dsr(sharpe, num_trials, num_observations, sr_std=0.0):\n    \"\"\"\n    Calculate Deflated Sharpe Ratio\n    \n    Args:\n        sharpe: Observed Sharpe ratio\n        num_trials: Number of strategies tested\n        num_observations: Sample size (trades or periods)\n        sr_std: Standard deviation of Sharpe (from MC)\n    \n    Returns:\n        DSR value\n    \"\"\"\n    # Expected maximum Sharpe from random trials\n    # Using extreme value theory\n    gamma = 0.5772  # Euler-Mascheroni constant\n    sharpe_max = np.sqrt(np.log(num_trials) / (2 * np.pi)) * \\\n                 (1 - gamma / np.sqrt(2 * np.log(num_trials)))\n    \n    # Standard error of Sharpe\n    if sr_std > 0:\n        se_sharpe = sr_std / np.sqrt(num_observations)\n    else:\n        se_sharpe = np.sqrt((1 + sharpe**2 / 2) / num_observations)\n    \n    # Deflated Sharpe Ratio\n    if se_sharpe == 0:\n        return 1.0 if sharpe > sharpe_max else 0.0\n    \n    z_score = (sharpe - sharpe_max) / se_sharpe\n    dsr = stats.norm.cdf(z_score)\n    \n    return dsr\n\n# Example usage\nobserved_sharpe = 1.2\nnum_trials = 50  # Tested 50 parameter combinations\nnum_obs = 120  # 120 trades\n\ndsr = calculate_dsr(observed_sharpe, num_trials, num_obs)\nprint(f\"DSR: {dsr:.3f}\")\n\nif dsr > 1.0:\n    print(\"‚úÖ Excellent - exceeds random expectation\")\nelif dsr > 0.7:\n    print(\"‚úÖ Good - likely not data-mined\")\nelif dsr > 0.5:\n    print(\"‚ö†Ô∏è Marginal - possible data mining\")\nelse:\n    print(\"‚ùå Poor - likely data-mined\")\n```\n\n**Number of Trials Impact**:\n\n```python\n# Same Sharpe (1.2), different trials\n\n# 10 trials (small optimization)\nDSR_10 = 0.92  # Good\n\n# 100 trials (large optimization)\nDSR_100 = 0.68  # Marginal\n\n# 1000 trials (extensive data mining)\nDSR_1000 = 0.35  # Poor\n\n# More trials ‚Üí Lower DSR (harder to prove significance)\n```\n\n**When to Use DSR**:\n\n‚úÖ **Use DSR When**:\n- Ran optimization (tested multiple parameters)\n- Tested many strategy variations\n- Need to account for selection bias\n- Institutional validation required\n\n‚ùå **Don't Need DSR When**:\n- Single strategy, no optimization\n- First iteration of hypothesis\n- Early development stage\n\n**Tracking Number of Trials**:\n\n```json\n// iteration_state.json\n{\n  \"cost_tracking\": {\n    \"backtests_run\": 1,\n    \"optimization_attempts\": 50,  // Number of trials for DSR\n    \"total_hypotheses_tested\": 5\n  }\n}\n```\n\n**DSR in Decision Framework**:\n\n```python\nif oos_sharpe > 0.7 and dsr > 0.7:\n    decision = \"DEPLOY_STRATEGY\"\nelif oos_sharpe > 0.5 and dsr > 0.5:\n    decision = \"PROCEED_WITH_CAUTION\"\nelse:\n    decision = \"ABANDON_HYPOTHESIS\"\n```",
      "tags": [
        "dsr",
        "data_mining",
        "bias"
      ],
      "priority": 2,
      "related_sections": [
        "probabilistic_sharpe_ratio",
        "monte_carlo_validation"
      ]
    },
    {
      "id": "minimum_track_record",
      "title": "Minimum Track Record Length (MinTRL)",
      "content": "**MinTRL Definition**: Time needed to be confident strategy's Sharpe exceeds benchmark.\n\n**Purpose**: Answer the question: \"How long must I paper trade before I'm confident this works?\"\n\n**Formula**:\n```python\nMinTRL = N * ((Z_Œ± * œÉ) / (Sharpe - Benchmark))¬≤\n\nwhere:\n- N = Current sample size (years)\n- Z_Œ± = Z-score for confidence level (1.65 for 95%)\n- œÉ = Sharpe ratio standard deviation\n- Sharpe = Observed Sharpe ratio\n- Benchmark = Target Sharpe (e.g., 0.0)\n```\n\n**Interpretation**:\n\n**MinTRL < 1 year**: Excellent ‚úÖ\n- Can verify quickly\n- High signal-to-noise ratio\n- Strong strategy\n\n**MinTRL < 2 years**: Good ‚úÖ\n- Reasonable verification time\n- Acceptable for deployment\n- Normal strategy\n\n**MinTRL < 3 years**: Acceptable ‚ö†Ô∏è\n- Longer verification needed\n- Deploy with extended paper trading\n- Marginal strategy\n\n**MinTRL > 3 years**: Concerning ‚ùå\n- Very long verification needed\n- Low signal-to-noise ratio\n- Questionable strategy\n\n**MinTRL > 5 years**: Poor ‚ùå\n- Impractically long verification\n- Likely not worth pursuing\n- Consider abandoning\n\n**Python Implementation**:\n```python\nimport numpy as np\n\ndef calculate_min_trl(sharpe, sharpe_std, benchmark=0.0, confidence=0.95):\n    \"\"\"\n    Calculate Minimum Track Record Length\n    \n    Args:\n        sharpe: Observed Sharpe ratio\n        sharpe_std: Standard deviation of Sharpe (from MC or time series)\n        benchmark: Benchmark Sharpe to exceed (default 0.0)\n        confidence: Confidence level (default 0.95 for 95%)\n    \n    Returns:\n        Years needed to verify strategy\n    \"\"\"\n    from scipy import stats\n    \n    # Z-score for confidence level\n    z_alpha = stats.norm.ppf(confidence)\n    \n    # Avoid division by zero\n    if sharpe <= benchmark:\n        return np.inf  # Strategy doesn't exceed benchmark\n    \n    if sharpe_std == 0:\n        return 0  # Perfect strategy (unrealistic)\n    \n    # MinTRL formula\n    min_trl = ((z_alpha * sharpe_std) / (sharpe - benchmark)) ** 2\n    \n    return min_trl\n\n# Example usage\nobserved_sharpe = 0.85\nsharpe_std = 0.20  # From MC or rolling window\n\nmin_trl = calculate_min_trl(observed_sharpe, sharpe_std, benchmark=0.0)\nprint(f\"MinTRL: {min_trl:.1f} years\")\n\nif min_trl < 1.0:\n    print(\"‚úÖ Excellent - fast verification\")\nelif min_trl < 2.0:\n    print(\"‚úÖ Good - reasonable verification\")\nelif min_trl < 3.0:\n    print(\"‚ö†Ô∏è Acceptable - extended verification needed\")\nelse:\n    print(\"‚ùå Poor - verification time too long\")\n```\n\n**MinTRL Examples**:\n\n```python\n# High Sharpe, Low Volatility (Best Case)\nSharpe = 1.5\nStd = 0.15\nMinTRL = 0.3 years  # 4 months - Excellent!\n\n# Medium Sharpe, Medium Volatility (Normal)\nSharpe = 0.85\nStd = 0.25\nMinTRL = 1.8 years  # Acceptable\n\n# Low Sharpe, High Volatility (Worst Case)\nSharpe = 0.40\nStd = 0.35\nMinTRL = 6.8 years  # Impractical\n```\n\n**Factors Affecting MinTRL**:\n\n**Improves MinTRL** (shorter verification):\n- Higher Sharpe ratio\n- Lower Sharpe volatility\n- More consistent returns\n- Lower benchmark\n\n**Worsens MinTRL** (longer verification):\n- Lower Sharpe ratio\n- Higher Sharpe volatility\n- Inconsistent returns\n- Higher benchmark\n\n**Using MinTRL in Practice**:\n\n```python\n# After walk-forward validation\noos_sharpe = 0.75\noos_sharpe_std = 0.22  # From rolling windows\n\nmin_trl = calculate_min_trl(oos_sharpe, oos_sharpe_std)\n\nif min_trl < 1.0:\n    # Paper trade for 6-12 months, then deploy\n    paper_trade_months = 12\nelif min_trl < 2.0:\n    # Paper trade for 1-2 years before confidence\n    paper_trade_months = 24\nelse:\n    # Too long to verify - reconsider strategy\n    decision = \"ESCALATE_TO_HUMAN\"\n```\n\n**MinTRL in Decision Framework**:\n\n```python\nif oos_sharpe > 0.7 and min_trl < 2.0:\n    decision = \"DEPLOY_STRATEGY\"\n    paper_trade_period = min(min_trl * 12, 24)  # Months, max 2 years\nelif oos_sharpe > 0.5 and min_trl < 3.0:\n    decision = \"PROCEED_WITH_CAUTION\"\n    paper_trade_period = min(min_trl * 12, 36)  # Months, max 3 years\nelse:\n    decision = \"ABANDON_HYPOTHESIS\"\n```",
      "tags": [
        "mintrl",
        "track_record",
        "verification"
      ],
      "priority": 2,
      "related_sections": [
        "probabilistic_sharpe_ratio",
        "decision_criteria"
      ]
    },
    {
      "id": "trade_count_requirements",
      "title": "Trade Count Requirements for Validation",
      "content": "**Why Trade Count Matters**: Statistical significance requires sufficient sample size.\n\n**Minimum Requirements**:\n\n**In-Sample**:\n- **Minimum**: 30 trades\n- **Preferred**: 50+ trades\n- **Ideal**: 100+ trades\n\n**Out-of-Sample**:\n- **Minimum**: 10 trades\n- **Preferred**: 20+ trades\n- **Ideal**: 50+ trades\n\n**Total (IS + OOS)**:\n- **Minimum**: 40 trades\n- **Preferred**: 70+ trades\n- **Ideal**: 150+ trades\n\n**Why These Numbers?**\n\n**Central Limit Theorem**:\n- N ‚â• 30: Distribution approaches normal\n- Enables statistical tests\n- Confidence intervals become reliable\n\n**Sharpe Ratio Stability**:\n```python\n# Sharpe standard error decreases with ‚àöN\nSE(Sharpe) ‚âà ‚àö((1 + Sharpe¬≤/2) / N)\n\n# 30 trades: SE ‚âà 0.19 (high uncertainty)\n# 100 trades: SE ‚âà 0.10 (moderate uncertainty)\n# 300 trades: SE ‚âà 0.06 (low uncertainty)\n```\n\n**Trade Count Issues**:\n\n**Too Few Trades (< 30)**:\n```python\nIS: 15 trades, OOS: 4 trades\n\nProblems:\n- High variance in metrics\n- Unreliable Sharpe ratio\n- Can't detect overfitting\n- Results mostly noise\n\nDecision: ESCALATE_TO_HUMAN (insufficient data)\n```\n\n**Acceptable Trades (30-100)**:\n```python\nIS: 85 trades, OOS: 22 trades\n\nCharacteristics:\n- Moderate statistical power\n- Sharpe somewhat reliable\n- Can detect severe overfitting\n- Acceptable for deployment\n\nDecision: Proceed if other metrics good\n```\n\n**Many Trades (100+)**:\n```python\nIS: 320 trades, OOS: 85 trades\n\nCharacteristics:\n- High statistical power\n- Sharpe very reliable\n- Can detect subtle overfitting\n- Strong confidence in results\n\nDecision: High confidence deployment\n```\n\n**Trade Rate Consistency Check**:\n\n```python\n# Check if OOS trade rate matches IS\nis_years = 4.0\noos_years = 1.0\nis_trades = 120\noos_trades = 32\n\n# Calculate annual trade rates\nis_rate = is_trades / is_years  # 30 trades/year\noos_rate = oos_trades / oos_years  # 32 trades/year\n\nrate_change = (oos_rate - is_rate) / is_rate  # +6.7%\n\nif abs(rate_change) < 0.30:  # Within 30%\n    print(\"‚úÖ Trade rate consistent\")\nelse:\n    print(\"‚ùå Trade rate changed significantly - suspicious\")\n```\n\n**When Trade Count is Too Low**:\n\n**Option 1: Extend Time Period**\n```python\n# Current: 2019-2023 (5 years) ‚Üí 60 trades\n# Extended: 2015-2023 (9 years) ‚Üí 110 trades ‚úÖ\n```\n\n**Option 2: Lower Frequency**\n```python\n# Current: Daily bars ‚Üí 45 trades\n# Switch to: Hourly bars ‚Üí 280 trades ‚úÖ\n# Caution: Changes strategy characteristics\n```\n\n**Option 3: Accept Limitation**\n```python\n# If trade count unavoidably low:\ndecision = \"PROCEED_WITH_CAUTION\"\nrequire_longer_paper_trading = True\nmonitor_closely_in_production = True\n```\n\n**Trade Count in Decision Logic**:\n\n```python\nis_trades = 85\noos_trades = 18\noos_sharpe = 0.75\ndegradation = 0.20\n\nif oos_trades < 10:\n    decision = \"ESCALATE_TO_HUMAN\"\n    reason = \"Insufficient OOS trades (< 10)\"\nelif oos_trades < 20 and degradation > 0.25:\n    decision = \"ESCALATE_TO_HUMAN\"\n    reason = \"Low OOS trades + high degradation\"\nelif is_trades < 30:\n    decision = \"ESCALATE_TO_HUMAN\"\n    reason = \"Insufficient IS trades (< 30)\"\nelse:\n    # Proceed with normal thresholds\n    if degradation < 0.15 and oos_sharpe > 0.7:\n        decision = \"DEPLOY_STRATEGY\"\n    # ... other conditions\n```",
      "tags": [
        "trades",
        "sample_size",
        "statistics"
      ],
      "priority": 2,
      "related_sections": [
        "walk_forward_basics",
        "decision_criteria"
      ]
    },
    {
      "id": "decision_criteria",
      "title": "Phase 5 Decision Criteria",
      "content": "**Comprehensive Decision Matrix**\n\n**DEPLOY_STRATEGY** ‚úÖ\n\nAll must be true:\n- Degradation < 15% (excellent generalization)\n- Robustness > 0.75 (maintains 75%+ performance)\n- OOS Sharpe > 0.7 (good absolute performance)\n- OOS Trades ‚â• 10 (minimum statistical significance)\n\n**Next Steps**:\n- Paper trade for MinTRL period (or 6-12 months minimum)\n- Monitor closely for first 3 months live\n- Deploy with position sizing at 50% initially\n\n**PROCEED_WITH_CAUTION** ‚ö†Ô∏è\n\nAll must be true:\n- Degradation < 30% (acceptable generalization)\n- Robustness > 0.60 (maintains 60%+ performance)\n- OOS Sharpe > 0.5 (acceptable absolute performance)\n- OOS Trades ‚â• 10 (minimum statistical significance)\n\n**Next Steps**:\n- Paper trade for 12-24 months\n- Require higher MinTRL threshold\n- Deploy with position sizing at 25% initially\n- Monitor very closely, be ready to stop\n\n**ABANDON_HYPOTHESIS** ‚ùå\n\nAny of these:\n- Degradation > 40% (severe overfitting)\n- Robustness < 0.5 (poor generalization)\n- OOS Sharpe < 0 (negative performance)\n- OOS Trades < 5 (insufficient data)\n\n**Next Steps**:\n- Do NOT deploy\n- Document failure reason\n- Start new hypothesis\n- Learn from failure patterns\n\n**ESCALATE_TO_HUMAN** üîç\n\nAny of these:\n- Results in borderline zone (30-40% degradation)\n- Conflicting signals (good Sharpe but poor trades)\n- Insufficient data (5-10 OOS trades)\n- Unusual patterns requiring judgment\n\n**Next Steps**:\n- Human review required\n- Additional analysis\n- Possible extended validation\n- Case-by-case decision\n\n**Example Decisions**:\n\n```python\n# Example 1: DEPLOY_STRATEGY ‚úÖ\nIS_Sharpe = 0.95\nOOS_Sharpe = 0.88\nIS_Trades = 125\nOOS_Trades = 34\nDegradation = 7.4%\nRobustness = 0.93\n\nDecision: DEPLOY_STRATEGY\nReason: \"Minimal degradation (7.4%), high robustness (0.93), good OOS Sharpe (0.88)\"\nPaper Trade: 6-12 months\nInitial Size: 50%\n```\n\n```python\n# Example 2: PROCEED_WITH_CAUTION ‚ö†Ô∏è\nIS_Sharpe = 0.88\nOOS_Sharpe = 0.65\nIS_Trades = 95\nOOS_Trades = 18\nDegradation = 26.1%\nRobustness = 0.74\n\nDecision: PROCEED_WITH_CAUTION\nReason: \"Acceptable degradation (26%), borderline robustness (0.74)\"\nPaper Trade: 18-24 months\nInitial Size: 25%\n```\n\n```python\n# Example 3: ABANDON_HYPOTHESIS ‚ùå\nIS_Sharpe = 1.20\nOOS_Sharpe = 0.38\nIS_Trades = 145\nOOS_Trades = 12\nDegradation = 68.3%\nRobustness = 0.32\n\nDecision: ABANDON_HYPOTHESIS\nReason: \"Severe degradation (68%), very low robustness (0.32) - clear overfitting\"\nNext: Start new hypothesis\n```\n\n```python\n# Example 4: ESCALATE_TO_HUMAN üîç\nIS_Sharpe = 0.85\nOOS_Sharpe = 0.58\nIS_Trades = 88\nOOS_Trades = 8\nDegradation = 31.8%\nRobustness = 0.68\n\nDecision: ESCALATE_TO_HUMAN\nReason: \"Borderline degradation (32%), low OOS trades (8) - needs review\"\nNext: Extend OOS period or human judgment\n```\n\n**Additional Checks**:\n\n```python\n# Multi-metric consistency check\nif all([\n    sharpe_degradation < 0.30,\n    sortino_degradation < 0.30,\n    calmar_degradation < 0.30\n]):\n    consistency = \"GOOD\"\nelse:\n    consistency = \"POOR\"\n    decision = \"ESCALATE_TO_HUMAN\"\n    reason = \"Inconsistent metric degradation - suspicious\"\n```",
      "tags": [
        "decisions",
        "thresholds",
        "phase5"
      ],
      "priority": 1,
      "related_sections": [
        "performance_degradation",
        "robustness_score"
      ]
    },
    {
      "id": "qc_coding_standards",
      "title": "QuantConnect Coding Standards",
      "content": "**Essential QC Platform Coding Rules**\n\nWhen modifying strategy code for validation (IS/OOS splits), follow these QuantConnect platform standards.\n\n**Complete Reference**: See HELP/qc_guide.json for full specification\n\n---\n\n## Syntax and Naming\n\n‚úÖ **CORRECT**:\n```python\n# Variables and functions: snake_case\ndef calculate_delta_ratio(symbol, expiry):\n    \"\"\"Calculate delta ratio for option.\"\"\"\n    rsi_period = 14\n    return ratio\n\n# Constants: UPPERCASE\nMINUTE = 60\nDAILY = \"1d\"\nMAX_HOLDINGS = 10\n```\n\n‚ùå **WRONG - AVOID PascalCase**:\n```python\ndef CalculateDeltaRatio(Symbol, Expiry):  # PascalCase - WRONG (legacy QC)\n    RSI_Period = 14  # Mixed case - WRONG\n\nclass MyStrategy:  # PascalCase class - AVOID (use snake_case)\n    pass\n```\n\n**Rules**:\n- **ALWAYS** use `snake_case` for variables, functions, and classes\n- **ONLY** use `UPPERCASE` for constants (MINUTE, DAILY, MAX_POSITIONS, etc.)\n- **NEVER** use `PascalCase` (legacy QuantConnect convention - avoid at all costs)\n- Follow PEP8 (4 spaces, no tabs)\n- Triple-quoted docstrings\n\n---\n\n## Emojis - BANNED on QC Platform\n\n‚ùå **CRITICAL: NEVER use emojis in QC code**\n\nEmojis cause **syntax errors** and **compilation failures** on QuantConnect platform.\n\n**WRONG - Will cause syntax error**:\n```python\ndef Initialize(self):\n    self.SetStartDate(2019, 1, 1)  # üìÖ Start date - SYNTAX ERROR!\n    self.spy = self.AddEquity(\"SPY\", Resolution.Daily)  # üìà Add SPY - ERROR!\n    \n    # üöÄ Initialize strategy - SYNTAX ERROR!\n    self.rsi_period = 14\n```\n\n**Error Message**:\n```\nRuntimeError: Syntax error in algorithm. \nInvalid character in identifier at line X\n```\n\n**CORRECT - Use plain text comments**:\n```python\ndef Initialize(self):\n    # Set backtest start date\n    self.SetStartDate(2019, 1, 1)\n    \n    # Add SPY equity\n    self.spy = self.AddEquity(\"SPY\", Resolution.Daily)\n    \n    # Initialize strategy parameters\n    self.rsi_period = 14\n```\n\n**Why Emojis Fail**:\n- QC platform uses Python 3.8 with strict ASCII enforcement\n- Emojis are Unicode characters (U+1F600 etc.)\n- Python parser rejects non-ASCII in code (outside strings)\n- Compilation fails before backtest even runs\n\n**Even in strings - AVOID**:\n```python\n# ‚ùå RISKY - May work in strings but avoid anyway\nself.Debug(\"Portfolio value: üí∞ $100,000\")  # Avoid\n\n# ‚úÖ SAFE - Use plain text\nself.Debug(\"Portfolio value: $100,000\")  # Correct\n```\n\n**Rule**: **NO EMOJIS ANYWHERE** in QC code - comments, strings, or identifiers.\n\n---\n\n## Date Modification for Walk-Forward\n\nWhen splitting data for IS/OOS, modify `SetStartDate()` and `SetEndDate()` calls:\n\n‚úÖ **CORRECT**:\n```python\n# Original strategy (2019-2023)\nclass MyStrategy(QCAlgorithm):\n    def Initialize(self):\n        self.SetStartDate(2019, 1, 1)\n        self.SetEndDate(2023, 12, 31)\n        # ...\n\n# In-Sample modification (2019-2022)\n# qc_validate modifies to:\nself.SetStartDate(2019, 1, 1)\nself.SetEndDate(2022, 12, 31)  # Changed for IS\n\n# Out-of-Sample modification (2023)\n# qc_validate modifies to:\nself.SetStartDate(2023, 1, 1)  # Changed for OOS\nself.SetEndDate(2023, 12, 31)\n```\n\n**CRITICAL**: qc_validate.py automatically modifies these dates using regex:\n```python\nimport re\n\n# Modify for IS\nis_code = re.sub(\n    r'SetEndDate\\((\\d+),\\s*(\\d+),\\s*(\\d+)\\)',\n    'SetEndDate(2022, 12, 31)',\n    strategy_code\n)\n\n# Modify for OOS\noos_code = re.sub(\n    r'SetStartDate\\((\\d+),\\s*(\\d+),\\s*(\\d+)\\)',\n    'SetStartDate(2023, 1, 1)',\n    strategy_code\n)\n```\n\n**Rule**: Do NOT hardcode dates in variables - always use `SetStartDate()` and `SetEndDate()` method calls\n\n---\n\n## Environment Compatibility\n\n‚úÖ **ALLOWED** imports:\n```python\nfrom AlgorithmImports import *\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n```\n\n‚ùå **FORBIDDEN** imports:\n```python\nimport requests  # Not available in QC\nimport yfinance  # Not available in QC\nfrom dotenv import load_env  # Not available in QC\nimport plotly  # Not available in QC\nimport os  # File operations not allowed\n```\n\n**Rules**:\n- No file I/O operations\n- No subprocess or os.system calls\n- No async/await or multithreading\n- QC runtime is synchronous single-threaded\n\n---\n\n## Algorithm Structure\n\n‚úÖ **CORRECT**:\n```python\nclass MyStrategy(QCAlgorithm):\n    def Initialize(self):\n        self.SetStartDate(2019, 1, 1)\n        self.SetEndDate(2023, 12, 31)\n        self.SetCash(100000)\n        \n        # Add securities in Initialize\n        self.spy = self.AddEquity(\"SPY\", Resolution.Daily)\n        \n        # Create indicators in Initialize\n        self.rsi = self.RSI(\"SPY\", 14)\n    \n    def OnData(self, data):\n        if self.IsWarmingUp:\n            return\n        \n        # Use self.Debug(), not print()\n        self.Debug(f\"RSI: {self.rsi.Current.Value:.2f}\")\n```\n\n‚ùå **WRONG**:\n```python\nclass MyStrategy(QCAlgorithm):\n    def OnData(self, data):\n        # WRONG: Adding securities in OnData\n        self.spy = self.AddEquity(\"SPY\", Resolution.Daily)\n        \n        # WRONG: Using print()\n        print(\"RSI:\", self.rsi.Current.Value)\n        \n        # WRONG: Not checking IsWarmingUp\n        if self.rsi.Current.Value < 30:\n            self.SetHoldings(\"SPY\", 1.0)\n```\n\n**Rules**:\n- Define dates, cash, securities in `Initialize()`\n- Create indicators in `Initialize()`\n- Use `self.Debug()`, not `print()`\n- Guard with `if self.IsWarmingUp: return`\n- Never add securities in `OnData()`\n\n---\n\n## Logging Standards\n\n‚úÖ **CORRECT**:\n```python\nself.Debug(f\"Signal: {signal}, RSI: {self.rsi.Current.Value:.2f}\")\nself.Log(f\"Position opened: {self.Portfolio.TotalPortfolioValue}\")\nself.Error(f\"Failed to execute trade: {ex}\")\n```\n\n‚ùå **WRONG**:\n```python\nprint(\"Signal:\", signal)  # WRONG - use self.Debug()\nlogging.info(\"Position opened\")  # WRONG - no external logging\n```\n\n**Rules**:\n- `self.Debug()` - Quick logging (throttle in OnData)\n- `self.Log()` - Standard logging\n- `self.Error()` - Exceptions\n- Never use `print()` or external logging frameworks\n\n---\n\n## Data Access\n\n‚úÖ **CORRECT**:\n```python\ndef Initialize(self):\n    # Warm up indicators\n    self.SetWarmup(100)\n    self.rsi = self.RSI(\"SPY\", 14)\n\ndef OnData(self, data):\n    if self.IsWarmingUp:\n        return  # Don't trade during warmup\n    \n    if self.rsi.IsReady:\n        rsi_value = self.rsi.Current.Value\n```\n\n‚ùå **WRONG**:\n```python\ndef OnData(self, data):\n    # WRONG: Not checking IsWarmingUp\n    rsi_value = self.rsi.Current.Value  # May not be ready!\n    \n    # WRONG: Expensive history call in loop\n    for symbol in self.symbols:\n        history = self.History([symbol], 100, Resolution.Minute)\n```\n\n**Rules**:\n- Always check `self.IsWarmingUp`\n- Always check `indicator.IsReady`\n- Avoid `History()` calls in loops\n- Prefetch data outside loops\n\n---\n\n## Validation-Specific Rules\n\n**CRITICAL: NEVER Peek at OOS Data**\n\n‚ùå **WRONG**:\n```python\n# Running OOS backtest\noos_sharpe = run_oos_backtest()\n\n# WRONG: Adjusting strategy based on OOS results!\nif oos_sharpe < 0.5:\n    # This contaminates validation!\n    strategy = modify_strategy_parameters()\n    oos_sharpe = run_oos_backtest()  # Invalid!\n```\n\n‚úÖ **CORRECT**:\n```python\n# Finalize strategy (lock parameters)\nfinal_strategy = finalize_strategy()\n\n# Run OOS backtest (test only, no changes)\noos_sharpe = run_oos_backtest(final_strategy)\n\n# Make decision based on results\nif oos_sharpe < threshold:\n    decision = \"ABANDON\"  # Don't adjust, abandon or start over\nelse:\n    decision = \"DEPLOY\"\n```\n\n**Rule**: OOS is for TESTING only. Never adjust strategy based on OOS results.\n\n---\n\n## Quick Checklist\n\nBefore running validation, verify:\n\n- ‚úÖ `snake_case` for variables/functions/classes, `UPPERCASE` for constants\n- ‚úÖ Valid QC imports only (no requests, yfinance, etc.)\n- ‚úÖ `Initialize()` defines dates, cash, securities\n- ‚úÖ `SetStartDate()` and `SetEndDate()` use method calls (not variables)\n- ‚úÖ No file I/O or async\n- ‚úÖ `OnData()` guards with `IsWarmingUp`\n- ‚úÖ Use `self.Debug()` not `print()`\n- ‚úÖ Indicators created in `Initialize()`\n- ‚úÖ Strategy finalized BEFORE OOS testing\n- ‚úÖ No emojis in code\n\n**Full Specification**: HELP/qc_guide.json",
      "tags": [
        "coding_standards",
        "qc_platform",
        "best_practices",
        "validation"
      ],
      "priority": 1,
      "related_sections": [
        "walk_forward_basics",
        "common_errors"
      ]
    },
    {
      "id": "common_errors",
      "title": "Common Errors and Fixes",
      "content": "**Error: \"No project_id in iteration_state.json\"**\n\n**Cause**: Project not created yet\n\n**Fix**:\n```bash\n# Run backtest first (creates project)\n/qc-backtest\n\n# Verify project_id\ncat iteration_state.json | jq '.project.project_id'\n```\n\n**Error: \"Insufficient out-of-sample trades (< 10)\"**\n\n**Cause**: OOS period too short or strategy doesn't trade enough\n\n**Fix 1 - Extend OOS Period**:\n```python\n# Instead of 80/20 split\nsplit = 0.70  # Use 70/30 for more OOS trades\n\n# Or extend total backtest period\ntotal_period = \"2017-2024\"  # 8 years instead of 5\n```\n\n**Fix 2 - Use Different Time Resolution**:\n```python\n# If using daily bars with few trades\n# Consider hourly bars (more data points)\n# Caution: Changes strategy characteristics\n```\n\n**Fix 3 - Accept Limitation**:\n```python\nif oos_trades < 10:\n    decision = \"ESCALATE_TO_HUMAN\"\n    recommendation = \"Extend paper trading to compensate for low sample size\"\n```\n\n**Error: \"Strategy file not found in project\"**\n\n**Cause**: main.py doesn't exist in QC project\n\n**Fix**:\n```python\n# Upload strategy to project\nwith open('strategy.py') as f:\n    code = f.read()\n\napi.upload_file(project_id, \"main.py\", code)\n```\n\n**Error: \"Negative out-of-sample Sharpe\"**\n\n**Cause**: Strategy loses money OOS - severe overfitting or regime change\n\n**Decision**:\n```python\nif oos_sharpe < 0:\n    decision = \"ABANDON_HYPOTHESIS\"\n    reason = \"Negative OOS performance - strategy failed validation\"\n    next_step = \"Analyze failure, design new hypothesis\"\n```\n\n**Error: \"OOS trade rate differs significantly from IS\"**\n\n**Cause**: Strategy behavior changed between IS/OOS\n\n**Analysis**:\n```python\nis_rate = is_trades / is_years\noos_rate = oos_trades / oos_years\nrate_change = (oos_rate - is_rate) / is_rate\n\nif rate_change < -0.50:  # 50% fewer trades\n    warning = \"Strategy stopped trading OOS - likely overfitting to IS conditions\"\n    decision = \"ESCALATE_TO_HUMAN\"\nelif rate_change > 1.0:  # 100% more trades\n    warning = \"Strategy trading much more OOS - behavior changed\"\n    decision = \"ESCALATE_TO_HUMAN\"\n```\n\n**Error: \"Time split not working correctly\"**\n\n**Cause**: Strategy code modification error\n\n**Fix**:\n```python\n# Verify dates in modified code\nprint(f\"IS Start: {is_start_date}\")\nprint(f\"IS End: {is_end_date}\")\nprint(f\"OOS Start: {oos_start_date}\")\nprint(f\"OOS End: {oos_end_date}\")\n\n# Check no overlap\nassert is_end_date < oos_start_date, \"IS and OOS periods overlap!\"\n\n# Check dates match project backtest period\noriginal_period = (2019, 2023)\nassert (is_start, oos_end) == original_period, \"Dates don't match\"\n```\n\n**Error: \"API authentication failed\"**\n\n**Cause**: Missing or invalid QC credentials\n\n**Fix**:\n```bash\n# Check .env file\ncat .env\n\n# Should contain:\nQUANTCONNECT_USER_ID=your_user_id\nQUANTCONNECT_API_TOKEN=your_api_token\n\n# Get credentials from:\n# https://www.quantconnect.com/account\n```\n\n**Error: \"Project not found\"**\n\n**Cause**: project_id in iteration_state.json doesn't exist\n\n**Fix**:\n```bash\n# Verify project exists\npython -c \"from qc_api import QuantConnectAPI; \\\n           api = QuantConnectAPI(); \\\n           print(api.list_projects())\"\n\n# If deleted, run /qc-backtest to create new one\n```",
      "tags": [
        "errors",
        "troubleshooting",
        "fixes"
      ],
      "priority": 2,
      "related_sections": [
        "workflow_overview",
        "qc_platform_access"
      ]
    }
  ],
  "examples": [
    {
      "title": "Excellent Validation Result (DEPLOY)",
      "description": "Minimal degradation, high robustness - deploy with confidence",
      "code": "In-Sample (2019-2022):\n  Sharpe: 0.95\n  Drawdown: 18%\n  Trades: 125\n  Win Rate: 48%\n\nOut-of-Sample (2023):\n  Sharpe: 0.88\n  Drawdown: 21%\n  Trades: 34\n  Win Rate: 46%\n\nCalculations:\n  Degradation: (0.95 - 0.88) / 0.95 = 7.4%\n  Robustness: 0.88 / 0.95 = 0.93",
      "output": "Decision: DEPLOY_STRATEGY ‚úÖ\n\nReasoning:\n  ‚úÖ Degradation 7.4% < 15% threshold (excellent)\n  ‚úÖ Robustness 0.93 > 0.75 threshold (high)\n  ‚úÖ OOS Sharpe 0.88 > 0.7 threshold (good)\n  ‚úÖ OOS Trades 34 > 10 threshold (sufficient)\n  ‚úÖ Trade rate consistent (125/4 ‚âà 31/yr, 34/1 = 34/yr)\n  ‚úÖ Win rate stable (48% ‚Üí 46%)\n\nNext Steps:\n  - Paper trade 6-12 months\n  - Initial position size: 50%\n  - Monitor closely first 3 months\n  - Increase to 100% if performance holds",
      "tags": [
        "excellent",
        "deploy",
        "validation"
      ]
    },
    {
      "title": "Acceptable Validation (CAUTION)",
      "description": "Moderate degradation - deploy with monitoring",
      "code": "In-Sample (2019-2022):\n  Sharpe: 0.88\n  Trades: 95\n\nOut-of-Sample (2023):\n  Sharpe: 0.65\n  Trades: 18\n\nCalculations:\n  Degradation: (0.88 - 0.65) / 0.88 = 26.1%\n  Robustness: 0.65 / 0.88 = 0.74",
      "output": "Decision: PROCEED_WITH_CAUTION ‚ö†Ô∏è\n\nReasoning:\n  ‚úÖ Degradation 26% < 30% threshold (acceptable)\n  ‚ö†Ô∏è Robustness 0.74 ‚âà 0.75 threshold (borderline)\n  ‚úÖ OOS Sharpe 0.65 > 0.5 threshold (acceptable)\n  ‚úÖ OOS Trades 18 > 10 threshold (sufficient)\n\nConcerns:\n  - Degradation in higher range\n  - Robustness barely meets threshold\n\nNext Steps:\n  - Paper trade 18-24 months (longer period)\n  - Initial position size: 25% (conservative)\n  - Monitor very closely\n  - Be ready to stop if live performance degrades further",
      "tags": [
        "acceptable",
        "caution",
        "validation"
      ]
    },
    {
      "title": "Failed Validation (ABANDON)",
      "description": "Severe degradation - clear overfitting",
      "code": "In-Sample (2019-2022):\n  Sharpe: 1.20\n  Trades: 145\n  Win Rate: 62%\n\nOut-of-Sample (2023):\n  Sharpe: 0.38\n  Trades: 12\n  Win Rate: 42%\n\nCalculations:\n  Degradation: (1.20 - 0.38) / 1.20 = 68.3%\n  Robustness: 0.38 / 1.20 = 0.32",
      "output": "Decision: ABANDON_HYPOTHESIS ‚ùå\n\nRed Flags:\n  ‚ùå Degradation 68% >> 40% threshold (severe)\n  ‚ùå Robustness 0.32 << 0.5 threshold (very low)\n  ‚ùå Trade count dropped 92% (145/4yr ‚Üí 12/1yr)\n  ‚ùå Win rate dropped 20% (62% ‚Üí 42%)\n\nDiagnosis: Clear overfitting to in-sample period\n- High IS Sharpe (1.20) suspicious\n- Parameters fit to IS noise\n- Strategy doesn't generalize\n\nNext Steps:\n  - Do NOT deploy\n  - Analyze failure patterns\n  - Design simpler hypothesis\n  - Ensure sufficient OOS trades\n  - Avoid over-optimization",
      "tags": [
        "failed",
        "abandon",
        "overfitting"
      ]
    },
    {
      "title": "Borderline Result (ESCALATE)",
      "description": "Mixed signals - human review needed",
      "code": "In-Sample (2019-2022):\n  Sharpe: 0.85\n  Trades: 88\n\nOut-of-Sample (2023):\n  Sharpe: 0.58\n  Trades: 8\n\nCalculations:\n  Degradation: (0.85 - 0.58) / 0.85 = 31.8%\n  Robustness: 0.58 / 0.85 = 0.68",
      "output": "Decision: ESCALATE_TO_HUMAN üîç\n\nBorderline Issues:\n  ‚ö†Ô∏è Degradation 32% > 30% threshold (slightly over)\n  ‚ö†Ô∏è Robustness 0.68 < 0.75 but > 0.60 (borderline)\n  ‚ö†Ô∏è OOS Trades 8 < 10 threshold (insufficient)\n\nMixed Signals:\n  - Degradation just over threshold\n  - OOS Sharpe still positive (0.58)\n  - Too few OOS trades for confidence\n\nHuman Review Needed:\n  - Extend OOS period for more trades?\n  - Accept limited data and proceed with caution?\n  - Require longer paper trading?\n  - Abandon and redesign?\n\nRecommendation: Extend validation period or require 24+ months paper trading",
      "tags": [
        "borderline",
        "escalate",
        "review"
      ]
    }
  ],
  "faqs": [
    {
      "question": "Can I adjust my strategy based on out-of-sample results?",
      "answer": "NO. NEVER adjust strategy based on OOS results. This is the cardinal rule of validation. OOS is for TESTING only. If you adjust based on OOS, it becomes in-sample and validation is invalid. If OOS fails, either deploy baseline or abandon hypothesis - do not optimize for OOS.",
      "tags": [
        "critical",
        "oos",
        "contamination"
      ],
      "related_sections": [
        "walk_forward_basics",
        "decision_criteria"
      ]
    },
    {
      "question": "What if out-of-sample performance is negative (Sharpe < 0)?",
      "answer": "ABANDON_HYPOTHESIS immediately. Negative OOS performance means strategy loses money on unseen data - clear evidence of severe overfitting or strategy failure. Do not deploy. Analyze failure, design new hypothesis, and start over.",
      "tags": [
        "failure",
        "negative_performance"
      ],
      "related_sections": [
        "decision_criteria",
        "performance_degradation"
      ]
    },
    {
      "question": "How much degradation is normal?",
      "answer": "Research shows 33-50% degradation is NORMAL across strategies. However, for deployment: < 15% is excellent, 15-30% is acceptable, 30-40% requires human review, > 40% indicates abandoning. Lower degradation = more robust strategy.",
      "tags": [
        "degradation",
        "thresholds"
      ],
      "related_sections": [
        "performance_degradation",
        "walk_forward_basics"
      ]
    },
    {
      "question": "What if I only have 8 trades out-of-sample?",
      "answer": "ESCALATE_TO_HUMAN. Minimum 10 trades required for basic statistical significance. With < 10 trades: (1) Extend OOS period for more data, (2) Accept limitation and require longer paper trading, or (3) Use lower time resolution (e.g., hourly vs daily bars). 8 trades is insufficient for confident validation.",
      "tags": [
        "trades",
        "sample_size"
      ],
      "related_sections": [
        "trade_count_requirements",
        "common_errors"
      ]
    },
    {
      "question": "Should I run Monte Carlo validation?",
      "answer": "Only AFTER walk-forward validation passes. MC adds statistical confidence (PSR, DSR, MinTRL) but doesn't replace walk-forward. Workflow: (1) Walk-forward first, (2) If passed and want institutional validation, run MC, (3) If walk-forward failed, skip MC and fix strategy instead.",
      "tags": [
        "monte_carlo",
        "workflow"
      ],
      "related_sections": [
        "monte_carlo_validation",
        "workflow_overview"
      ]
    },
    {
      "question": "Can I create a new project for validation?",
      "answer": "NO. NEVER create new project in validation phase. MUST use project_id from iteration_state.json (created by /qc-backtest). Use existing project to maintain workflow continuity and validate correct strategy version. Always read project_id from iteration_state.json.",
      "tags": [
        "project_id",
        "critical",
        "workflow"
      ],
      "related_sections": [
        "workflow_overview",
        "qc_platform_access"
      ]
    }
  ],
  "related_tools": [
    "qc_backtest",
    "qc_optimize",
    "decision_framework",
    "backtesting_analysis"
  ],
  "metadata": {
    "created": "2025-11-13",
    "updated": "2025-11-13",
    "version": "2.3.2",
    "authors": [
      "Claude"
    ],
    "skill": "quantconnect-validation",
    "changelog": "v2.3.2: Added CRITICAL emoji ban rule - emojis cause syntax errors on QC platform. v2.3.1: Corrected naming rules. v2.3.0: Added QC coding standards"
  }
}