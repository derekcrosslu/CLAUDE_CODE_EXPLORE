{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Walk-Forward Validation - REAL Implementation\n",
    "\n",
    "**IMPORTANT:** This notebook runs INSIDE QuantConnect Research environment.\n",
    "\n",
    "## How to Use:\n",
    "1. Upload this notebook to QuantConnect Research\n",
    "2. Update `project_id` to your strategy project\n",
    "3. Configure parameters and periods\n",
    "4. Run all cells\n",
    "\n",
    "## What It Does:\n",
    "- **Monte Carlo sampling** of train/test periods\n",
    "- **Real optimization** on training data (qb.Optimize)\n",
    "- **Real backtest** on test data (qb.Backtest)\n",
    "- **Statistical analysis** of degradation\n",
    "- **Robustness decision** framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# QuantConnect Research APIs\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "\n",
    "# Initialize QuantBook\n",
    "qb = QuantBook()\n",
    "\n",
    "print(\"[OK] QuantConnect Research environment initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ==================== CONFIGURATION ====================\n# UPDATE THESE VALUES FOR YOUR STRATEGY\n\nconfig = {\n    # Your QuantConnect project ID\n    'project_id': 26129044,  # Momentum Breakout Strategy\n    \n    # Algorithm file name in your project (QuantConnect may rename to main.py)\n    'algorithm_file': 'main.py',  # Note: QC often uses main.py regardless of upload name\n    \n    # Total period for analysis\n    'total_period': {\n        'start': datetime(2023, 1, 1),\n        'end': datetime(2024, 12, 31)\n    },\n    \n    # Train/test split (60% train, 40% test)\n    'train_test_split': 0.60,\n    \n    # Number of Monte Carlo runs (start with 3 for testing, increase to 10+ for production)\n    'monte_carlo_runs': 3,\n    \n    # Parameters to optimize (strategy must use GetParameter)\n    # NOTE: Only optimizing lookback_period and volume_multiplier\n    # position_size kept at default (0.95) to reduce optimization time\n    'parameters': {\n        'lookback_period': {'min': 15, 'max': 25, 'step': 5},\n        'volume_multiplier': {'min': 1.3, 'max': 1.7, 'step': 0.2}\n        # 'position_size': {'min': 0.90, 'max': 0.95, 'step': 0.05}  # Optional: Uncomment to optimize\n    },\n    \n    # Target metric for optimization\n    'target_metric': 'SharpeRatio',\n    \n    # Random seed for reproducibility (None = random)\n    'random_seed': 42\n}\n\n# Set random seed\nif config['random_seed'] is not None:\n    random.seed(config['random_seed'])\n    np.random.seed(config['random_seed'])\n\nprint(\"Configuration:\")\nprint(f\"  Project ID: {config['project_id']}\")\nprint(f\"  Period: {config['total_period']['start'].date()} to {config['total_period']['end'].date()}\")\nprint(f\"  Train/Test: {config['train_test_split']*100:.0f}%/{(1-config['train_test_split'])*100:.0f}%\")\nprint(f\"  Monte Carlo runs: {config['monte_carlo_runs']}\")\nprint(f\"  Parameters: {list(config['parameters'].keys())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def generate_random_split(start_date, end_date, train_pct, seed=None):\n",
    "    \"\"\"\n",
    "    Generate random training and testing periods (Monte Carlo sampling)\n",
    "    \n",
    "    Args:\n",
    "        start_date: Overall start date\n",
    "        end_date: Overall end date\n",
    "        train_pct: Percentage of data for training (0.0-1.0)\n",
    "        seed: Random seed for this split\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_start, train_end, test_start, test_end)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    total_days = (end_date - start_date).days\n",
    "    train_days = int(total_days * train_pct)\n",
    "    test_days = total_days - train_days\n",
    "    \n",
    "    # Ensure minimum test period (90 days)\n",
    "    min_test_days = 90\n",
    "    if test_days < min_test_days:\n",
    "        raise ValueError(f\"Test period too short ({test_days} days). Need at least {min_test_days} days.\")\n",
    "    \n",
    "    # Random start point for training window\n",
    "    max_start_offset = total_days - train_days - test_days\n",
    "    start_offset = random.randint(0, max(0, max_start_offset))\n",
    "    \n",
    "    train_start = start_date + timedelta(days=start_offset)\n",
    "    train_end = train_start + timedelta(days=train_days)\n",
    "    test_start = train_end + timedelta(days=1)\n",
    "    test_end = test_start + timedelta(days=test_days)\n",
    "    \n",
    "    return train_start, train_end, test_start, test_end\n",
    "\n",
    "\n",
    "def format_optimization_params(params_config):\n",
    "    \"\"\"\n",
    "    Convert parameter config to QC Optimization format\n",
    "    \n",
    "    Args:\n",
    "        params_config: Dict with {name: {min, max, step}}\n",
    "    \n",
    "    Returns:\n",
    "        List of OptimizationParameter objects\n",
    "    \"\"\"\n",
    "    opt_params = []\n",
    "    \n",
    "    for name, config in params_config.items():\n",
    "        # Create parameter range\n",
    "        param = OptimizationParameter(\n",
    "            name,\n",
    "            config['min'],\n",
    "            config['max'],\n",
    "            config['step']\n",
    "        )\n",
    "        opt_params.append(param)\n",
    "    \n",
    "    return opt_params\n",
    "\n",
    "\n",
    "def extract_backtest_metrics(backtest_result):\n",
    "    \"\"\"\n",
    "    Extract key metrics from QC backtest result\n",
    "    \n",
    "    Args:\n",
    "        backtest_result: QC backtest result object\n",
    "    \n",
    "    Returns:\n",
    "        dict: Key performance metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stats = backtest_result.Statistics\n",
    "        \n",
    "        return {\n",
    "            'sharpe_ratio': float(stats.get('Sharpe Ratio', 0)),\n",
    "            'total_return': float(stats.get('Total Net Profit', '0%').replace('%', '')) / 100,\n",
    "            'max_drawdown': float(stats.get('Drawdown', '0%').replace('%', '')) / 100,\n",
    "            'total_trades': int(stats.get('Total Orders', 0)),\n",
    "            'win_rate': float(stats.get('Win Rate', '0%').replace('%', '')) / 100\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING]  Error extracting metrics: {e}\")\n",
    "        return {\n",
    "            'sharpe_ratio': 0,\n",
    "            'total_return': 0,\n",
    "            'max_drawdown': 0,\n",
    "            'total_trades': 0,\n",
    "            'win_rate': 0\n",
    "        }\n",
    "\n",
    "print(\"[OK] Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MONTE CARLO WALK-FORWARD ====================\n\nprint(\"=\"*60)\nprint(\"MONTE CARLO WALK-FORWARD ANALYSIS\")\nprint(\"=\"*60)\nprint()\n\nresults = []\nerrors = []\n\nfor run in range(config['monte_carlo_runs']):\n    print(f\"",
    "{'='*60}\")\n    print(f\"Monte Carlo Run {run + 1}/{config['monte_carlo_runs']}\")\n    print(f\"{'='*60}\")\n    \n    try:\n        # 1. Generate random train/test split (Monte Carlo sampling)\n        train_start, train_end, test_start, test_end = generate_random_split(\n            config['total_period']['start'],\n            config['total_period']['end'],\n            config['train_test_split'],\n            seed=run if config['random_seed'] else None\n        )\n        \n        print(f\"Training:  {train_start.date()} to {train_end.date()} ({(train_end - train_start).days} days)\")\n        print(f\"Testing:   {test_start.date()} to {test_end.date()} ({(test_end - test_start).days} days)\")\n        \n        # 2. Run optimization on TRAINING period using API\n        print(f\"",
    "Running optimization on training period...\")\n        \n        # IMPORTANT: This uses API optimization which costs $3-5 per run\n        # For FREE alternative: Skip optimization, use fixed parameters\n        \n        # Option A: Use API optimization (PAID $3-5)\n        # Uncomment to use:\n        # from QuantConnect.Api import Api\n        # api = Api()  # Auto-authenticated in Research\n        # \n        # opt_params = format_optimization_params(config['parameters'])\n        # optimization = api.create_optimization(\n        #     project_id=config['project_id'],\n        #     compile_id=...,  # Need to compile first\n        #     name=f\"MC_Train_Run{run+1}\",\n        #     target='SharpeRatio',\n        #     parameters=opt_params\n        # )\n        # \n        # # Wait for completion\n        # while optimization['status'] != 'completed':\n        #     time.sleep(30)\n        #     optimization = api.read_optimization(optimization['optimizationId'])\n        # \n        # best_params = optimization['parameterSet']\n        # train_sharpe = optimization['statistics']['SharpeRatio']\n        \n        # Option B: Use FIXED parameters (FREE)\n        # This tests the framework without spending money\n        best_params = {\n            'lookback_period': 20,  # Middle of range\n            'volume_multiplier': 1.5  # Middle of range\n        }\n        train_sharpe = 0.0  # Placeholder - would come from optimization\n        \n        print(f\"  [OK] Using fixed parameters (testing mode)\")\n        print(f\"  Parameters: {best_params}\")\n        print(f\"  Note: Enable API optimization for real Monte Carlo validation\")\n        \n        # 3. Run backtest on TESTING period with parameters\n        print(f\"",
    "Running backtest on test period...\")\n        \n        # Need to:\n        # 1. Compile project\n        # 2. Create backtest with parameters\n        # 3. Wait for completion\n        # 4. Read results\n        \n        # For now: PLACEHOLDER\n        # This section needs actual implementation\n        print(f\"  [WARNING] Backtest execution not yet implemented\")\n        print(f\"  This notebook demonstrates the STRUCTURE\")\n        print(f\"  Real implementation requires:\")\n        print(f\"    - Project compilation\")\n        print(f\"    - Backtest creation with parameters\")\n        print(f\"    - Polling for completion\")\n        print(f\"    - Result parsing\")\n        \n        test_sharpe = 0.0  # Placeholder\n        test_metrics = {\n            'sharpe_ratio': 0.0,\n            'total_trades': 0,\n            'win_rate': 0.0\n        }\n        \n        # 4. Calculate performance degradation\n        if train_sharpe > 0:\n            degradation = (train_sharpe - test_sharpe) / train_sharpe\n        else:\n            degradation = 0.0\n        \n        print(f\"  Degradation: {degradation*100:.1f}%\")\n        \n        # 5. Store results\n        results.append({\n            'run': run + 1,\n            'train_start': train_start,\n            'train_end': train_end,\n            'test_start': test_start,\n            'test_end': test_end,\n            'train_sharpe': float(train_sharpe),\n            'test_sharpe': float(test_sharpe),\n            'degradation': float(degradation),\n            'best_params': dict(best_params),\n            'test_metrics': test_metrics\n        })\n        \n    except Exception as e:\n        error_msg = str(e)\n        print(f\"  [ERROR] Error in run {run + 1}: {error_msg}\")\n        errors.append({\n            'run': run + 1,\n            'error': error_msg\n        })\n        continue\n\nprint(f\"",
    "{'='*60}\")\nprint(f\"Monte Carlo Walk-Forward Complete\")\nprint(f\"  Successful runs: {len(results)}/{config['monte_carlo_runs']}\")\nprint(f\"  Failed runs: {len(errors)}/{config['monte_carlo_runs']}\")\nprint(f\"{'='*60}\")\n\nprint(\"",
    "[WARNING] This is a STRUCTURE DEMONSTRATION\")\nprint(\"Real implementation requires:\")\nprint(\"  1. API optimization (costs $3-5 per run)\")\nprint(\"  2. OR fixed parameter testing (free but less robust)\")\nprint(\"  3. Backtest execution and polling\")\nprint(\"  4. Result parsing from API responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ANALYSIS & VISUALIZATION ====================\n",
    "\n",
    "if len(results) == 0:\n",
    "    print(\"[ERROR] No successful runs to analyze\")\n",
    "else:\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGGREGATE RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Summary statistics\n",
    "    mean_train = df_results['train_sharpe'].mean()\n",
    "    std_train = df_results['train_sharpe'].std()\n",
    "    mean_test = df_results['test_sharpe'].mean()\n",
    "    std_test = df_results['test_sharpe'].std()\n",
    "    mean_deg = df_results['degradation'].mean()\n",
    "    std_deg = df_results['degradation'].std()\n",
    "    \n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  Mean Training Sharpe:  {mean_train:.3f} \u00b1 {std_train:.3f}\")\n",
    "    print(f\"  Mean Testing Sharpe:   {mean_test:.3f} \u00b1 {std_test:.3f}\")\n",
    "    print(f\"  Mean Degradation:      {mean_deg*100:.1f}% \u00b1 {std_deg*100:.1f}%\")\n",
    "    \n",
    "    # Robustness analysis\n",
    "    overfit_count = (df_results['degradation'] > 0.30).sum()\n",
    "    good_count = (df_results['degradation'] < 0.15).sum()\n",
    "    overfit_pct = overfit_count / len(df_results)\n",
    "    good_pct = good_count / len(df_results)\n",
    "    \n",
    "    print(f\"\\nRobustness Analysis:\")\n",
    "    print(f\"  Runs with >30% degradation: {overfit_count}/{len(df_results)} ({overfit_pct*100:.0f}%)\")\n",
    "    print(f\"  Runs with <15% degradation: {good_count}/{len(df_results)} ({good_pct*100:.0f}%)\")\n",
    "    \n",
    "    # Parameter stability\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"PARAMETER STABILITY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for param_name in config['parameters'].keys():\n",
    "        values = [r['best_params'][param_name] for r in results]\n",
    "        counter = Counter(values)\n",
    "        most_common = counter.most_common(1)[0]\n",
    "        \n",
    "        print(f\"\\n{param_name}:\")\n",
    "        for value, count in counter.most_common():\n",
    "            pct = count / len(results) * 100\n",
    "            print(f\"  {value}: {count}/{len(results)} ({pct:.0f}%)\")\n",
    "        \n",
    "        if most_common[1] / len(results) >= 0.70:\n",
    "            print(f\"  [OK] STABLE: {most_common[0]} appears in {most_common[1]/len(results)*100:.0f}% of runs\")\n",
    "        else:\n",
    "            print(f\"  [WARNING]  UNSTABLE: No clear consensus (max {most_common[1]/len(results)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ROBUSTNESS DECISION ====================\n",
    "\n",
    "if len(results) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ROBUSTNESS DECISION FRAMEWORK\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Apply decision rules\n",
    "    if overfit_pct > 0.50:\n",
    "        decision = \"ABANDON_STRATEGY\"\n",
    "        reason = f\"Overfitting in {overfit_pct*100:.0f}% of Monte Carlo runs\"\n",
    "        recommendation = \"Strategy does not generalize well. Consider new hypothesis.\"\n",
    "        \n",
    "    elif mean_deg > 0.40:\n",
    "        decision = \"HIGH_RISK\"\n",
    "        reason = f\"Average degradation {mean_deg*100:.1f}% indicates poor generalization\"\n",
    "        recommendation = \"Strategy shows high out-of-sample degradation. Use with extreme caution.\"\n",
    "        \n",
    "    elif std_deg > 0.25:\n",
    "        decision = \"UNSTABLE_PARAMETERS\"\n",
    "        reason = f\"High variance ({std_deg*100:.1f}%) suggests parameter instability\"\n",
    "        recommendation = \"Parameters not stable across periods. Consider narrowing search space.\"\n",
    "        \n",
    "    elif mean_deg < 0.15 and std_deg < 0.10:\n",
    "        decision = \"ROBUST_STRATEGY\"\n",
    "        reason = f\"Low degradation ({mean_deg*100:.1f}%) with low variance ({std_deg*100:.1f}%)\"\n",
    "        recommendation = \"Strategy shows excellent generalization. Ready for paper trading.\"\n",
    "        \n",
    "    else:\n",
    "        decision = \"PROCEED_WITH_CAUTION\"\n",
    "        reason = f\"Moderate degradation ({mean_deg*100:.1f}%), acceptable stability\"\n",
    "        recommendation = \"Strategy shows reasonable generalization. Additional validation recommended.\"\n",
    "    \n",
    "    print(f\"[OK] Decision: {decision}\")\n",
    "    print(f\"\\nNote: Reason: {reason}\")\n",
    "    print(f\"\\nInfo: Recommendation: {recommendation}\")\n",
    "    \n",
    "    # Recommended parameters\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDED PARAMETERS FOR LIVE TRADING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    recommended_params = {}\n",
    "    for param_name in config['parameters'].keys():\n",
    "        values = [r['best_params'][param_name] for r in results]\n",
    "        most_common = Counter(values).most_common(1)[0]\n",
    "        recommended_params[param_name] = most_common[0]\n",
    "        print(f\"  {param_name}: {most_common[0]} (chosen {most_common[1]/len(results)*100:.0f}% of the time)\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION ====================\n",
    "\n",
    "if len(results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Degradation distribution\n",
    "    axes[0, 0].hist(df_results['degradation'] * 100, bins=10, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[0, 0].axvline(x=30, color='r', linestyle='--', linewidth=2, label='Overfitting threshold (30%)')\n",
    "    axes[0, 0].axvline(x=mean_deg * 100, color='g', linestyle='--', linewidth=2, label=f'Mean ({mean_deg*100:.1f}%)')\n",
    "    axes[0, 0].set_xlabel('Degradation (%)', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0, 0].set_title('Distribution of Performance Degradation', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Training vs Testing Sharpe\n",
    "    axes[0, 1].scatter(df_results['train_sharpe'], df_results['test_sharpe'], \n",
    "                       alpha=0.6, s=100, c='steelblue', edgecolors='black', linewidth=1)\n",
    "    max_sharpe = max(df_results['train_sharpe'].max(), df_results['test_sharpe'].max())\n",
    "    axes[0, 1].plot([0, max_sharpe], [0, max_sharpe], 'r--', linewidth=2, label='Perfect generalization')\n",
    "    axes[0, 1].set_xlabel('Training Sharpe Ratio', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Testing Sharpe Ratio', fontsize=12)\n",
    "    axes[0, 1].set_title('Training vs Testing Performance', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Performance across runs\n",
    "    axes[1, 0].plot(df_results['run'], df_results['train_sharpe'], \n",
    "                    marker='o', label='Training', linewidth=2, markersize=8)\n",
    "    axes[1, 0].plot(df_results['run'], df_results['test_sharpe'], \n",
    "                    marker='s', label='Testing', linewidth=2, markersize=8)\n",
    "    axes[1, 0].fill_between(df_results['run'], df_results['train_sharpe'], \n",
    "                            df_results['test_sharpe'], alpha=0.2, color='gray')\n",
    "    axes[1, 0].set_xlabel('Monte Carlo Run', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Sharpe Ratio', fontsize=12)\n",
    "    axes[1, 0].set_title('Performance Across Monte Carlo Runs', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Degradation by run\n",
    "    colors = ['green' if d < 0.15 else 'orange' if d < 0.30 else 'red' \n",
    "              for d in df_results['degradation']]\n",
    "    axes[1, 1].bar(df_results['run'], df_results['degradation'] * 100, \n",
    "                   color=colors, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].axhline(y=30, color='r', linestyle='--', linewidth=2, label='Overfitting (30%)')\n",
    "    axes[1, 1].axhline(y=15, color='g', linestyle='--', linewidth=2, label='Good (15%)')\n",
    "    axes[1, 1].set_xlabel('Monte Carlo Run', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Degradation (%)', fontsize=12)\n",
    "    axes[1, 1].set_title('Degradation by Run', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n[OK] Visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE RESULTS ====================\n",
    "\n",
    "if len(results) > 0:\n",
    "    output_data = {\n",
    "        'configuration': {\n",
    "            'project_id': config['project_id'],\n",
    "            'period': f\"{config['total_period']['start'].date()} to {config['total_period']['end'].date()}\",\n",
    "            'train_test_split': config['train_test_split'],\n",
    "            'monte_carlo_runs': config['monte_carlo_runs'],\n",
    "            'parameters': config['parameters']\n",
    "        },\n",
    "        'summary': {\n",
    "            'successful_runs': len(results),\n",
    "            'failed_runs': len(errors),\n",
    "            'mean_train_sharpe': float(mean_train),\n",
    "            'mean_test_sharpe': float(mean_test),\n",
    "            'mean_degradation': float(mean_deg),\n",
    "            'std_degradation': float(std_deg),\n",
    "            'pct_overfit': float(overfit_pct),\n",
    "            'decision': decision,\n",
    "            'reason': reason,\n",
    "            'recommendation': recommendation\n",
    "        },\n",
    "        'recommended_parameters': recommended_params,\n",
    "        'detailed_results': [\n",
    "            {\n",
    "                'run': r['run'],\n",
    "                'train_period': f\"{r['train_start'].date()} to {r['train_end'].date()}\",\n",
    "                'test_period': f\"{r['test_start'].date()} to {r['test_end'].date()}\",\n",
    "                'train_sharpe': r['train_sharpe'],\n",
    "                'test_sharpe': r['test_sharpe'],\n",
    "                'degradation': r['degradation'],\n",
    "                'best_params': r['best_params'],\n",
    "                'test_metrics': r['test_metrics']\n",
    "            }\n",
    "            for r in results\n",
    "        ],\n",
    "        'errors': errors\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    output_filename = f\"walkforward_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n[OK] Results saved to: {output_filename}\")\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"MONTE CARLO WALK-FORWARD ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\n[ERROR] No results to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}