{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Monte Carlo Validation\n",
    "\n",
    "**Project ID:** 26204235\n",
    "\n",
    "This notebook implements advanced Monte Carlo validation metrics:\n",
    "- **PSR (Probabilistic Sharpe Ratio):** ≥0.95 threshold\n",
    "- **DSR (Deflated Sharpe Ratio):** Multiple testing correction\n",
    "- **MinTRL (Minimum Track Record Length):** Required observations\n",
    "- **Bootstrap Resampling:** 1,000 runs\n",
    "- **Permutation Testing:** p < 0.05\n",
    "- **MC Drawdown Distribution:** 99th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize QuantBook\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import json\n",
    "\n",
    "qb = QuantBook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load backtest results\nfrom QuantConnect.Api import Api\nfrom datetime import datetime, timezone\n\napi = Api()\nproject_id = qb.project_id\n\n# Backtest ID from H7 Statistical Arbitrage backtest\nbacktest_id = '67dd62a13c9acfba69bb3493'\n\nprint(f'Project ID: {project_id}')\nprint(f'Using backtest: {backtest_id}')\n\ndef read_equity_curve(project_id, backtest_id):\n    \"\"\"\n    Extract equity curve from QC backtest results.\n    Pattern verified from working QC Research notebooks.\n    \"\"\"\n    bt = api.read_backtest(project_id, backtest_id)\n    res = getattr(bt, \"result\", {}) or {}\n\n    # Walk nested structure to find equity series\n    def _walk_series(o, name_hint=None):\n        if isinstance(o, dict):\n            vals = o.get(\"Values\") or o.get(\"values\")\n            if isinstance(vals, list) and len(vals)>0 and all(isinstance(v, (dict,)) for v in vals):\n                nm = (o.get(\"Name\") or o.get(\"name\") or name_hint or \"\")\n                yield (nm.lower(), vals)\n            ser = o.get(\"Series\") or o.get(\"series\")\n            if isinstance(ser, dict):\n                for v in ser.values():\n                    yield from _walk_series(v, o.get(\"Name\") or o.get(\"name\"))\n            if isinstance(ser, list):\n                for v in ser:\n                    yield from _walk_series(v, o.get(\"Name\") or o.get(\"name\"))\n            for k,v in o.items():\n                if k not in (\"Series\",\"series\",\"Values\",\"values\",\"Name\",\"name\"):\n                    yield from _walk_series(v, k)\n        elif isinstance(o, list):\n            for it in o:\n                yield from _walk_series(it, name_hint)\n\n    cands = list(_walk_series(res))\n    chosen = None\n\n    # Find equity series\n    for nm, vals in cands:\n        if \"equity\" in nm or (\"portfolio\" in nm and \"value\" in nm):\n            chosen = vals\n            break\n\n    # Fallback: use longest series\n    if chosen is None and cands:\n        chosen = max(cands, key=lambda t: len(t[1]))[1]\n\n    if not chosen:\n        raise ValueError(\"No equity curve data found in backtest results\")\n\n    # Parse data points\n    rows = []\n    for p in chosen:\n        x = p.get(\"x\") or p.get(\"time\") or p.get(\"Time\")\n        y = p.get(\"y\") or p.get(\"value\") or p.get(\"Value\")\n        if x is None or y is None:\n            continue\n        ts = datetime.fromtimestamp(x/1000.0, tz=timezone.utc) if isinstance(x,(int,float)) else pd.to_datetime(x, utc=True)\n        rows.append((ts, float(y)))\n\n    return pd.DataFrame(rows, columns=[\"TimeTS\",\"Equity\"]).sort_values(\"TimeTS\")\n\n# Extract equity curve\nequity_df = read_equity_curve(project_id, backtest_id)\n\n# Calculate returns\nreturns = equity_df['Equity'].pct_change().dropna()\n\nprint(f'Loaded {len(equity_df)} equity data points')\nprint(f'Date range: {equity_df[\"TimeTS\"].min()} to {equity_df[\"TimeTS\"].max()}')\nprint(f'Total returns: {len(returns)} observations')\nprint(f'Mean return: {returns.mean():.6f}')\nprint(f'Std return: {returns.std():.6f}')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probabilistic Sharpe Ratio (PSR)\n",
    "\n",
    "PSR calculates the probability that the true Sharpe ratio exceeds a benchmark.\n",
    "\n",
    "**Threshold:** ≥0.95 (95% confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calculate_psr(returns, benchmark_sr=0.0):\n    \"\"\"Calculate Probabilistic Sharpe Ratio\"\"\"\n    n = len(returns)\n    sr = returns.mean() / returns.std() * np.sqrt(252)  # Annualized\n    skew = stats.skew(returns)\n    kurt = stats.kurtosis(returns)\n    \n    # Standard error of Sharpe ratio (adjusted for non-normality)\n    se_sr = np.sqrt((1 + (sr**2)/2 - skew*sr + ((kurt-3)/4)*(sr**2)) / (n-1))\n    \n    # PSR\n    psr = stats.norm.cdf((sr - benchmark_sr) / se_sr)\n    \n    return psr, sr, skew, kurt\n\npsr, sharpe, skew, kurt = calculate_psr(returns)\n\nprint(f'Sharpe Ratio: {sharpe:.3f}')\nprint(f'Skewness: {skew:.3f}')\nprint(f'Kurtosis: {kurt:.3f}')\nprint(f'PSR: {psr:.4f}')\nprint(f\"Status: {'PASS' if psr >= 0.95 else 'FAIL'} (threshold: 0.95)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deflated Sharpe Ratio (DSR)\n",
    "\n",
    "DSR corrects for multiple testing bias (trying many strategies/parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calculate_dsr(returns, n_trials=10, benchmark_sr=0.0):\n    \"\"\"Calculate Deflated Sharpe Ratio\"\"\"\n    n = len(returns)\n    sr = returns.mean() / returns.std() * np.sqrt(252)\n    skew = stats.skew(returns)\n    kurt = stats.kurtosis(returns)\n    \n    # Variance of Sharpe ratio\n    var_sr = (1 + (sr**2)/2 - skew*sr + ((kurt-3)/4)*(sr**2)) / (n-1)\n    \n    # Expected maximum Sharpe from n_trials (under null)\n    gamma = 0.5772  # Euler-Mascheroni constant\n    max_sr_expected = np.sqrt(var_sr) * ((1-gamma)*stats.norm.ppf(1-1/n_trials) + gamma*stats.norm.ppf(1-1/(n_trials*np.e)))\n    \n    # DSR\n    dsr = stats.norm.cdf((sr - max_sr_expected) / np.sqrt(var_sr))\n    \n    return dsr\n\n# Assume 10 trials (conservative estimate)\ndsr = calculate_dsr(returns, n_trials=10)\n\nprint(f'DSR: {dsr:.4f}')\nprint(f\"Status: {'PASS' if dsr >= 0.95 else 'MARGINAL' if dsr >= 0.90 else 'FAIL'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Minimum Track Record Length (MinTRL)\n",
    "\n",
    "Required number of observations for statistical confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calculate_min_trl(returns, target_sr=1.0, confidence=0.95):\n    \"\"\"Calculate Minimum Track Record Length\"\"\"\n    sr = returns.mean() / returns.std() * np.sqrt(252)\n    skew = stats.skew(returns)\n    kurt = stats.kurtosis(returns)\n    \n    z = stats.norm.ppf(confidence)\n    \n    # MinTRL formula\n    min_trl = ((z / (sr - target_sr))**2) * (1 + (sr**2)/2 - skew*sr + ((kurt-3)/4)*(sr**2))\n    \n    return int(np.ceil(min_trl))\n\nmin_trl = calculate_min_trl(returns)\ncurrent_length = len(returns)\n\nprint(f'Current track record: {current_length} observations')\nprint(f'MinTRL required: {min_trl} observations')\nprint(f\"Status: {'SUFFICIENT' if current_length >= min_trl else 'INSUFFICIENT'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bootstrap Resampling (1,000 runs)\n",
    "\n",
    "Generate alternative equity curves to assess robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_returns(returns, n_simulations=1000):\n",
    "    \"\"\"Bootstrap resample returns\"\"\"\n",
    "    n = len(returns)\n",
    "    sharpe_dist = []\n",
    "    drawdown_dist = []\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Resample with replacement\n",
    "        resampled = np.random.choice(returns, size=n, replace=True)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sr = resampled.mean() / resampled.std() * np.sqrt(252)\n",
    "        sharpe_dist.append(sr)\n",
    "        \n",
    "        # Calculate drawdown\n",
    "        cum_returns = (1 + resampled).cumprod()\n",
    "        running_max = np.maximum.accumulate(cum_returns)\n",
    "        drawdown = (cum_returns - running_max) / running_max\n",
    "        max_dd = drawdown.min()\n",
    "        drawdown_dist.append(abs(max_dd))\n",
    "    \n",
    "    return np.array(sharpe_dist), np.array(drawdown_dist)\n",
    "\n",
    "print('Running 1,000 bootstrap simulations...')\n",
    "sharpe_dist, drawdown_dist = bootstrap_returns(returns, n_simulations=1000)\n",
    "\n",
    "print(f'\\nBootstrap Sharpe Distribution:')\n",
    "print(f'  Mean: {sharpe_dist.mean():.3f}')\n",
    "print(f'  Median: {np.median(sharpe_dist):.3f}')\n",
    "print(f'  5th percentile: {np.percentile(sharpe_dist, 5):.3f}')\n",
    "print(f'  95th percentile: {np.percentile(sharpe_dist, 95):.3f}')\n",
    "\n",
    "print(f'\\nBootstrap Drawdown Distribution:')\n",
    "print(f'  Mean: {drawdown_dist.mean():.1%}')\n",
    "print(f'  Median: {np.median(drawdown_dist):.1%}')\n",
    "print(f'  99th percentile (worst case): {np.percentile(drawdown_dist, 99):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Permutation Testing\n",
    "\n",
    "Test if results are statistically significant (p < 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def permutation_test(returns, n_permutations=1000):\n    \"\"\"Permutation test for statistical significance\"\"\"\n    observed_sr = returns.mean() / returns.std() * np.sqrt(252)\n    \n    # Shuffle returns and calculate Sharpe\n    permuted_srs = []\n    for _ in range(n_permutations):\n        shuffled = np.random.permutation(returns)\n        sr = shuffled.mean() / shuffled.std() * np.sqrt(252)\n        permuted_srs.append(sr)\n    \n    permuted_srs = np.array(permuted_srs)\n    \n    # p-value: proportion of permuted SRs >= observed SR\n    p_value = (np.sum(permuted_srs >= observed_sr) + 1) / (n_permutations + 1)\n    \n    return p_value\n\nprint('Running permutation test (1,000 permutations)...')\np_value = permutation_test(returns)\n\nprint(f'\\nPermutation Test:')\nprint(f'  p-value: {p_value:.4f}')\nprint(f\"  Status: {'SIGNIFICANT' if p_value < 0.05 else 'NOT SIGNIFICANT'} (threshold: p < 0.05)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Decision\n",
    "\n",
    "Based on all advanced Monte Carlo metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "results = {\n",
    "    'psr': float(psr),\n",
    "    'dsr': float(dsr),\n",
    "    'sharpe_ratio': float(sharpe),\n",
    "    'min_trl': int(min_trl),\n",
    "    'current_trl': int(len(returns)),\n",
    "    'bootstrap_sharpe_mean': float(sharpe_dist.mean()),\n",
    "    'bootstrap_sharpe_5th': float(np.percentile(sharpe_dist, 5)),\n",
    "    'bootstrap_drawdown_99th': float(np.percentile(drawdown_dist, 99)),\n",
    "    'permutation_pvalue': float(p_value),\n",
    "    'skewness': float(skew),\n",
    "    'kurtosis': float(kurt)\n",
    "}\n",
    "\n",
    "# Decision logic\n",
    "if psr < 0.95:\n",
    "    decision = 'FAILED_PSR'\n",
    "    reason = f'PSR {psr:.3f} < 0.95 (insufficient statistical significance)'\n",
    "elif p_value > 0.05:\n",
    "    decision = 'FAILED_PERMUTATION'\n",
    "    reason = f'p-value {p_value:.4f} > 0.05 (not statistically significant)'\n",
    "elif len(returns) < min_trl:\n",
    "    decision = 'INSUFFICIENT_DATA'\n",
    "    reason = f'Track record {len(returns)} < MinTRL {min_trl}'\n",
    "else:\n",
    "    decision = 'ROBUST_STRATEGY'\n",
    "    reason = f'PSR {psr:.3f}, p-value {p_value:.4f}, all tests passed'\n",
    "\n",
    "results['decision'] = decision\n",
    "results['reason'] = reason\n",
    "\n",
    "print('='*60)\n",
    "print('FINAL DECISION')\n",
    "print('='*60)\n",
    "print(f'Decision: {decision}')\n",
    "print(f'Reason: {reason}')\n",
    "print('\\nCopy the JSON below and paste when prompted by qc_validate collect-results:')\n",
    "print('='*60)\n",
    "print(json.dumps(results, indent=2))\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}