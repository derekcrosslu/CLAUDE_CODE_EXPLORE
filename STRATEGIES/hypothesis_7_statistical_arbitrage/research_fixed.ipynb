{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Monte Carlo Validation\n",
        "\n",
        "**Project ID:** 26204235\n",
        "\n",
        "This notebook implements advanced Monte Carlo validation metrics:\n",
        "- **PSR (Probabilistic Sharpe Ratio):** ≥0.95 threshold\n",
        "- **DSR (Deflated Sharpe Ratio):** Multiple testing correction\n",
        "- **MinTRL (Minimum Track Record Length):** Required observations\n",
        "- **Bootstrap Resampling:** 1,000 runs\n",
        "- **Permutation Testing:** p < 0.05\n",
        "- **MC Drawdown Distribution:** 99th percentile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize QuantBook\n",
        "from QuantConnect import *\n",
        "from QuantConnect.Research import *\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import json\n",
        "\n",
        "qb = QuantBook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get latest backtest for this project\n",
        "project_id = 26204235\n",
        "\n",
        "# List all backtests and get the most recent\n",
        "project = qb.ReadProject(project_id)\n",
        "backtests = project['backtests']\n",
        "\n",
        "if not backtests:\n",
        "    raise ValueError(\"No backtests found for this project!\")\n",
        "\n",
        "# Get most recent backtest\n",
        "latest_backtest_id = backtests[0]['backtestId']\n",
        "print(f'Using latest backtest: {latest_backtest_id}')\n",
        "\n",
        "# Fetch backtest\n",
        "backtest = qb.ReadBacktest(project_id, latest_backtest_id)\n",
        "\n",
        "# Extract equity curve\n",
        "equity_curve = backtest.Charts['Strategy Equity'].Series['Equity'].Values\n",
        "returns = pd.Series([p.y for p in equity_curve]).pct_change().dropna()\n",
        "\n",
        "# Extract trades\n",
        "trades = backtest.Orders\n",
        "\n",
        "print(f'Loaded {len(returns)} return observations')\n",
        "print(f'Total trades: {len(trades)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Probabilistic Sharpe Ratio (PSR)\n",
        "\n",
        "PSR calculates the probability that the true Sharpe ratio exceeds a benchmark.\n",
        "\n",
        "**Threshold:** ≥0.95 (95% confidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_psr(returns, benchmark_sr=0.0):\n",
        "    \"\"\"Calculate Probabilistic Sharpe Ratio\"\"\"\n",
        "    n = len(returns)\n",
        "    sr = returns.mean() / returns.std() * np.sqrt(252)  # Annualized\n",
        "    skew = stats.skew(returns)\n",
        "    kurt = stats.kurtosis(returns)\n",
        "    \n",
        "    # Standard error of Sharpe ratio (adjusted for non-normality)\n",
        "    se_sr = np.sqrt((1 + (sr**2)/2 - skew*sr + ((kurt-3)/4)*(sr**2)) / (n-1))\n",
        "    \n",
        "    # PSR\n",
        "    psr = stats.norm.cdf((sr - benchmark_sr) / se_sr)\n",
        "    \n",
        "    return psr, sr, skew, kurt\n",
        "\n",
        "psr, sharpe, skew, kurt = calculate_psr(returns)\n",
        "\n",
        "print(f'Sharpe Ratio: {sharpe:.3f}')\n",
        "print(f'Skewness: {skew:.3f}')\n",
        "print(f'Kurtosis: {kurt:.3f}')\n",
        "print(f'PSR: {psr:.4f}')\n",
        "print(f\"Status: {'✅ PASS' if psr >= 0.95 else '❌ FAIL'} (threshold: 0.95)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Deflated Sharpe Ratio (DSR)\n",
        "\n",
        "DSR corrects for multiple testing bias (trying many strategies/parameters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_dsr(returns, n_trials=10, benchmark_sr=0.0):\n",
        "    \"\"\"Calculate Deflated Sharpe Ratio\"\"\"\n",
        "    n = len(returns)\n",
        "    sr = returns.mean() / returns.std() * np.sqrt(252)\n",
        "    skew = stats.skew(returns)\n",
        "    kurt = stats.kurtosis(returns)\n",
        "    \n",
        "    # Variance of Sharpe ratio\n",
        "    var_sr = (1 + (sr**2)/2 - skew*sr + ((kurt-3)/4)*(sr**2)) / (n-1)\n",
        "    \n",
        "    # Expected maximum Sharpe from n_trials (under null)\n",
        "    gamma = 0.5772  # Euler-Mascheroni constant\n",
        "    max_sr_expected = np.sqrt(var_sr) * ((1-gamma)*stats.norm.ppf(1-1/n_trials) + gamma*stats.norm.ppf(1-1/(n_trials*np.e)))\n",
        "    \n",
        "    # DSR\n",
        "    dsr = stats.norm.cdf((sr - max_sr_expected) / np.sqrt(var_sr))\n",
        "    \n",
        "    return dsr\n",
        "\n",
        "# Assume 10 trials (conservative estimate)\n",
        "dsr = calculate_dsr(returns, n_trials=10)\n",
        "\n",
        "print(f'DSR: {dsr:.4f}')\n",
        "print(f\"Status: {'✅ PASS' if dsr >= 0.95 else '⚠️ MARGINAL' if dsr >= 0.90 else '❌ FAIL'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Minimum Track Record Length (MinTRL)\n",
        "\n",
        "Required number of observations for statistical confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_min_trl(returns, target_sr=1.0, confidence=0.95):\n",
        "    \"\"\"Calculate Minimum Track Record Length\"\"\"\n",
        "    sr = returns.mean() / returns.std() * np.sqrt(252)\n",
        "    skew = stats.skew(returns)\n",
        "    kurt = stats.kurtosis(returns)\n",
        "    \n",
        "    z = stats.norm.ppf(confidence)\n",
        "    \n",
        "    # MinTRL formula\n",
        "    min_trl = ((z / (sr - target_sr))**2) * (1 + (sr**2)/2 - skew*sr + ((kurt-3)/4)*(sr**2))\n",
        "    \n",
        "    return int(np.ceil(min_trl))\n",
        "\n",
        "min_trl = calculate_min_trl(returns)\n",
        "current_length = len(returns)\n",
        "\n",
        "print(f'Current track record: {current_length} observations')\n",
        "print(f'MinTRL required: {min_trl} observations')\n",
        "print(f\"Status: {'✅ SUFFICIENT' if current_length >= min_trl else '❌ INSUFFICIENT'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Bootstrap Resampling (1,000 runs)\n",
        "\n",
        "Generate alternative equity curves to assess robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bootstrap_returns(returns, n_simulations=1000):\n",
        "    \"\"\"Bootstrap resample returns\"\"\"\n",
        "    n = len(returns)\n",
        "    sharpe_dist = []\n",
        "    drawdown_dist = []\n",
        "    \n",
        "    for _ in range(n_simulations):\n",
        "        # Resample with replacement\n",
        "        resampled = np.random.choice(returns, size=n, replace=True)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        sr = resampled.mean() / resampled.std() * np.sqrt(252)\n",
        "        sharpe_dist.append(sr)\n",
        "        \n",
        "        # Calculate drawdown\n",
        "        cum_returns = (1 + resampled).cumprod()\n",
        "        running_max = np.maximum.accumulate(cum_returns)\n",
        "        drawdown = (cum_returns - running_max) / running_max\n",
        "        max_dd = drawdown.min()\n",
        "        drawdown_dist.append(abs(max_dd))\n",
        "    \n",
        "    return np.array(sharpe_dist), np.array(drawdown_dist)\n",
        "\n",
        "print('Running 1,000 bootstrap simulations...')\n",
        "sharpe_dist, drawdown_dist = bootstrap_returns(returns, n_simulations=1000)\n",
        "\n",
        "print(f'\\nBootstrap Sharpe Distribution:')\n",
        "print(f'  Mean: {sharpe_dist.mean():.3f}')\n",
        "print(f'  Median: {np.median(sharpe_dist):.3f}')\n",
        "print(f'  5th percentile: {np.percentile(sharpe_dist, 5):.3f}')\n",
        "print(f'  95th percentile: {np.percentile(sharpe_dist, 95):.3f}')\n",
        "\n",
        "print(f'\\nBootstrap Drawdown Distribution:')\n",
        "print(f'  Mean: {drawdown_dist.mean():.1%}')\n",
        "print(f'  Median: {np.median(drawdown_dist):.1%}')\n",
        "print(f'  99th percentile (worst case): {np.percentile(drawdown_dist, 99):.1%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Permutation Testing\n",
        "\n",
        "Test if results are statistically significant (p < 0.05)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def permutation_test(returns, n_permutations=1000):\n",
        "    \"\"\"Permutation test for statistical significance\"\"\"\n",
        "    observed_sr = returns.mean() / returns.std() * np.sqrt(252)\n",
        "    \n",
        "    # Shuffle returns and calculate Sharpe\n",
        "    permuted_srs = []\n",
        "    for _ in range(n_permutations):\n",
        "        shuffled = np.random.permutation(returns)\n",
        "        sr = shuffled.mean() / shuffled.std() * np.sqrt(252)\n",
        "        permuted_srs.append(sr)\n",
        "    \n",
        "    permuted_srs = np.array(permuted_srs)\n",
        "    \n",
        "    # p-value: proportion of permuted SRs >= observed SR\n",
        "    p_value = (np.sum(permuted_srs >= observed_sr) + 1) / (n_permutations + 1)\n",
        "    \n",
        "    return p_value\n",
        "\n",
        "print('Running permutation test (1,000 permutations)...')\n",
        "p_value = permutation_test(returns)\n",
        "\n",
        "print(f'\\nPermutation Test:')\n",
        "print(f'  p-value: {p_value:.4f}')\n",
        "print(f\"  Status: {'✅ SIGNIFICANT' if p_value < 0.05 else '❌ NOT SIGNIFICANT'} (threshold: p < 0.05)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Decision\n",
        "\n",
        "Based on all advanced Monte Carlo metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all results\n",
        "results = {\n",
        "    'psr': float(psr),\n",
        "    'dsr': float(dsr),\n",
        "    'sharpe_ratio': float(sharpe),\n",
        "    'min_trl': int(min_trl),\n",
        "    'current_trl': int(len(returns)),\n",
        "    'bootstrap_sharpe_mean': float(sharpe_dist.mean()),\n",
        "    'bootstrap_sharpe_5th': float(np.percentile(sharpe_dist, 5)),\n",
        "    'bootstrap_drawdown_99th': float(np.percentile(drawdown_dist, 99)),\n",
        "    'permutation_pvalue': float(p_value),\n",
        "    'skewness': float(skew),\n",
        "    'kurtosis': float(kurt)\n",
        "}\n",
        "\n",
        "# Decision logic\n",
        "if psr < 0.95:\n",
        "    decision = 'FAILED_PSR'\n",
        "    reason = f'PSR {psr:.3f} < 0.95 (insufficient statistical significance)'\n",
        "elif p_value > 0.05:\n",
        "    decision = 'FAILED_PERMUTATION'\n",
        "    reason = f'p-value {p_value:.4f} > 0.05 (not statistically significant)'\n",
        "elif len(returns) < min_trl:\n",
        "    decision = 'INSUFFICIENT_DATA'\n",
        "    reason = f'Track record {len(returns)} < MinTRL {min_trl}'\n",
        "else:\n",
        "    decision = 'ROBUST_STRATEGY'\n",
        "    reason = f'PSR {psr:.3f}, p-value {p_value:.4f}, all tests passed'\n",
        "\n",
        "results['decision'] = decision\n",
        "results['reason'] = reason\n",
        "\n",
        "print('='*60)\n",
        "print('FINAL DECISION')\n",
        "print('='*60)\n",
        "print(f'Decision: {decision}')\n",
        "print(f'Reason: {reason}')\n",
        "print('\\nCopy the JSON below and paste when prompted by qc_validate collect-results:')\n",
        "print('='*60)\n",
        "print(json.dumps(results, indent=2))\n",
        "print('='*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
