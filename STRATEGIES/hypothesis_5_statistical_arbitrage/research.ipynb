{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monte Carlo Walk-Forward Validation - Statistical Arbitrage Strategy\n",
        "\n",
        "**Strategy**: Hypothesis 5 - Statistical Arbitrage Pairs Trading  \n",
        "**Project ID**: 26140717  \n",
        "**Optimized Sharpe**: 1.829  \n",
        "**Baseline Sharpe**: 0.127  \n",
        "\n",
        "## Optimized Parameters to Validate:\n",
        "- z_entry_threshold: 1.5\n",
        "- z_exit_threshold: 1.0\n",
        "- lookback_period: 30\n",
        "- position_size_per_pair: 0.40\n",
        "- max_holding_days: 30\n",
        "- stop_loss_z: 4.0\n",
        "\n",
        "## Approach:\n",
        "Uses QuantBook to:\n",
        "1. Access historical data for pairs (PNC/KBE, ARCC/AMLP, RBA/SMFG, ENB/WEC)\n",
        "2. Run Monte Carlo splits (random train/test periods)\n",
        "3. Execute strategy logic locally in Python\n",
        "4. Calculate Sharpe ratio for each period\n",
        "5. Analyze degradation (train vs test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ QuantConnect Research environment initialized\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "from collections import Counter, deque\n",
        "import json\n",
        "\n",
        "# QuantConnect Research\n",
        "from QuantConnect import *\n",
        "from QuantConnect.Research import QuantBook\n",
        "\n",
        "# Initialize QuantBook\n",
        "qb = QuantBook()\n",
        "\n",
        "print(\"âœ“ QuantConnect Research environment initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Pairs: 4\n",
            "  Period: 2022-01-01 to 2024-12-31\n",
            "  Train/Test: 70%/30%\n",
            "  Monte Carlo runs: 20 (testing gradually toward 1000+)\n",
            "  Parameters: {'z_entry_threshold': 1.5, 'z_exit_threshold': 1.0, 'lookback_period': 30, 'position_size_per_pair': 0.4, 'max_holding_days': 30, 'stop_loss_z': 4.0}\n",
            "  Baseline Sharpe: 1.829\n"
          ]
        }
      ],
      "source": "# ==================== CONFIGURATION ====================\n\nconfig = {\n    'project_id': 26140717,\n    \n    # Pairs to trade\n    'pairs': [\n        {'long': 'PNC', 'short': 'KBE', 'name': 'PNC_KBE'},\n        {'long': 'ARCC', 'short': 'AMLP', 'name': 'ARCC_AMLP'},\n        {'long': 'RBA', 'short': 'SMFG', 'name': 'RBA_SMFG'},\n        {'long': 'ENB', 'short': 'WEC', 'name': 'ENB_WEC'}\n    ],\n    \n    # Total period for analysis\n    'total_period': {\n        'start': datetime(2022, 1, 1),\n        'end': datetime(2024, 12, 31)  # Use only historical data (no future dates)\n    },\n    \n    # Monte Carlo configuration\n    'train_test_split': 0.70,\n    'monte_carlo_runs': 100,  # Gradual scaling: 20 â†’ 50 â†’ 100 â†’ ... â†’ 1000+\n    'random_seed': 42,\n    \n    # Optimized parameters to test\n    'parameters': {\n        'z_entry_threshold': 1.5,\n        'z_exit_threshold': 1.0,\n        'lookback_period': 30,\n        'position_size_per_pair': 0.40,\n        'max_holding_days': 30,\n        'stop_loss_z': 4.0\n    },\n    \n    'baseline_sharpe': 1.829,\n    'initial_capital': 100000\n}\n\n# Set random seed\nif config['random_seed']:\n    random.seed(config['random_seed'])\n    np.random.seed(config['random_seed'])\n\nprint(\"Configuration:\")\nprint(f\"  Pairs: {len(config['pairs'])}\")\nprint(f\"  Period: {config['total_period']['start'].date()} to {config['total_period']['end'].date()}\")\nprint(f\"  Train/Test: {config['train_test_split']*100:.0f}%/{(1-config['train_test_split'])*100:.0f}%\")\nprint(f\"  Monte Carlo runs: {config['monte_carlo_runs']} (testing gradually toward 1000+)\")\nprint(f\"  Parameters: {config['parameters']}\")\nprint(f\"  Baseline Sharpe: {config['baseline_sharpe']:.3f}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subscribing to securities...\n",
            "  âœ“ PNC_KBE: PNC/KBE\n",
            "  âœ“ ARCC_AMLP: ARCC/AMLP\n",
            "  âœ“ RBA_SMFG: RBA/SMFG\n",
            "  âœ“ ENB_WEC: ENB/WEC\n",
            "\n",
            "âœ“ Subscribed to 4 pairs\n"
          ]
        }
      ],
      "source": [
        "# ==================== SUBSCRIBE TO SECURITIES ====================\n",
        "\n",
        "print(\"Subscribing to securities...\")\n",
        "\n",
        "symbols = {}\n",
        "for pair in config['pairs']:\n",
        "    long_sym = qb.AddEquity(pair['long'], Resolution.Daily).Symbol\n",
        "    short_sym = qb.AddEquity(pair['short'], Resolution.Daily).Symbol\n",
        "    symbols[pair['name']] = {'long': long_sym, 'short': short_sym}\n",
        "    print(f\"  âœ“ {pair['name']}: {pair['long']}/{pair['short']}\")\n",
        "\n",
        "print(f\"\\nâœ“ Subscribed to {len(symbols)} pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ==================== HELPER FUNCTIONS ====================\n\ndef generate_random_split(start_date, end_date, train_pct, seed=None):\n    \"\"\"Generate random train/test split for Monte Carlo - GUARANTEES test_end <= end_date\n\n    Strategy: Work backwards from end_date to ensure test period never exceeds boundary.\n    We randomly position a combined train+test window within the available range,\n    but ALWAYS anchor the test period to end no later than end_date.\n    \"\"\"\n    # Seed parameter kept for API compatibility but not used\n    # MC runs should use sequential random state, not reset per call\n\n    total_days = (end_date - start_date).days\n    train_days = int(total_days * train_pct)\n    test_days = total_days - train_days\n\n    # Ensure minimum test period (60 calendar days â‰ˆ 40-45 trading days)\n    min_test_days = 60\n    if test_days < min_test_days:\n        test_days = min_test_days\n        train_days = total_days - test_days\n\n    # NEW APPROACH: Position the test period randomly, but ALWAYS end at or before end_date\n    # Test period can start anywhere from (start_date + train_days + 1) to (end_date - test_days)\n    earliest_test_start = start_date + timedelta(days=train_days + 1)\n    latest_test_start = end_date - timedelta(days=test_days)\n\n    if latest_test_start < earliest_test_start:\n        # Not enough room - use sequential split\n        train_start = start_date\n        train_end = start_date + timedelta(days=train_days)\n        test_start = train_end + timedelta(days=1)\n        test_end = end_date  # GUARANTEED to be end_date\n    else:\n        # Random positioning of test start (but test always ends at or before end_date)\n        days_range = (latest_test_start - earliest_test_start).days\n        random_offset = random.randint(0, max(0, days_range))\n\n        test_start = earliest_test_start + timedelta(days=random_offset)\n        test_end = test_start + timedelta(days=test_days)\n\n        # CRITICAL: Ensure test_end does not exceed end_date\n        if test_end > end_date:\n            test_end = end_date\n            test_start = test_end - timedelta(days=test_days)\n\n        # Calculate training period (ends day before test starts)\n        train_end = test_start - timedelta(days=1)\n        train_start = train_end - timedelta(days=train_days)\n\n        # Ensure train_start doesn't go before start_date\n        if train_start < start_date:\n            train_start = start_date\n            train_end = test_start - timedelta(days=1)\n\n    # Final validation\n    assert test_end <= end_date, f\"BUG: test_end {test_end} exceeds end_date {end_date}\"\n    assert train_start >= start_date, f\"BUG: train_start {train_start} before start_date {start_date}\"\n\n    return train_start, train_end, test_start, test_end\n\n\ndef calculate_spread(long_prices, short_prices):\n    \"\"\"Calculate spread between two price series\"\"\"\n    return np.log(long_prices) - np.log(short_prices)\n\n\ndef calculate_zscore(spread, lookback):\n    \"\"\"Calculate z-score using rolling window\"\"\"\n    if len(spread) < lookback:\n        return pd.Series([np.nan] * len(spread), index=spread.index)\n    \n    rolling_mean = spread.rolling(window=lookback).mean()\n    rolling_std = spread.rolling(window=lookback).std(ddof=1)\n    \n    zscore = (spread - rolling_mean) / rolling_std\n    return zscore\n\n\ndef simulate_strategy(data, params):\n    \"\"\"\n    Simulate statistical arbitrage strategy on historical data\n    \n    Args:\n        data: Dict of DataFrames with price data for each pair\n        params: Strategy parameters\n    \n    Returns:\n        equity_curve: Daily portfolio values\n        trades: List of trade records\n    \"\"\"\n    capital = config['initial_capital']\n    equity_curve = []\n    trades = []\n    \n    # Get all dates (union of all pair dates)\n    all_dates = sorted(set().union(*[set(df.index) for df in data.values()]))\n    \n    # Track positions for each pair\n    positions = {pair['name']: None for pair in config['pairs']}\n    \n    for date in all_dates:\n        daily_pnl = 0\n        \n        # Process each pair\n        for pair in config['pairs']:\n            pair_name = pair['name']\n            if pair_name not in data:\n                continue\n                \n            df = data[pair_name]\n            \n            if date not in df.index:\n                continue\n            \n            # Get current prices and z-score\n            current_data = df.loc[:date]\n            if len(current_data) < params['lookback_period']:\n                continue\n            \n            long_price = df.loc[date, 'long_price']\n            short_price = df.loc[date, 'short_price']\n            z_score = df.loc[date, 'zscore']\n            \n            if np.isnan(z_score):\n                continue\n            \n            pos = positions[pair_name]\n            \n            # Check exit conditions\n            if pos is not None:\n                days_held = (date - pos['entry_date']).days\n                \n                # Calculate current P&L\n                if pos['direction'] == 'long_spread':\n                    pnl = (long_price / pos['entry_long'] - 1) * pos['long_shares'] * pos['entry_long']\n                    pnl -= (short_price / pos['entry_short'] - 1) * pos['short_shares'] * pos['entry_short']\n                else:\n                    pnl = (short_price / pos['entry_short'] - 1) * pos['short_shares'] * pos['entry_short']\n                    pnl -= (long_price / pos['entry_long'] - 1) * pos['long_shares'] * pos['entry_long']\n                \n                daily_pnl += pnl - pos['last_pnl']\n                pos['last_pnl'] = pnl\n                \n                # Exit conditions\n                exit_signal = False\n                exit_reason = None\n                \n                if abs(z_score) < params['z_exit_threshold']:\n                    exit_signal = True\n                    exit_reason = 'mean_reversion'\n                elif days_held >= params['max_holding_days']:\n                    exit_signal = True\n                    exit_reason = 'timeout'\n                elif abs(z_score) > params['stop_loss_z']:\n                    exit_signal = True\n                    exit_reason = 'stop_loss'\n                \n                if exit_signal:\n                    capital += pnl\n                    trades.append({\n                        'pair': pair_name,\n                        'entry_date': pos['entry_date'],\n                        'exit_date': date,\n                        'entry_z': pos['entry_z'],\n                        'exit_z': z_score,\n                        'pnl': pnl,\n                        'exit_reason': exit_reason,\n                        'days_held': days_held\n                    })\n                    positions[pair_name] = None\n            \n            # Check entry conditions (if no position)\n            if positions[pair_name] is None:\n                if abs(z_score) > params['z_entry_threshold']:\n                    # Calculate position sizes (dollar-neutral)\n                    pair_capital = capital * params['position_size_per_pair']\n                    \n                    if z_score > 0:  # Short spread (long short, short long)\n                        direction = 'short_spread'\n                        long_shares = pair_capital / (2 * long_price)\n                        short_shares = pair_capital / (2 * short_price)\n                    else:  # Long spread (long long, short short)\n                        direction = 'long_spread'\n                        long_shares = pair_capital / (2 * long_price)\n                        short_shares = pair_capital / (2 * short_price)\n                    \n                    positions[pair_name] = {\n                        'entry_date': date,\n                        'entry_z': z_score,\n                        'entry_long': long_price,\n                        'entry_short': short_price,\n                        'long_shares': long_shares,\n                        'short_shares': short_shares,\n                        'direction': direction,\n                        'last_pnl': 0\n                    }\n        \n        # Record equity\n        equity_curve.append({'date': date, 'equity': capital})\n    \n    return pd.DataFrame(equity_curve).set_index('date'), trades\n\n\ndef calculate_sharpe(equity_curve):\n    \"\"\"Calculate annualized Sharpe ratio\"\"\"\n    returns = equity_curve['equity'].pct_change().dropna()\n    if len(returns) == 0 or returns.std() == 0:\n        return 0.0\n    \n    sharpe = returns.mean() / returns.std() * np.sqrt(252)  # Annualized\n    return sharpe\n\n\nprint(\"âœ“ Helper functions loaded\")"
    },
    {
      "cell_type": "code",
      "source": "# ==================== ADVANCED MONTE CARLO METRICS ====================\n\nfrom scipy import stats\nfrom scipy.special import ndtr  # Standard normal CDF\n\ndef calculate_return_statistics(equity_curve):\n    \"\"\"Calculate comprehensive return statistics including higher moments\"\"\"\n    returns = equity_curve['equity'].pct_change().dropna()\n    \n    if len(returns) == 0:\n        return {\n            'mean': 0, 'std': 0, 'skewness': 0, 'kurtosis': 3,\n            'sharpe': 0, 'observations': 0\n        }\n    \n    return {\n        'mean': float(returns.mean()),\n        'std': float(returns.std()),\n        'skewness': float(stats.skew(returns)),\n        'kurtosis': float(stats.kurtosis(returns, fisher=False)),  # Pearson kurtosis (normal = 3)\n        'sharpe': float(returns.mean() / returns.std() * np.sqrt(252)) if returns.std() > 0 else 0,\n        'observations': len(returns)\n    }\n\n\ndef calculate_probabilistic_sharpe_ratio(sharpe, returns_stats, benchmark_sr=0.0):\n    \"\"\"\n    Calculate Probabilistic Sharpe Ratio (PSR)\n    \n    PSR represents the probability that the true Sharpe ratio exceeds a benchmark.\n    Industry threshold: PSR â‰¥ 0.95 for statistical significance at 95% confidence.\n    \n    Args:\n        sharpe: Estimated Sharpe ratio\n        returns_stats: Dict with 'observations', 'skewness', 'kurtosis'\n        benchmark_sr: Benchmark Sharpe ratio to test against (default 0.0)\n    \n    Returns:\n        PSR value between 0 and 1\n    \n    Reference: Bailey & LÃ³pez de Prado (2012) \"The Sharpe Ratio Efficient Frontier\"\n    \"\"\"\n    T = returns_stats['observations']\n    skew = returns_stats['skewness']\n    kurt = returns_stats['kurtosis']\n    \n    if T < 2:\n        return 0.0\n    \n    # Standard error of Sharpe ratio (accounting for non-normality)\n    # ÏƒÌ‚(ÅœR) = âˆš[(1 + (SRÂ²/2) - Î³â‚ƒÂ·SR + ((Î³â‚„-3)/4)Â·SRÂ²) / (T-1)]\n    variance_term = 1 + (sharpe**2 / 2) - skew * sharpe + ((kurt - 3) / 4) * sharpe**2\n    variance_term = max(variance_term, 0.0001)  # Prevent negative variance\n    \n    se_sharpe = np.sqrt(variance_term / (T - 1))\n    \n    # PSR = Î¦[(ÅœR - SR*) / ÏƒÌ‚(ÅœR)]\n    # where Î¦ is the standard normal CDF\n    if se_sharpe > 0:\n        z_score = (sharpe - benchmark_sr) / se_sharpe\n        psr = ndtr(z_score)  # Standard normal CDF\n    else:\n        psr = 1.0 if sharpe > benchmark_sr else 0.0\n    \n    return float(psr)\n\n\ndef calculate_minimum_track_record_length(sharpe, returns_stats, benchmark_sr=0.0, confidence=0.95):\n    \"\"\"\n    Calculate Minimum Track Record Length (MinTRL)\n    \n    MinTRL is the time period required to reject the null hypothesis that SR â‰¤ SR*\n    with specified confidence level.\n    \n    Args:\n        sharpe: Estimated Sharpe ratio\n        returns_stats: Dict with 'skewness', 'kurtosis'\n        benchmark_sr: Benchmark Sharpe ratio\n        confidence: Confidence level (default 0.95 for 95%)\n    \n    Returns:\n        Minimum number of observations required\n    \n    Reference: Bailey & LÃ³pez de Prado (2014) \"The Deflated Sharpe Ratio\"\n    \"\"\"\n    if sharpe <= benchmark_sr:\n        return float('inf')  # Cannot achieve significance if SR â‰¤ SR*\n    \n    skew = returns_stats['skewness']\n    kurt = returns_stats['kurtosis']\n    \n    # Z-score for confidence level\n    z_alpha = stats.norm.ppf(confidence)\n    \n    # MinTRL = 1 + [1 - Î³â‚ƒÃ—SR + (Î³â‚„-1)/4 Ã— SRÂ²] Ã— (Z_Î±/(SR-SR*))Â²\n    factor = 1 - skew * sharpe + ((kurt - 1) / 4) * sharpe**2\n    factor = max(factor, 0.0001)\n    \n    min_trl = 1 + factor * (z_alpha / (sharpe - benchmark_sr))**2\n    \n    return int(np.ceil(min_trl))\n\n\ndef calculate_deflated_sharpe_ratio(sharpe_ratios, confidence=0.95):\n    \"\"\"\n    Calculate Deflated Sharpe Ratio (DSR) to correct for multiple testing\n    \n    When testing N strategies, DSR adjusts for the probability of finding\n    lucky results purely by chance.\n    \n    Args:\n        sharpe_ratios: List/array of Sharpe ratios from multiple tests\n        confidence: Confidence level (default 0.95)\n    \n    Returns:\n        DSR value (probability of genuine edge after multiple testing)\n    \n    Reference: Bailey & LÃ³pez de Prado (2014)\n    \"\"\"\n    sharpe_array = np.array(sharpe_ratios)\n    N = len(sharpe_array)\n    \n    if N <= 1:\n        return 1.0  # No multiple testing correction needed\n    \n    # Variance of Sharpe ratios\n    var_sharpes = np.var(sharpe_array, ddof=1)\n    \n    if var_sharpes == 0:\n        return 1.0\n    \n    # Euler-Mascheroni constant\n    gamma_em = 0.5772156649\n    \n    # Expected maximum Sharpe under null hypothesis\n    # SRâ‚€* = âˆšVar[{ÅœRâ‚™}] Ã— [(1-Î³)Â·Î¦â»Â¹[1-1/N] + Î³Â·Î¦â»Â¹[1-(1/N)Â·eâ»Â¹]]\n    term1 = (1 - gamma_em) * stats.norm.ppf(1 - 1/N)\n    term2 = gamma_em * stats.norm.ppf(1 - (1/N) * np.exp(-1))\n    \n    sr_expected_max = np.sqrt(var_sharpes) * (term1 + term2)\n    \n    # Best observed Sharpe\n    sr_best = np.max(sharpe_array)\n    \n    # Calculate PSR using expected max as benchmark\n    # Simplified: assuming normal distribution for DSR calculation\n    if sr_best > sr_expected_max:\n        z_score = (sr_best - sr_expected_max) / np.sqrt(var_sharpes / N)\n        dsr = ndtr(z_score)\n    else:\n        dsr = 0.5  # No better than random\n    \n    return float(dsr)\n\n\ndef analyze_mc_distribution(values):\n    \"\"\"\n    Analyze distribution of Monte Carlo results\n    \n    Returns comprehensive statistics including percentiles, which are\n    more robust than mean for non-normal distributions.\n    \n    Args:\n        values: Array of values from MC simulations\n    \n    Returns:\n        Dict with distribution statistics\n    \"\"\"\n    arr = np.array(values)\n    \n    if len(arr) == 0:\n        return {\n            'mean': 0, 'median': 0, 'std': 0,\n            'p10': 0, 'p25': 0, 'p75': 0, 'p90': 0,\n            'min': 0, 'max': 0, 'skewness': 0, 'kurtosis': 3\n        }\n    \n    return {\n        'mean': float(np.mean(arr)),\n        'median': float(np.median(arr)),\n        'std': float(np.std(arr, ddof=1)),\n        'p10': float(np.percentile(arr, 10)),   # Worst case in 90% of scenarios\n        'p25': float(np.percentile(arr, 25)),\n        'p75': float(np.percentile(arr, 75)),\n        'p90': float(np.percentile(arr, 90)),\n        'min': float(np.min(arr)),\n        'max': float(np.max(arr)),\n        'skewness': float(stats.skew(arr)),\n        'kurtosis': float(stats.kurtosis(arr, fisher=False))\n    }\n\n\ndef calculate_confidence_interval(values, confidence=0.95):\n    \"\"\"Calculate confidence interval for a set of values\"\"\"\n    arr = np.array(values)\n    if len(arr) < 2:\n        return (0, 0)\n    \n    mean = np.mean(arr)\n    se = stats.sem(arr)  # Standard error of mean\n    z_score = stats.norm.ppf((1 + confidence) / 2)\n    \n    ci_lower = mean - z_score * se\n    ci_upper = mean + z_score * se\n    \n    return (float(ci_lower), float(ci_upper))\n\n\ndef assess_sample_size_adequacy(n_trades, autocorrelation=0.0):\n    \"\"\"\n    Assess whether sample size is adequate for statistical validation\n    \n    Args:\n        n_trades: Number of trades\n        autocorrelation: Average return correlation (0 = independent)\n    \n    Returns:\n        Dict with assessment\n    \"\"\"\n    # Adjust for autocorrelation\n    # n_effective = n_actual Ã— (1 - Ï)\n    n_effective = n_trades * (1 - abs(autocorrelation))\n    \n    # Thresholds\n    if n_effective >= 1000:\n        adequacy = \"EXCELLENT\"\n        confidence = \"High confidence in statistical inference\"\n    elif n_effective >= 100:\n        adequacy = \"GOOD\"\n        confidence = \"Adequate for production systems\"\n    elif n_effective >= 50:\n        adequacy = \"MARGINAL\"\n        confidence = \"Minimum acceptable, use with caution\"\n    elif n_effective >= 30:\n        adequacy = \"WEAK\"\n        confidence = \"Preliminary hypothesis testing only\"\n    else:\n        adequacy = \"INSUFFICIENT\"\n        confidence = \"Too few samples for reliable inference\"\n    \n    return {\n        'n_actual': n_trades,\n        'n_effective': int(n_effective),\n        'autocorrelation': autocorrelation,\n        'adequacy': adequacy,\n        'confidence': confidence,\n        'recommended_minimum': 100\n    }\n\n\nprint(\"âœ“ Advanced Monte Carlo metrics functions loaded\")\nprint(\"  - Probabilistic Sharpe Ratio (PSR)\")\nprint(\"  - Deflated Sharpe Ratio (DSR)\")\nprint(\"  - Minimum Track Record Length (MinTRL)\")\nprint(\"  - Distribution analysis with percentiles\")\nprint(\"  - Sample size adequacy assessment\")\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "MONTE CARLO WALK-FORWARD ANALYSIS - STATISTICAL ARBITRAGE\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Monte Carlo Run 1/20\n",
            "======================================================================\n",
            "Training:  2022-07-17 to 2024-08-21 (766 days)\n",
            "Testing:   2024-08-22 to 2025-07-16 (328 days)\n",
            "\n",
            "Fetching training data...\n",
            "  âš  Skipping PNC_KBE: no data\n",
            "  âš  Skipping ARCC_AMLP: no data\n",
            "  âš  Skipping RBA_SMFG: no data\n",
            "  âš  Skipping ENB_WEC: no data\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "No training data available for any pair",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  âœ“ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m days\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_data) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo training data available for any pair\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  âœ“ Fetched data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pairs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# 3. Run strategy on TRAINING data\u001b[39;00m\n",
            "\u001b[31mValueError\u001b[39m: No training data available for any pair"
          ]
        }
      ],
      "source": "# ==================== MONTE CARLO WALK-FORWARD ====================\n\nprint(\"=\"*70)\nprint(\"MONTE CARLO WALK-FORWARD ANALYSIS - STATISTICAL ARBITRAGE\")\nprint(\"=\"*70)\nprint()\n\nresults = []\nerrors = []\n\nfor run in range(config['monte_carlo_runs']):\n    print(f\"\\n{'='*70}\")\n    print(f\"Monte Carlo Run {run + 1}/{config['monte_carlo_runs']}\")\n    print(f\"{'='*70}\")\n    \n    try:\n        # 1. Generate random train/test split\n        train_start, train_end, test_start, test_end = generate_random_split(\n            config['total_period']['start'],\n            config['total_period']['end'],\n            config['train_test_split'],\n            seed=(config['random_seed'] + run) if config['random_seed'] else None\n        )\n        \n        print(f\"Training:  {train_start.date()} to {train_end.date()} ({(train_end - train_start).days} days)\")\n        print(f\"Testing:   {test_start.date()} to {test_end.date()} ({(test_end - test_start).days} days)\")\n        \n        # 2. Fetch historical data for TRAINING period\n        print(f\"\\nFetching training data...\")\n        train_data = {}\n        for pair in config['pairs']:\n            # Fetch history - use list with single symbol to get clean DataFrame\n            long_hist = qb.History([symbols[pair['name']]['long']], train_start, train_end, Resolution.Daily)\n            short_hist = qb.History([symbols[pair['name']]['short']], train_start, train_end, Resolution.Daily)\n            \n            if long_hist.empty or short_hist.empty:\n                print(f\"  âš  Skipping {pair['name']}: no data\")\n                continue\n            \n            # Extract close prices - handle multi-index if present\n            if isinstance(long_hist.index, pd.MultiIndex):\n                long_close = long_hist['close'].droplevel(0)\n                short_close = short_hist['close'].droplevel(0)\n            else:\n                long_close = long_hist['close']\n                short_close = short_hist['close']\n            \n            # Create aligned DataFrame\n            df = pd.DataFrame({\n                'long_price': long_close,\n                'short_price': short_close\n            }).dropna()\n            \n            # Only require lookback period worth of data\n            if len(df) < config['parameters']['lookback_period']:\n                print(f\"  âš  Skipping {pair['name']}: insufficient data ({len(df)} rows, need {config['parameters']['lookback_period']})\")\n                continue\n            \n            # Calculate spread and z-score\n            df['spread'] = np.log(df['long_price']) - np.log(df['short_price'])\n            df['zscore'] = calculate_zscore(df['spread'], config['parameters']['lookback_period'])\n            \n            train_data[pair['name']] = df\n            print(f\"  âœ“ {pair['name']}: {len(df)} days\")\n        \n        if len(train_data) == 0:\n            raise ValueError(\"No training data available for any pair\")\n        \n        print(f\"  âœ“ Fetched data for {len(train_data)} pairs\")\n        \n        # 3. Run strategy on TRAINING data\n        print(f\"Running strategy on training period...\")\n        train_equity, train_trades = simulate_strategy(train_data, config['parameters'])\n        train_sharpe = calculate_sharpe(train_equity)\n        print(f\"  âœ“ Training Sharpe: {train_sharpe:.3f} ({len(train_trades)} trades)\")\n        \n        # 4. Fetch historical data for TESTING period\n        print(f\"\\nFetching testing data...\")\n        test_data = {}\n        for pair in config['pairs']:\n            # Fetch history\n            long_hist = qb.History([symbols[pair['name']]['long']], test_start, test_end, Resolution.Daily)\n            short_hist = qb.History([symbols[pair['name']]['short']], test_start, test_end, Resolution.Daily)\n            \n            if long_hist.empty or short_hist.empty:\n                print(f\"  âš  Skipping {pair['name']}: no data\")\n                continue\n            \n            # Extract close prices\n            if isinstance(long_hist.index, pd.MultiIndex):\n                long_close = long_hist['close'].droplevel(0)\n                short_close = short_hist['close'].droplevel(0)\n            else:\n                long_close = long_hist['close']\n                short_close = short_hist['close']\n            \n            # Create aligned DataFrame\n            df = pd.DataFrame({\n                'long_price': long_close,\n                'short_price': short_close\n            }).dropna()\n            \n            # Only require lookback period worth of data\n            if len(df) < config['parameters']['lookback_period']:\n                print(f\"  âš  Skipping {pair['name']}: insufficient data ({len(df)} rows, need {config['parameters']['lookback_period']})\")\n                continue\n            \n            # Calculate spread and z-score\n            df['spread'] = np.log(df['long_price']) - np.log(df['short_price'])\n            df['zscore'] = calculate_zscore(df['spread'], config['parameters']['lookback_period'])\n            \n            test_data[pair['name']] = df\n            print(f\"  âœ“ {pair['name']}: {len(df)} days\")\n        \n        if len(test_data) == 0:\n            raise ValueError(\"No testing data available for any pair\")\n        \n        print(f\"  âœ“ Fetched data for {len(test_data)} pairs\")\n        \n        # 5. Run strategy on TESTING data\n        print(f\"Running strategy on testing period...\")\n        test_equity, test_trades = simulate_strategy(test_data, config['parameters'])\n        test_sharpe = calculate_sharpe(test_equity)\n        print(f\"  âœ“ Testing Sharpe: {test_sharpe:.3f} ({len(test_trades)} trades)\")\n\n        # 5a. Calculate return statistics for advanced metrics\n        train_stats = calculate_return_statistics(train_equity)\n        test_stats = calculate_return_statistics(test_equity)\n\n        # Calculate PSR for test period\n        test_psr = calculate_probabilistic_sharpe_ratio(\n            test_sharpe,\n            test_stats,\n            benchmark_sr=0.0  # Test against SR > 0\n        )\n\n        # Calculate MinTRL\n        min_trl = calculate_minimum_track_record_length(\n            test_sharpe,\n            test_stats,\n            benchmark_sr=0.0\n        ) if test_sharpe > 0 else float('inf')\n\n        print(f\"  PSR: {test_psr:.3f}, MinTRL: {min_trl if min_trl != float('inf') else 'N/A'}\")\n        \n        # 6. Calculate degradation\n        if train_sharpe > 0:\n            degradation = (train_sharpe - test_sharpe) / train_sharpe\n        else:\n            degradation = 1.0\n        \n        print(f\"  Degradation: {degradation*100:.1f}%\")\n        \n        # Store results\n        results.append({\n            'run': run + 1,\n            'train_start': train_start,\n            'train_end': train_end,\n            'test_start': test_start,\n            'test_end': test_end,\n            'train_sharpe': float(train_sharpe),\n            'test_sharpe': float(test_sharpe),\n            'degradation': float(degradation),\n            'train_trades': len(train_trades),\n            'test_trades': len(test_trades),\n            # Advanced metrics\n            'test_psr': float(test_psr),\n            'test_skewness': test_stats['skewness'],\n            'test_kurtosis': test_stats['kurtosis'],\n            'test_observations': test_stats['observations'],\n            'min_trl': min_trl if min_trl != float('inf') else None,\n            'train_skewness': train_stats['skewness'],\n            'train_kurtosis': train_stats['kurtosis'],\n            'train_observations': train_stats['observations'],\n        })\n        \n        print(f\"  âœ“ Run {run + 1} complete\")\n        \n    except Exception as e:\n        import traceback\n        error_msg = str(e)\n        traceback_str = traceback.format_exc()\n        print(f\"  âœ— Error in run {run + 1}: {error_msg}\")\n        print(f\"  Traceback:\\n{traceback_str}\")\n        errors.append({'run': run + 1, 'error': error_msg, 'traceback': traceback_str})\n        continue\n\nprint(f\"\\n{'='*70}\")\nprint(f\"Monte Carlo Walk-Forward Complete\")\nprint(f\"  Successful runs: {len(results)}/{config['monte_carlo_runs']}\")\nprint(f\"  Failed runs: {len(errors)}/{config['monte_carlo_runs']}\")\nprint(f\"{'='*70}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ==================== ANALYSIS ====================\n\nif len(results) == 0:\n    print(\"âœ— No successful runs to analyze\")\nelse:\n    df_results = pd.DataFrame(results)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"AGGREGATE RESULTS\")\n    print(\"=\"*70)\n    \n    # Basic statistics\n    mean_train = df_results['train_sharpe'].mean()\n    std_train = df_results['train_sharpe'].std()\n    mean_test = df_results['test_sharpe'].mean()\n    std_test = df_results['test_sharpe'].std()\n    mean_deg = df_results['degradation'].mean()\n    std_deg = df_results['degradation'].std()\n    \n    print(f\"\\nPerformance Metrics:\")\n    print(f\"  Baseline Sharpe (original):  {config['baseline_sharpe']:.3f}\")\n    print(f\"  Mean Training Sharpe:         {mean_train:.3f} Â± {std_train:.3f}\")\n    print(f\"  Mean Testing Sharpe:          {mean_test:.3f} Â± {std_test:.3f}\")\n    print(f\"  Mean Degradation:             {mean_deg*100:.1f}% Â± {std_deg*100:.1f}%\")\n    \n    # ==================== ADVANCED MONTE CARLO METRICS ====================\n    \n    print(f\"\\n\" + \"=\"*70)\n    print(\"ADVANCED MONTE CARLO METRICS\")\n    print(\"=\"*70)\n    \n    # PSR Analysis\n    test_psrs = [r['test_psr'] for r in results]\n    psr_dist = analyze_mc_distribution(test_psrs)\n    \n    print(f\"\\nðŸ“Š Probabilistic Sharpe Ratio (PSR):\")\n    print(f\"   Median PSR: {psr_dist['median']:.3f}\")\n    print(f\"   10th Percentile: {psr_dist['p10']:.3f} â† Use for conservative planning\")\n    print(f\"   90th Percentile: {psr_dist['p90']:.3f}\")\n    print(f\"   Status: {'âœ“ PASS' if psr_dist['p10'] >= 0.95 else 'âš  MARGINAL' if psr_dist['p10'] >= 0.90 else 'âœ— FAIL'}\")\n    print(f\"   (Threshold: 10th %ile PSR â‰¥ 0.95 for statistical significance)\")\n    \n    # Test Sharpe Distribution\n    test_sharpe_dist = analyze_mc_distribution(df_results['test_sharpe'])\n    print(f\"\\nðŸ“ˆ Test Sharpe Distribution:\")\n    print(f\"   10th Percentile: {test_sharpe_dist['p10']:.3f}\")\n    print(f\"   Median: {test_sharpe_dist['median']:.3f}\")\n    print(f\"   90th Percentile: {test_sharpe_dist['p90']:.3f}\")\n    print(f\"   Skewness: {test_sharpe_dist['skewness']:.2f}\")\n    print(f\"   Kurtosis: {test_sharpe_dist['kurtosis']:.2f}\")\n    \n    # Confidence Interval\n    ci_lower, ci_upper = calculate_confidence_interval(df_results['test_sharpe'])\n    print(f\"\\nðŸ“ 95% Confidence Interval:\")\n    print(f\"   Range: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n    print(f\"   Profitable: {'âœ“ YES' if ci_lower > 0 else 'âœ— NO'} (lower bound {'>' if ci_lower > 0 else 'â‰¤'} 0)\")\n    \n    # Deflated Sharpe Ratio\n    test_sharpes = df_results['test_sharpe'].values\n    dsr = calculate_deflated_sharpe_ratio(test_sharpes)\n    print(f\"\\nðŸ”¬ Deflated Sharpe Ratio (DSR):\")\n    print(f\"   DSR: {dsr:.3f}\")\n    print(f\"   Accounts for: {len(test_sharpes)} Monte Carlo trials\")\n    print(f\"   Status: {'âœ“ SIGNIFICANT' if dsr >= 0.95 else 'âš  MARGINAL' if dsr >= 0.90 else 'âœ— NOT SIGNIFICANT'}\")\n    \n    # MinTRL Analysis\n    valid_min_trls = [r['min_trl'] for r in results if r['min_trl'] is not None and r['min_trl'] != float('inf')]\n    if valid_min_trls:\n        mean_min_trl = np.mean(valid_min_trls)\n        actual_obs = df_results['test_observations'].mean()\n        print(f\"\\nâ± Minimum Track Record Length (MinTRL):\")\n        print(f\"   Required: {mean_min_trl:.0f} observations (mean)\")\n        print(f\"   Actual: {actual_obs:.0f} observations\")\n        print(f\"   Status: {'âœ“ ADEQUATE' if actual_obs >= mean_min_trl else 'âœ— INSUFFICIENT'}\")\n    \n    # Sample Size Adequacy\n    mean_test_trades = df_results['test_trades'].mean()\n    sample_assessment = assess_sample_size_adequacy(int(mean_test_trades))\n    print(f\"\\nðŸ”¢ Sample Size Assessment:\")\n    print(f\"   Test Trades (mean): {mean_test_trades:.0f}\")\n    print(f\"   Adequacy: {sample_assessment['adequacy']}\")\n    print(f\"   Confidence: {sample_assessment['confidence']}\")\n    print(f\"   Recommended Minimum: {sample_assessment['recommended_minimum']}+\")\n    \n    # ==================== BETTER OVERFITTING INDICATORS ====================\n    \n    print(f\"\\n\" + \"=\"*70)\n    print(\"OVERFITTING INDICATORS\")\n    print(\"=\"*70)\n    \n    # 1. Test Sharpe Stability (Coefficient of Variation)\n    test_sharpe_cv = (std_test / mean_test) if mean_test != 0 else float('inf')\n    print(f\"\\n1. Test Sharpe Stability:\")\n    print(f\"   Coefficient of Variation: {test_sharpe_cv:.2f}\")\n    print(f\"   Interpretation: {'STABLE' if test_sharpe_cv < 0.5 else 'UNSTABLE' if test_sharpe_cv < 1.0 else 'HIGHLY UNSTABLE'}\")\n    print(f\"   (Lower is better: <0.5 stable, 0.5-1.0 moderate, >1.0 unstable)\")\n    \n    # 2. Walk-Forward Efficiency\n    wf_efficiency = mean_test / mean_train if mean_train != 0 else 0\n    print(f\"\\n2. Walk-Forward Efficiency:\")\n    print(f\"   OOS Sharpe / IS Sharpe: {wf_efficiency:.1%}\")\n    print(f\"   Interpretation: {'EXCELLENT' if wf_efficiency > 0.80 else 'GOOD' if wf_efficiency > 0.60 else 'ACCEPTABLE' if wf_efficiency > 0.40 else 'WEAK' if wf_efficiency > 0.25 else 'SEVERE OVERFIT'}\")\n    print(f\"   (Expected: 25-80% for robust strategies)\")\n    \n    # 3. Test Sharpe vs Baseline\n    test_vs_baseline = (mean_test - config['baseline_sharpe']) / config['baseline_sharpe'] if config['baseline_sharpe'] != 0 else 0\n    print(f\"\\n3. Test Sharpe vs Baseline:\")\n    print(f\"   Difference: {test_vs_baseline:+.1%}\")\n    print(f\"   Mean Test: {mean_test:.3f} vs Baseline: {config['baseline_sharpe']:.3f}\")\n    if abs(test_vs_baseline) < 0.15:\n        print(f\"   Interpretation: CONSISTENT (within 15%)\")\n    elif test_vs_baseline > 0.15:\n        print(f\"   Interpretation: UNEXPECTED - Test > Baseline (possible data issue)\")\n    else:\n        print(f\"   Interpretation: DEGRADED - Test << Baseline\")\n    \n    # 4. Trade Count Analysis\n    mean_train_trades = df_results['train_trades'].mean()\n    mean_test_trades = df_results['test_trades'].mean()\n    min_test_trades = df_results['test_trades'].min()\n    print(f\"\\n4. Trade Count Analysis:\")\n    print(f\"   Mean Training Trades: {mean_train_trades:.1f}\")\n    print(f\"   Mean Testing Trades:  {mean_test_trades:.1f}\")\n    print(f\"   Min Testing Trades:   {min_test_trades}\")\n    print(f\"   Statistical Reliability: {'GOOD' if min_test_trades >= 30 else 'MARGINAL' if min_test_trades >= 15 else 'INSUFFICIENT'}\")\n    print(f\"   (Need 30+ trades for statistical significance)\")\n    \n    # 5. Consistency Analysis (% of runs with positive test Sharpe)\n    positive_test_pct = (df_results['test_sharpe'] > 0).sum() / len(df_results)\n    print(f\"\\n5. Consistency Analysis:\")\n    print(f\"   Runs with positive test Sharpe: {(df_results['test_sharpe'] > 0).sum()}/{len(df_results)} ({positive_test_pct:.0%})\")\n    print(f\"   Interpretation: {'HIGHLY CONSISTENT' if positive_test_pct >= 0.90 else 'CONSISTENT' if positive_test_pct >= 0.75 else 'MODERATE' if positive_test_pct >= 0.60 else 'INCONSISTENT'}\")\n    \n    # 6. Sample Size Assessment\n    print(f\"\\n6. Sample Size Assessment:\")\n    print(f\"   Total runs: {len(df_results)}\")\n    print(f\"   Statistical Power: {'ROBUST' if len(df_results) >= 1000 else 'ACCEPTABLE' if len(df_results) >= 100 else 'WEAK' if len(df_results) >= 50 else 'INSUFFICIENT' if len(df_results) >= 20 else 'ANECDOTAL'}\")\n    print(f\"   Minimum required: 1000+ runs\")\n    if len(df_results) < 1000:\n        print(f\"   âš  WARNING: Results not statistically reliable with {len(df_results)} runs\")\n    \n    # ==================== OVERALL ASSESSMENT ====================\n    \n    print(f\"\\n\" + \"=\"*70)\n    print(\"OVERALL ROBUSTNESS ASSESSMENT\")\n    print(\"=\"*70 + \"\\n\")\n    \n    # Calculate robustness score\n    score_components = []\n    \n    # Test Sharpe stability (weight: 20%)\n    if test_sharpe_cv < 0.5:\n        score_components.append(('Sharpe Stability', 20, 20))\n    elif test_sharpe_cv < 1.0:\n        score_components.append(('Sharpe Stability', 20, 10))\n    else:\n        score_components.append(('Sharpe Stability', 20, 0))\n    \n    # Walk-forward efficiency (weight: 30%)\n    if wf_efficiency > 0.80:\n        score_components.append(('WF Efficiency', 30, 30))\n    elif wf_efficiency > 0.60:\n        score_components.append(('WF Efficiency', 30, 25))\n    elif wf_efficiency > 0.40:\n        score_components.append(('WF Efficiency', 30, 15))\n    elif wf_efficiency > 0.25:\n        score_components.append(('WF Efficiency', 30, 5))\n    else:\n        score_components.append(('WF Efficiency', 30, 0))\n    \n    # Test vs baseline (weight: 20%)\n    if abs(test_vs_baseline) < 0.15:\n        score_components.append(('Test vs Baseline', 20, 20))\n    elif abs(test_vs_baseline) < 0.30:\n        score_components.append(('Test vs Baseline', 20, 10))\n    else:\n        score_components.append(('Test vs Baseline', 20, 0))\n    \n    # Trade count (weight: 15%)\n    if min_test_trades >= 30:\n        score_components.append(('Trade Count', 15, 15))\n    elif min_test_trades >= 15:\n        score_components.append(('Trade Count', 15, 8))\n    else:\n        score_components.append(('Trade Count', 15, 0))\n    \n    # Consistency (weight: 15%)\n    if positive_test_pct >= 0.90:\n        score_components.append(('Consistency', 15, 15))\n    elif positive_test_pct >= 0.75:\n        score_components.append(('Consistency', 15, 12))\n    elif positive_test_pct >= 0.60:\n        score_components.append(('Consistency', 15, 8))\n    else:\n        score_components.append(('Consistency', 15, 0))\n    \n    total_score = sum(s[2] for s in score_components)\n    max_score = sum(s[1] for s in score_components)\n    \n    print(\"Score Breakdown:\")\n    for name, max_pts, earned_pts in score_components:\n        print(f\"  {name}: {earned_pts}/{max_pts}\")\n    \n    print(f\"\\nTotal Robustness Score: {total_score}/{max_score} ({total_score/max_score*100:.0f}%)\")\n    \n    # Final decision\n    if len(df_results) < 1000:\n        decision = \"INSUFFICIENT_SAMPLES\"\n        reason = f\"Only {len(df_results)} runs (need 1000+ for validation)\"\n        recommendation = \"Continue scaling to 1000+ runs before making conclusion\"\n    elif total_score >= 85:\n        decision = \"ROBUST_STRATEGY\"\n        reason = f\"Score {total_score}/{max_score} - strong generalization\"\n        recommendation = \"Strategy passes validation - ready for paper trading\"\n    elif total_score >= 70:\n        decision = \"PROCEED_WITH_CAUTION\"\n        reason = f\"Score {total_score}/{max_score} - acceptable but not excellent\"\n        recommendation = \"Strategy shows reasonable robustness - additional validation recommended\"\n    elif total_score >= 50:\n        decision = \"WEAK_ROBUSTNESS\"\n        reason = f\"Score {total_score}/{max_score} - multiple concerns\"\n        recommendation = \"Strategy shows weak generalization - use with caution or re-optimize\"\n    else:\n        decision = \"ABANDON_STRATEGY\"\n        reason = f\"Score {total_score}/{max_score} - severe overfitting\"\n        recommendation = \"Strategy fails validation - consider new hypothesis\"\n    \n    print(f\"\\nâœ“ Decision: {decision}\")\n    print(f\"  Reason: {reason}\")\n    print(f\"  Recommendation: {recommendation}\")\n    \n    # Save results\n    output_data = {\n        'strategy': 'Statistical Arbitrage Pairs Trading',\n        'hypothesis_id': 5,\n        'project_id': config['project_id'],\n        'summary': {\n            'sample_size': len(results),\n            'successful_runs': len(results),\n            'failed_runs': len(errors),\n            'mean_train_sharpe': float(mean_train),\n            'std_train_sharpe': float(std_train),\n            'mean_test_sharpe': float(mean_test),\n            'std_test_sharpe': float(std_test),\n            'mean_degradation': float(mean_deg),\n            'std_degradation': float(std_deg),\n            'test_sharpe_cv': float(test_sharpe_cv),\n            'wf_efficiency': float(wf_efficiency),\n            'test_vs_baseline_pct': float(test_vs_baseline),\n            'mean_train_trades': float(mean_train_trades),\n            'mean_test_trades': float(mean_test_trades),\n            'min_test_trades': int(min_test_trades),\n            'positive_test_pct': float(positive_test_pct),\n            'robustness_score': int(total_score),\n            'max_score': int(max_score),\n            'decision': decision,\n            'reason': reason,\n            'recommendation': recommendation\n        },\n        'detailed_results': results,\n        'errors': errors\n    }\n    \n    filename = f\"walkforward_stat_arb_h5_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n    with open(filename, 'w') as f:\n        json.dump(output_data, f, indent=2, default=str)\n    \n    print(f\"\\nâœ“ Results saved to: {filename}\")\n    print(\"\\n\" + \"=\"*70)\n    print(\"MONTE CARLO WALK-FORWARD ANALYSIS COMPLETE\")\n    print(\"=\"*70)"
    },
    {
      "cell_type": "code",
      "source": "# ==================== DISPLAY HTML REPORT INLINE ====================\n\nfrom IPython.display import HTML, display\n\nif len(results) > 0:\n    df_results = pd.DataFrame(results)\n    \n    # Recalculate metrics for HTML\n    mean_train = df_results['train_sharpe'].mean()\n    std_train = df_results['train_sharpe'].std()\n    mean_test = df_results['test_sharpe'].mean()\n    std_test = df_results['test_sharpe'].std()\n    mean_deg = df_results['degradation'].mean()\n    std_deg = df_results['degradation'].std()\n    test_sharpe_cv = (std_test / mean_test) if mean_test != 0 else float('inf')\n    wf_efficiency = mean_test / mean_train if mean_train != 0 else 0\n    test_vs_baseline = (mean_test - config['baseline_sharpe']) / config['baseline_sharpe'] if config['baseline_sharpe'] != 0 else 0\n    mean_train_trades = df_results['train_trades'].mean()\n    mean_test_trades = df_results['test_trades'].mean()\n    min_test_trades = df_results['test_trades'].min()\n    positive_test_pct = (df_results['test_sharpe'] > 0).sum() / len(df_results)\n    \n    # Calculate advanced metrics for HTML\n    test_psrs = [r['test_psr'] for r in results]\n    psr_dist = analyze_mc_distribution(test_psrs)\n    test_sharpe_dist = analyze_mc_distribution(df_results['test_sharpe'])\n    ci_lower, ci_upper = calculate_confidence_interval(df_results['test_sharpe'])\n    dsr = calculate_deflated_sharpe_ratio(df_results['test_sharpe'].values)\n    test_sharpes = df_results['test_sharpe'].values\n    \n    # Calculate score components\n    score_components = []\n    if test_sharpe_cv < 0.5:\n        score_components.append(('Sharpe Stability', 20, 20))\n    elif test_sharpe_cv < 1.0:\n        score_components.append(('Sharpe Stability', 20, 10))\n    else:\n        score_components.append(('Sharpe Stability', 20, 0))\n    \n    if wf_efficiency > 0.80:\n        score_components.append(('WF Efficiency', 30, 30))\n    elif wf_efficiency > 0.60:\n        score_components.append(('WF Efficiency', 30, 25))\n    elif wf_efficiency > 0.40:\n        score_components.append(('WF Efficiency', 30, 15))\n    elif wf_efficiency > 0.25:\n        score_components.append(('WF Efficiency', 30, 5))\n    else:\n        score_components.append(('WF Efficiency', 30, 0))\n    \n    if abs(test_vs_baseline) < 0.15:\n        score_components.append(('Test vs Baseline', 20, 20))\n    elif abs(test_vs_baseline) < 0.30:\n        score_components.append(('Test vs Baseline', 20, 10))\n    else:\n        score_components.append(('Test vs Baseline', 20, 0))\n    \n    if min_test_trades >= 30:\n        score_components.append(('Trade Count', 15, 15))\n    elif min_test_trades >= 15:\n        score_components.append(('Trade Count', 15, 8))\n    else:\n        score_components.append(('Trade Count', 15, 0))\n    \n    if positive_test_pct >= 0.90:\n        score_components.append(('Consistency', 15, 15))\n    elif positive_test_pct >= 0.75:\n        score_components.append(('Consistency', 15, 12))\n    elif positive_test_pct >= 0.60:\n        score_components.append(('Consistency', 15, 8))\n    else:\n        score_components.append(('Consistency', 15, 0))\n    \n    total_score = sum(s[2] for s in score_components)\n    max_score = sum(s[1] for s in score_components)\n    score_pct = total_score / max_score * 100\n    \n    # Decision\n    if len(results) < 1000:\n        decision = \"INSUFFICIENT_SAMPLES\"\n        reason = f\"Only {len(results)} runs (need 1000+ for validation)\"\n        recommendation = \"Continue scaling to 1000+ runs before making conclusion\"\n        box_color = \"#f39c12\"\n    elif total_score >= 85:\n        decision = \"ROBUST_STRATEGY\"\n        reason = f\"Score {total_score}/{max_score} - strong generalization\"\n        recommendation = \"Strategy passes validation - ready for paper trading\"\n        box_color = \"#27ae60\"\n    elif total_score >= 70:\n        decision = \"PROCEED_WITH_CAUTION\"\n        reason = f\"Score {total_score}/{max_score} - acceptable but not excellent\"\n        recommendation = \"Strategy shows reasonable robustness - additional validation recommended\"\n        box_color = \"#3498db\"\n    elif total_score >= 50:\n        decision = \"WEAK_ROBUSTNESS\"\n        reason = f\"Score {total_score}/{max_score} - multiple concerns\"\n        recommendation = \"Strategy shows weak generalization - use with caution or re-optimize\"\n        box_color = \"#f39c12\"\n    else:\n        decision = \"ABANDON_STRATEGY\"\n        reason = f\"Score {total_score}/{max_score} - severe overfitting\"\n        recommendation = \"Strategy fails validation - consider new hypothesis\"\n        box_color = \"#e74c3c\"\n    \n    # Build detailed results rows\n    results_rows = \"\"\n    for result in results:\n        results_rows += f\"\"\"\n                <tr>\n                    <td>{result['run']}</td>\n                    <td>{result['train_start'].date()} to {result['train_end'].date()}</td>\n                    <td>{result['test_start'].date()} to {result['test_end'].date()}</td>\n                    <td>{result['train_sharpe']:.3f}</td>\n                    <td>{result['test_sharpe']:.3f}</td>\n                    <td>{result['degradation']*100:.1f}%</td>\n                    <td>{result['train_trades']}</td>\n                    <td>{result['test_trades']}</td>\n                </tr>\"\"\"\n    \n    # Build score breakdown rows\n    score_rows = \"\"\n    for name, max_pts, earned_pts in score_components:\n        score_rows += f\"<tr><td>{name}</td><td>{max_pts}</td><td>{earned_pts}</td></tr>\"\n    \n    # Generate HTML (to display inline)\n    html = f\"\"\"\n    <style>\n        .mc-report {{ font-family: 'Segoe UI', Arial, sans-serif; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }}\n        .mc-report h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; margin-top: 0; }}\n        .mc-report h2 {{ color: #34495e; margin-top: 25px; border-bottom: 2px solid #ecf0f1; padding-bottom: 8px; }}\n        .mc-report .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(220px, 1fr)); gap: 15px; margin: 15px 0; }}\n        .mc-report .metric-card {{ background: #ecf0f1; padding: 15px; border-radius: 6px; border-left: 4px solid #3498db; }}\n        .mc-report .metric-label {{ font-size: 11px; color: #7f8c8d; text-transform: uppercase; margin-bottom: 5px; }}\n        .mc-report .metric-value {{ font-size: 28px; font-weight: bold; color: #2c3e50; }}\n        .mc-report .metric-subtext {{ font-size: 12px; color: #95a5a6; margin-top: 5px; }}\n        .mc-report table {{ width: 100%; border-collapse: collapse; margin: 15px 0; font-size: 13px; }}\n        .mc-report th {{ background: #34495e; color: white; padding: 10px; text-align: left; }}\n        .mc-report td {{ padding: 8px; border-bottom: 1px solid #ecf0f1; }}\n        .mc-report tr:hover {{ background: #f8f9fa; }}\n        .mc-report .status-good {{ color: #27ae60; font-weight: bold; }}\n        .mc-report .status-warn {{ color: #f39c12; font-weight: bold; }}\n        .mc-report .status-bad {{ color: #e74c3c; font-weight: bold; }}\n        .mc-report .decision-box {{ background: {box_color}; color: white; padding: 25px; border-radius: 8px; margin: 20px 0; text-align: center; }}\n        .mc-report .decision-box h2 {{ color: white; border: none; margin: 0 0 10px 0; }}\n        .mc-report .progress-bar {{ width: 100%; background: #ecf0f1; height: 25px; border-radius: 12px; overflow: hidden; margin: 10px 0; }}\n        .mc-report .progress-fill {{ background: linear-gradient(90deg, #3498db, #2ecc71); height: 100%; text-align: center; line-height: 25px; color: white; font-weight: bold; }}\n        .mc-report ul {{ line-height: 1.8; }}\n        .mc-report .dist-note {{ color: #7f8c8d; font-size: 14px; margin-bottom: 10px; font-style: italic; }}\n    </style>\n    \n    <div class=\"mc-report\">\n        <h1>ðŸ“Š Monte Carlo Walk-Forward Validation Report</h1>\n        <p><strong>Strategy:</strong> Hypothesis 5 - Statistical Arbitrage Pairs Trading</p>\n        <p><strong>Project ID:</strong> {config['project_id']} | <strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n        \n        <h2>Configuration</h2>\n        <ul>\n            <li><strong>Pairs:</strong> {', '.join([p['name'] for p in config['pairs']])}</li>\n            <li><strong>Date Range:</strong> {config['total_period']['start'].date()} to {config['total_period']['end'].date()}</li>\n            <li><strong>Train/Test Split:</strong> {config['train_test_split']*100:.0f}% / {(1-config['train_test_split'])*100:.0f}%</li>\n            <li><strong>Monte Carlo Runs:</strong> {len(results)} successful / {len(errors)} failed</li>\n            <li><strong>Parameters:</strong> z_entry={config['parameters']['z_entry_threshold']}, z_exit={config['parameters']['z_exit_threshold']}, lookback={config['parameters']['lookback_period']}</li>\n        </ul>\n        \n        <h2>Performance Summary</h2>\n        <div class=\"metric-grid\">\n            <div class=\"metric-card\">\n                <div class=\"metric-label\">Baseline Sharpe</div>\n                <div class=\"metric-value\">{config['baseline_sharpe']:.3f}</div>\n                <div class=\"metric-subtext\">From optimization</div>\n            </div>\n            <div class=\"metric-card\">\n                <div class=\"metric-label\">Mean Training Sharpe</div>\n                <div class=\"metric-value\">{mean_train:.3f}</div>\n                <div class=\"metric-subtext\">Â± {std_train:.3f}</div>\n            </div>\n            <div class=\"metric-card\">\n                <div class=\"metric-label\">Mean Testing Sharpe</div>\n                <div class=\"metric-value\">{mean_test:.3f}</div>\n                <div class=\"metric-subtext\">Â± {std_test:.3f}</div>\n            </div>\n            <div class=\"metric-card\">\n                <div class=\"metric-label\">Mean Degradation</div>\n                <div class=\"metric-value\">{mean_deg*100:.1f}%</div>\n                <div class=\"metric-subtext\">Â± {std_deg*100:.1f}%</div>\n            </div>\n            <div class=\"metric-card\">\n                <div class=\"metric-label\">Median Test PSR</div>\n                <div class=\"metric-value\">{psr_dist['median']:.3f}</div>\n                <div class=\"metric-subtext\">10th %ile: {psr_dist['p10']:.3f}</div>\n            </div>\n            <div class=\"metric-card\">\n                <div class=\"metric-label\">Deflated Sharpe (DSR)</div>\n                <div class=\"metric-value\">{dsr:.3f}</div>\n                <div class=\"metric-subtext\">{len(test_sharpes)} trials</div>\n            </div>\n        </div>\n        \n        <h2>Overfitting Indicators</h2>\n        <table>\n            <tr>\n                <th>Indicator</th>\n                <th>Value</th>\n                <th>Interpretation</th>\n                <th>Status</th>\n            </tr>\n            <tr>\n                <td>1. Test Sharpe Stability (CV)</td>\n                <td>{test_sharpe_cv:.2f}</td>\n                <td>{'STABLE' if test_sharpe_cv < 0.5 else 'MODERATE' if test_sharpe_cv < 1.0 else 'UNSTABLE'}</td>\n                <td class=\"{'status-good' if test_sharpe_cv < 0.5 else 'status-warn' if test_sharpe_cv < 1.0 else 'status-bad'}\">{'âœ“' if test_sharpe_cv < 0.5 else 'âš ' if test_sharpe_cv < 1.0 else 'âœ—'}</td>\n            </tr>\n            <tr>\n                <td>2. Walk-Forward Efficiency</td>\n                <td>{wf_efficiency:.1%}</td>\n                <td>{'EXCELLENT' if wf_efficiency > 0.80 else 'GOOD' if wf_efficiency > 0.60 else 'ACCEPTABLE' if wf_efficiency > 0.40 else 'WEAK'}</td>\n                <td class=\"{'status-good' if wf_efficiency > 0.60 else 'status-warn' if wf_efficiency > 0.40 else 'status-bad'}\">{'âœ“' if wf_efficiency > 0.60 else 'âš ' if wf_efficiency > 0.40 else 'âœ—'}</td>\n            </tr>\n            <tr>\n                <td>3. Test vs Baseline</td>\n                <td>{test_vs_baseline:+.1%}</td>\n                <td>{'CONSISTENT' if abs(test_vs_baseline) < 0.15 else 'DEGRADED' if test_vs_baseline < -0.15 else 'UNEXPECTED'}</td>\n                <td class=\"{'status-good' if abs(test_vs_baseline) < 0.15 else 'status-warn' if abs(test_vs_baseline) < 0.30 else 'status-bad'}\">{'âœ“' if abs(test_vs_baseline) < 0.15 else 'âš ' if abs(test_vs_baseline) < 0.30 else 'âœ—'}</td>\n            </tr>\n            <tr>\n                <td>4. Trade Count (min test)</td>\n                <td>{min_test_trades}</td>\n                <td>{'GOOD' if min_test_trades >= 30 else 'MARGINAL' if min_test_trades >= 15 else 'INSUFFICIENT'}</td>\n                <td class=\"{'status-good' if min_test_trades >= 30 else 'status-warn' if min_test_trades >= 15 else 'status-bad'}\">{'âœ“' if min_test_trades >= 30 else 'âš ' if min_test_trades >= 15 else 'âœ—'}</td>\n            </tr>\n            <tr>\n                <td>5. Consistency (positive runs)</td>\n                <td>{positive_test_pct:.0%}</td>\n                <td>{'HIGHLY CONSISTENT' if positive_test_pct >= 0.90 else 'CONSISTENT' if positive_test_pct >= 0.75 else 'MODERATE' if positive_test_pct >= 0.60 else 'INCONSISTENT'}</td>\n                <td class=\"{'status-good' if positive_test_pct >= 0.75 else 'status-warn' if positive_test_pct >= 0.60 else 'status-bad'}\">{'âœ“' if positive_test_pct >= 0.75 else 'âš ' if positive_test_pct >= 0.60 else 'âœ—'}</td>\n            </tr>\n            <tr>\n                <td>6. Sample Size</td>\n                <td>{len(results)} runs</td>\n                <td>{'ROBUST' if len(results) >= 1000 else 'ACCEPTABLE' if len(results) >= 100 else 'WEAK' if len(results) >= 50 else 'INSUFFICIENT'}</td>\n                <td class=\"{'status-good' if len(results) >= 1000 else 'status-warn' if len(results) >= 100 else 'status-bad'}\">{'âœ“' if len(results) >= 1000 else 'âš ' if len(results) >= 100 else 'âœ—'}</td>\n            </tr>\n            <tr>\n                <td>7. Probabilistic Sharpe Ratio</td>\n                <td>{psr_dist['median']:.3f} (p10: {psr_dist['p10']:.3f})</td>\n                <td>{'SIGNIFICANT' if psr_dist['p10'] >= 0.95 else 'MARGINAL' if psr_dist['p10'] >= 0.90 else 'NOT SIGNIFICANT'}</td>\n                <td class=\"{'status-good' if psr_dist['p10'] >= 0.95 else 'status-warn' if psr_dist['p10'] >= 0.90 else 'status-bad'}\">\n                    {'âœ“' if psr_dist['p10'] >= 0.95 else 'âš ' if psr_dist['p10'] >= 0.90 else 'âœ—'}\n                </td>\n            </tr>\n        </table>\n        \n        <h2>Distribution Analysis</h2>\n        <p class=\"dist-note\">Percentiles provide robust estimates for non-normal distributions. Use 10th percentile for conservative planning.</p>\n        <table>\n            <tr><th>Metric</th><th>10th %ile</th><th>Median</th><th>90th %ile</th><th>95% CI</th></tr>\n            <tr>\n                <td>Test Sharpe</td>\n                <td>{test_sharpe_dist['p10']:.3f}</td>\n                <td>{test_sharpe_dist['median']:.3f}</td>\n                <td>{test_sharpe_dist['p90']:.3f}</td>\n                <td>[{ci_lower:.3f}, {ci_upper:.3f}]</td>\n            </tr>\n            <tr>\n                <td>PSR</td>\n                <td>{psr_dist['p10']:.3f}</td>\n                <td>{psr_dist['median']:.3f}</td>\n                <td>{psr_dist['p90']:.3f}</td>\n                <td>N/A</td>\n            </tr>\n        </table>\n        \n        <h2>Robustness Score</h2>\n        <table>\n            <tr><th>Component</th><th>Weight</th><th>Earned</th></tr>\n            {score_rows}\n            <tr style=\"background: #ecf0f1; font-weight: bold;\">\n                <td>TOTAL</td>\n                <td>{max_score}</td>\n                <td>{total_score}</td>\n            </tr>\n        </table>\n        \n        <div class=\"progress-bar\">\n            <div class=\"progress-fill\" style=\"width: {score_pct}%\">{score_pct:.0f}%</div>\n        </div>\n        \n        <div class=\"decision-box\">\n            <h2>âœ“ DECISION: {decision}</h2>\n            <p><strong>Reason:</strong> {reason}</p>\n            <p><strong>Recommendation:</strong> {recommendation}</p>\n        </div>\n        \n        <h2>Detailed Results (All {len(results)} Runs)</h2>\n        <table>\n            <tr>\n                <th>Run</th>\n                <th>Train Period</th>\n                <th>Test Period</th>\n                <th>Train Sharpe</th>\n                <th>Test Sharpe</th>\n                <th>Degradation</th>\n                <th>Train Trades</th>\n                <th>Test Trades</th>\n            </tr>\n            {results_rows}\n        </table>\n    </div>\n    \"\"\"\n    \n    # Display inline\n    display(HTML(html))\n    print(f\"\\nâœ“ Complete HTML report displayed above with all {len(results)} runs\")\n    \nelse:\n    print(\"âœ— No results to generate HTML report\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Foundation-Py-Default",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}