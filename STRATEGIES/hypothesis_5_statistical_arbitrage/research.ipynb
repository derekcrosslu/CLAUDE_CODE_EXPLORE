{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Walk-Forward Validation - Statistical Arbitrage Strategy\n",
    "\n",
    "**Strategy**: Hypothesis 5 - Statistical Arbitrage Pairs Trading  \n",
    "**Project ID**: 26140717  \n",
    "**Optimized Sharpe**: 1.829  \n",
    "**Baseline Sharpe**: 0.127  \n",
    "\n",
    "## Optimized Parameters to Validate:\n",
    "- z_entry_threshold: 1.5\n",
    "- z_exit_threshold: 1.0\n",
    "- lookback_period: 30\n",
    "- position_size_per_pair: 0.40\n",
    "- max_holding_days: 30\n",
    "- stop_loss_z: 4.0\n",
    "\n",
    "## How to Use:\n",
    "1. Upload this notebook to QuantConnect Research (free with subscription data)\n",
    "2. Run all cells sequentially\n",
    "3. Monte Carlo will test parameter robustness across random time periods\n",
    "4. Results will show if 1.829 Sharpe is robust or overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from collections import Counter\n",
    "import json\n",
    "import time\n",
    "\n",
    "# QuantConnect Research APIs\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "from QuantConnect.Api import Api\n",
    "\n",
    "# Initialize QuantBook\n",
    "qb = QuantBook()\n",
    "api = Api()  # Auto-authenticated in Research\n",
    "\n",
    "print(\"✓ QuantConnect Research environment initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "config = {\n",
    "    # Your QuantConnect project ID\n",
    "    'project_id': 26140717,  # StatArb_H5_PARTIAL_FIX\n",
    "    \n",
    "    # Algorithm file name\n",
    "    'algorithm_file': 'main.py',\n",
    "    \n",
    "    # Total period for analysis (from original backtest)\n",
    "    'total_period': {\n",
    "        'start': datetime(2022, 1, 1),\n",
    "        'end': datetime(2025, 10, 31)\n",
    "    },\n",
    "    \n",
    "    # Train/test split (70% train, 30% test)\n",
    "    'train_test_split': 0.70,\n",
    "    \n",
    "    # Number of Monte Carlo runs (start with 5 for testing, increase to 10 for production)\n",
    "    'monte_carlo_runs': 5,\n",
    "    \n",
    "    # Parameters to optimize (from Round 2 optimization)\n",
    "    'parameters': {\n",
    "        'z_entry_threshold': {'min': 1.5, 'max': 2.5, 'step': 0.5},\n",
    "        'z_exit_threshold': {'min': 0.3, 'max': 1.0, 'step': 0.2},\n",
    "        'lookback_period': {'min': 30, 'max': 70, 'step': 20}\n",
    "    },\n",
    "    \n",
    "    # Fixed parameters (not optimized in Monte Carlo)\n",
    "    'fixed_parameters': {\n",
    "        'position_size_per_pair': 0.40,\n",
    "        'max_holding_days': 30,\n",
    "        'stop_loss_z': 4.0\n",
    "    },\n",
    "    \n",
    "    # Target metric\n",
    "    'target_metric': 'SharpeRatio',\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    'random_seed': 42,\n",
    "    \n",
    "    # Baseline performance (from optimization)\n",
    "    'baseline_sharpe': 1.829\n",
    "}\n",
    "\n",
    "# Set random seed\n",
    "if config['random_seed'] is not None:\n",
    "    random.seed(config['random_seed'])\n",
    "    np.random.seed(config['random_seed'])\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Project ID: {config['project_id']}\")\n",
    "print(f\"  Period: {config['total_period']['start'].date()} to {config['total_period']['end'].date()}\")\n",
    "print(f\"  Train/Test: {config['train_test_split']*100:.0f}%/{(1-config['train_test_split'])*100:.0f}%\")\n",
    "print(f\"  Monte Carlo runs: {config['monte_carlo_runs']}\")\n",
    "print(f\"  Parameters to optimize: {list(config['parameters'].keys())}\")\n",
    "print(f\"  Fixed parameters: {list(config['fixed_parameters'].keys())}\")\n",
    "print(f\"  Baseline Sharpe: {config['baseline_sharpe']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def generate_random_split(start_date, end_date, train_pct, seed=None):\n",
    "    \"\"\"\n",
    "    Generate random training and testing periods (Monte Carlo sampling)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    total_days = (end_date - start_date).days\n",
    "    train_days = int(total_days * train_pct)\n",
    "    test_days = total_days - train_days\n",
    "    \n",
    "    # Ensure minimum test period (120 days ~ 4 months)\n",
    "    min_test_days = 120\n",
    "    if test_days < min_test_days:\n",
    "        raise ValueError(f\"Test period too short ({test_days} days). Need at least {min_test_days} days.\")\n",
    "    \n",
    "    # Random start point for training window\n",
    "    max_start_offset = test_days\n",
    "    start_offset = random.randint(0, max(0, max_start_offset))\n",
    "    \n",
    "    train_start = start_date + timedelta(days=start_offset)\n",
    "    train_end = train_start + timedelta(days=train_days)\n",
    "    test_start = train_end + timedelta(days=1)\n",
    "    test_end = train_start + timedelta(days=total_days)\n",
    "    \n",
    "    return train_start, train_end, test_start, test_end\n",
    "\n",
    "\n",
    "def format_optimization_params(params_config):\n",
    "    \"\"\"\n",
    "    Convert parameter config to QC API optimization format\n",
    "    \"\"\"\n",
    "    opt_params = []\n",
    "    for name, config_param in params_config.items():\n",
    "        opt_params.append({\n",
    "            'name': name,\n",
    "            'min': config_param['min'],\n",
    "            'max': config_param['max'],\n",
    "            'step': config_param['step']\n",
    "        })\n",
    "    return opt_params\n",
    "\n",
    "\n",
    "def wait_for_optimization(api, opt_id, timeout=1800, poll_interval=30):\n",
    "    \"\"\"\n",
    "    Poll optimization until completion\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        if time.time() - start_time > timeout:\n",
    "            raise TimeoutError(f\"Optimization {opt_id} exceeded timeout\")\n",
    "        \n",
    "        result = api.ReadOptimization(opt_id)\n",
    "        status = result.Optimization.Status\n",
    "        \n",
    "        if status == \"completed\":\n",
    "            return result\n",
    "        elif status == \"error\" or status == \"cancelled\":\n",
    "            raise RuntimeError(f\"Optimization {opt_id} failed with status: {status}\")\n",
    "        \n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "\n",
    "def wait_for_backtest(api, project_id, backtest_id, timeout=600, poll_interval=10):\n",
    "    \"\"\"\n",
    "    Poll backtest until completion\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        if time.time() - start_time > timeout:\n",
    "            raise TimeoutError(f\"Backtest {backtest_id} exceeded timeout\")\n",
    "        \n",
    "        result = api.ReadBacktest(project_id, backtest_id)\n",
    "        \n",
    "        if result.Success and result.Backtest.Completed:\n",
    "            return result\n",
    "        \n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "\n",
    "def extract_sharpe(backtest_result):\n",
    "    \"\"\"\n",
    "    Extract Sharpe ratio from backtest result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stats = backtest_result.Backtest.Statistics\n",
    "        for stat in stats:\n",
    "            if stat.Key == 'Sharpe Ratio':\n",
    "                return float(stat.Value)\n",
    "        return 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "print(\"✓ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ==================== MONTE CARLO WALK-FORWARD ====================\n\nprint(\"=\"*70)\nprint(\"MONTE CARLO WALK-FORWARD ANALYSIS - STATISTICAL ARBITRAGE\")\nprint(\"=\"*70)\nprint()\n\nresults = []\nerrors = []\n\n# Compile project once at start\nprint(\"Compiling project...\")\ntry:\n    compile_response = api.CreateCompile(config['project_id'])\n    if not compile_response.Success:\n        raise RuntimeError(f\"Compilation failed: {compile_response.Errors}\")\n    compile_id = compile_response.CompileId\n    print(f\"✓ Compilation successful: {compile_id}\")\nexcept Exception as e:\n    print(f\"✗ Compilation error: {e}\")\n    print(\"Note: If in QC Research, compilation may be automatic\")\n    compile_id = None\n\nprint()\n\nfor run in range(config['monte_carlo_runs']):\n    print(f\"\\n{'='*70}\")\n    print(f\"Monte Carlo Run {run + 1}/{config['monte_carlo_runs']}\")\n    print(f\"{'='*70}\")\n    \n    try:\n        # 1. Generate random train/test split\n        train_start, train_end, test_start, test_end = generate_random_split(\n            config['total_period']['start'],\n            config['total_period']['end'],\n            config['train_test_split'],\n            seed=run if config['random_seed'] else None\n        )\n        \n        print(f\"Training:  {train_start.date()} to {train_end.date()} ({(train_end - train_start).days} days)\")\n        print(f\"Testing:   {test_start.date()} to {test_end.date()} ({(test_end - test_start).days} days)\")\n        \n        # 2. Run optimization on TRAINING period\n        print(f\"\\nRunning optimization on training period...\")\n        \n        # Format parameters for API\n        opt_params = []\n        for name, param_config in config['parameters'].items():\n            opt_params.append({\n                'name': name,\n                'min': param_config['min'],\n                'max': param_config['max'],\n                'step': param_config['step']\n            })\n        \n        # Build optimization config\n        opt_config = {\n            'projectId': config['project_id'],\n            'compileId': compile_id,\n            'name': f\"MC_Train_Run{run+1}_{train_start.strftime('%Y%m%d')}\",\n            'target': 'TotalPerformance.PortfolioStatistics.SharpeRatio',\n            'targetTo': 'max',\n            'strategy': 'QuantConnect.Optimizer.Strategies.GridSearchOptimizationStrategy',\n            'parameters': opt_params,\n            'nodeType': 'O2-8',\n            'parallelNodes': 2\n        }\n        \n        # Create optimization using REST API directly\n        import requests\n        import os\n        \n        user_id = os.environ.get('QC_USER_ID')\n        token = os.environ.get('QC_API_TOKEN')\n        \n        headers = {\n            'Authorization': f'Basic {token}',\n            'Content-Type': 'application/json'\n        }\n        \n        response = requests.post(\n            'https://www.quantconnect.com/api/v2/optimizations/create',\n            headers=headers,\n            json=opt_config\n        )\n        \n        opt_result = response.json()\n        \n        if not opt_result.get('success'):\n            raise RuntimeError(f\"Optimization creation failed: {opt_result.get('errors')}\")\n        \n        opt_id = opt_result['optimizationId']\n        print(f\"  Optimization ID: {opt_id}\")\n        print(f\"  Waiting for completion (this may take 10-20 minutes)...\")\n        \n        # Wait for optimization to complete\n        opt_complete = wait_for_optimization(api, opt_id, timeout=1800)\n        \n        # Extract best parameters and Sharpe\n        best_params = {}\n        for param in opt_complete.Optimization.ParameterSet:\n            best_params[param.Key] = param.Value\n        \n        train_sharpe = opt_complete.Optimization.Statistics.SharpeRatio\n        \n        print(f\"  ✓ Training Sharpe: {train_sharpe:.3f}\")\n        print(f\"  Best parameters: {best_params}\")\n        \n        # 3. Run backtest on TESTING period with best parameters\n        print(f\"\\nRunning backtest on test period...\")\n        \n        # Create backtest\n        backtest_result = api.CreateBacktest(\n            config['project_id'],\n            compile_id,\n            f\"MC_Test_Run{run+1}_{test_start.strftime('%Y%m%d')}\"\n        )\n        \n        if not backtest_result.Success:\n            raise RuntimeError(f\"Backtest creation failed: {backtest_result.Errors}\")\n        \n        backtest_id = backtest_result.BacktestId\n        print(f\"  Backtest ID: {backtest_id}\")\n        print(f\"  Waiting for completion...\")\n        \n        # Wait for backtest to complete\n        test_complete = wait_for_backtest(api, config['project_id'], backtest_id)\n        test_sharpe = extract_sharpe(test_complete)\n        \n        print(f\"  ✓ Testing Sharpe: {test_sharpe:.3f}\")\n        \n        # 4. Calculate degradation\n        if train_sharpe > 0:\n            degradation = (train_sharpe - test_sharpe) / train_sharpe\n        else:\n            degradation = 1.0\n        \n        print(f\"  Degradation: {degradation*100:.1f}%\")\n        \n        # 5. Store results\n        results.append({\n            'run': run + 1,\n            'train_start': train_start,\n            'train_end': train_end,\n            'test_start': test_start,\n            'test_end': test_end,\n            'train_sharpe': float(train_sharpe),\n            'test_sharpe': float(test_sharpe),\n            'degradation': float(degradation),\n            'best_params': dict(best_params),\n            'optimization_id': opt_id,\n            'backtest_id': backtest_id\n        })\n        \n        print(f\"  ✓ Run {run + 1} complete\")\n        \n    except Exception as e:\n        error_msg = str(e)\n        print(f\"  ✗ Error in run {run + 1}: {error_msg}\")\n        errors.append({\n            'run': run + 1,\n            'error': error_msg\n        })\n        continue\n\nprint(f\"\\n{'='*70}\")\nprint(f\"Monte Carlo Walk-Forward Complete\")\nprint(f\"  Successful runs: {len(results)}/{config['monte_carlo_runs']}\")\nprint(f\"  Failed runs: {len(errors)}/{config['monte_carlo_runs']}\")\nprint(f\"{'='*70}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ANALYSIS ====================\n",
    "\n",
    "if len(results) == 0:\n",
    "    print(\"✗ No successful runs to analyze\")\n",
    "else:\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"AGGREGATE RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Summary statistics\n",
    "    mean_train = df_results['train_sharpe'].mean()\n",
    "    std_train = df_results['train_sharpe'].std()\n",
    "    mean_test = df_results['test_sharpe'].mean()\n",
    "    std_test = df_results['test_sharpe'].std()\n",
    "    mean_deg = df_results['degradation'].mean()\n",
    "    std_deg = df_results['degradation'].std()\n",
    "    \n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  Baseline Sharpe (original): {config['baseline_sharpe']:.3f}\")\n",
    "    print(f\"  Mean Training Sharpe:        {mean_train:.3f} ± {std_train:.3f}\")\n",
    "    print(f\"  Mean Testing Sharpe:         {mean_test:.3f} ± {std_test:.3f}\")\n",
    "    print(f\"  Mean Degradation:            {mean_deg*100:.1f}% ± {std_deg*100:.1f}%\")\n",
    "    \n",
    "    # Robustness analysis\n",
    "    overfit_count = (df_results['degradation'] > 0.30).sum()\n",
    "    good_count = (df_results['degradation'] < 0.15).sum()\n",
    "    overfit_pct = overfit_count / len(df_results)\n",
    "    good_pct = good_count / len(df_results)\n",
    "    \n",
    "    print(f\"\\nRobustness Analysis:\")\n",
    "    print(f\"  Runs with >30% degradation (OVERFIT): {overfit_count}/{len(df_results)} ({overfit_pct*100:.0f}%)\")\n",
    "    print(f\"  Runs with <15% degradation (GOOD):    {good_count}/{len(df_results)} ({good_pct*100:.0f}%)\")\n",
    "    \n",
    "    # Parameter stability\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"PARAMETER STABILITY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for param_name in config['parameters'].keys():\n",
    "        values = [r['best_params'].get(param_name) for r in results if param_name in r['best_params']]\n",
    "        if values:\n",
    "            counter = Counter(values)\n",
    "            most_common = counter.most_common(1)[0]\n",
    "            \n",
    "            print(f\"\\n{param_name}:\")\n",
    "            for value, count in counter.most_common():\n",
    "                pct = count / len(values) * 100\n",
    "                print(f\"  {value}: {count}/{len(values)} ({pct:.0f}%)\")\n",
    "            \n",
    "            if most_common[1] / len(values) >= 0.70:\n",
    "                print(f\"  ✓ STABLE: {most_common[0]} appears in {most_common[1]/len(values)*100:.0f}% of runs\")\n",
    "            else:\n",
    "                print(f\"  ⚠ UNSTABLE: No clear consensus (max {most_common[1]/len(values)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ROBUSTNESS DECISION ====================\n",
    "\n",
    "if len(results) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ROBUSTNESS DECISION FRAMEWORK\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Apply decision rules\n",
    "    if overfit_pct > 0.50:\n",
    "        decision = \"ABANDON_STRATEGY\"\n",
    "        reason = f\"Overfitting in {overfit_pct*100:.0f}% of Monte Carlo runs\"\n",
    "        recommendation = \"Strategy does not generalize. Consider new hypothesis.\"\n",
    "        \n",
    "    elif mean_deg > 0.40:\n",
    "        decision = \"HIGH_RISK\"\n",
    "        reason = f\"Average degradation {mean_deg*100:.1f}% indicates poor generalization\"\n",
    "        recommendation = \"Strategy shows high degradation. Use with extreme caution.\"\n",
    "        \n",
    "    elif std_deg > 0.25:\n",
    "        decision = \"UNSTABLE_PARAMETERS\"\n",
    "        reason = f\"High variance ({std_deg*100:.1f}%) suggests parameter instability\"\n",
    "        recommendation = \"Parameters unstable across periods. Consider narrowing search space.\"\n",
    "        \n",
    "    elif mean_deg < 0.15 and std_deg < 0.10:\n",
    "        decision = \"ROBUST_STRATEGY\"\n",
    "        reason = f\"Low degradation ({mean_deg*100:.1f}%) with low variance ({std_deg*100:.1f}%)\"\n",
    "        recommendation = \"Strategy shows excellent generalization. Ready for paper trading.\"\n",
    "        \n",
    "    else:\n",
    "        decision = \"PROCEED_WITH_CAUTION\"\n",
    "        reason = f\"Moderate degradation ({mean_deg*100:.1f}%), acceptable stability\"\n",
    "        recommendation = \"Strategy shows reasonable generalization. Additional validation recommended.\"\n",
    "    \n",
    "    print(f\"✓ Decision: {decision}\")\n",
    "    print(f\"\\n  Reason: {reason}\")\n",
    "    print(f\"\\n  Recommendation: {recommendation}\")\n",
    "    \n",
    "    # Recommended parameters\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"RECOMMENDED PARAMETERS FOR LIVE TRADING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    recommended_params = {}\n",
    "    for param_name in config['parameters'].keys():\n",
    "        values = [r['best_params'].get(param_name) for r in results if param_name in r['best_params']]\n",
    "        if values:\n",
    "            most_common = Counter(values).most_common(1)[0]\n",
    "            recommended_params[param_name] = most_common[0]\n",
    "            print(f\"  {param_name}: {most_common[0]} (chosen {most_common[1]/len(values)*100:.0f}% of the time)\")\n",
    "    \n",
    "    print(f\"\\n  Fixed parameters (unchanged):\")\n",
    "    for param_name, value in config['fixed_parameters'].items():\n",
    "        print(f\"    {param_name}: {value}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE RESULTS ====================\n",
    "\n",
    "if len(results) > 0:\n",
    "    output_data = {\n",
    "        'strategy': 'Statistical Arbitrage Pairs Trading',\n",
    "        'hypothesis_id': 5,\n",
    "        'project_id': config['project_id'],\n",
    "        'configuration': {\n",
    "            'period': f\"{config['total_period']['start'].date()} to {config['total_period']['end'].date()}\",\n",
    "            'train_test_split': config['train_test_split'],\n",
    "            'monte_carlo_runs': config['monte_carlo_runs'],\n",
    "            'parameters': config['parameters'],\n",
    "            'fixed_parameters': config['fixed_parameters'],\n",
    "            'baseline_sharpe': config['baseline_sharpe']\n",
    "        },\n",
    "        'summary': {\n",
    "            'successful_runs': len(results),\n",
    "            'failed_runs': len(errors),\n",
    "            'mean_train_sharpe': float(mean_train),\n",
    "            'mean_test_sharpe': float(mean_test),\n",
    "            'mean_degradation': float(mean_deg),\n",
    "            'std_degradation': float(std_deg),\n",
    "            'pct_overfit': float(overfit_pct),\n",
    "            'decision': decision,\n",
    "            'reason': reason,\n",
    "            'recommendation': recommendation\n",
    "        },\n",
    "        'recommended_parameters': {\n",
    "            **recommended_params,\n",
    "            **config['fixed_parameters']\n",
    "        },\n",
    "        'detailed_results': [\n",
    "            {\n",
    "                'run': r['run'],\n",
    "                'train_period': f\"{r['train_start'].date()} to {r['train_end'].date()}\",\n",
    "                'test_period': f\"{r['test_start'].date()} to {r['test_end'].date()}\",\n",
    "                'train_sharpe': r['train_sharpe'],\n",
    "                'test_sharpe': r['test_sharpe'],\n",
    "                'degradation': r['degradation'],\n",
    "                'best_params': r['best_params'],\n",
    "                'optimization_id': r['optimization_id'],\n",
    "                'backtest_id': r['backtest_id']\n",
    "            }\n",
    "            for r in results\n",
    "        ],\n",
    "        'errors': errors\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    output_filename = f\"walkforward_stat_arb_h5_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to: {output_filename}\")\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"MONTE CARLO WALK-FORWARD ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nNext Steps:\")\n",
    "    print(f\"  1. Review decision: {decision}\")\n",
    "    print(f\"  2. Download results JSON file\")\n",
    "    print(f\"  3. Update iteration_state.json locally\")\n",
    "    if decision == \"ROBUST_STRATEGY\":\n",
    "        print(f\"  4. ✓ Strategy validated - ready for paper trading\")\n",
    "    elif decision in [\"PROCEED_WITH_CAUTION\", \"UNSTABLE_PARAMETERS\"]:\n",
    "        print(f\"  4. ⚠ Additional validation recommended before paper trading\")\n",
    "    else:\n",
    "        print(f\"  4. ✗ Strategy failed validation - consider new hypothesis\")\n",
    "else:\n",
    "    print(\"\\n✗ No results to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}