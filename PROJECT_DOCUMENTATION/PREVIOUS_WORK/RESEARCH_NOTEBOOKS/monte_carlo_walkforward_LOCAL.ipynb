{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Monte Carlo Walk-Forward Validation - LOCAL VERSION\n",
    "\n",
    "**Purpose**: Test notebook logic locally with synthetic data (NO QuantConnect)\n",
    "\n",
    "## Run Locally:\n",
    "```bash\n",
    "jupyter notebook monte_carlo_walkforward_LOCAL.ipynb\n",
    "```\n",
    "\n",
    "Or install Jupyter:\n",
    "```bash\n",
    "pip install jupyter\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries (NO QuantConnect dependencies)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from collections import Counter\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"[OK] Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "config = {\n",
    "    # Simulation settings\n",
    "    'project_id': 'LOCAL_TEST',\n",
    "    \n",
    "    # Total period for analysis\n",
    "    'total_period': {\n",
    "        'start': datetime(2023, 1, 1),\n",
    "        'end': datetime(2024, 12, 31)\n",
    "    },\n",
    "    \n",
    "    # Train/test split (60% train, 40% test)\n",
    "    'train_test_split': 0.60,\n",
    "    \n",
    "    # Number of Monte Carlo runs\n",
    "    'monte_carlo_runs': 10,\n",
    "    \n",
    "    # Parameters being \"optimized\" (synthetic)\n",
    "    'parameters': {\n",
    "        'lookback_period': {'min': 15, 'max': 25, 'step': 5},\n",
    "        'volume_multiplier': {'min': 1.3, 'max': 1.7, 'step': 0.2}\n",
    "    },\n",
    "    \n",
    "    # Synthetic data generation settings\n",
    "    'synthetic': {\n",
    "        'train_sharpe_mean': 1.5,  # Average training Sharpe\n",
    "        'train_sharpe_std': 0.3,   # Variance in training\n",
    "        'degradation_mean': 0.20,  # 20% average degradation\n",
    "        'degradation_std': 0.10    # Variance in degradation\n",
    "    },\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# Set random seed\n",
    "if config['random_seed'] is not None:\n",
    "    random.seed(config['random_seed'])\n",
    "    np.random.seed(config['random_seed'])\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Test Mode: LOCAL SYNTHETIC DATA\")\n",
    "print(f\"  Period: {config['total_period']['start'].date()} to {config['total_period']['end'].date()}\")\n",
    "print(f\"  Train/Test: {config['train_test_split']*100:.0f}%/{(1-config['train_test_split'])*100:.0f}%\")\n",
    "print(f\"  Monte Carlo runs: {config['monte_carlo_runs']}\")\n",
    "print(f\"  Parameters: {list(config['parameters'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def generate_random_split(start_date, end_date, train_pct, seed=None):\n",
    "    \"\"\"\n",
    "    Generate random training and testing periods (Monte Carlo sampling)\n",
    "    \n",
    "    Args:\n",
    "        start_date: Overall start date\n",
    "        end_date: Overall end date\n",
    "        train_pct: Percentage of data for training (0.0-1.0)\n",
    "        seed: Random seed for this split\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_start, train_end, test_start, test_end)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    total_days = (end_date - start_date).days\n",
    "    train_days = int(total_days * train_pct)\n",
    "    test_days = total_days - train_days\n",
    "    \n",
    "    # Ensure minimum test period (90 days)\n",
    "    min_test_days = 90\n",
    "    if test_days < min_test_days:\n",
    "        raise ValueError(f\"Test period too short ({test_days} days). Need at least {min_test_days} days.\")\n",
    "    \n",
    "    # Random start point for training window\n",
    "    max_start_offset = total_days - train_days - test_days\n",
    "    start_offset = random.randint(0, max(0, max_start_offset))\n",
    "    \n",
    "    train_start = start_date + timedelta(days=start_offset)\n",
    "    train_end = train_start + timedelta(days=train_days)\n",
    "    test_start = train_end + timedelta(days=1)\n",
    "    test_end = test_start + timedelta(days=test_days)\n",
    "    \n",
    "    return train_start, train_end, test_start, test_end\n",
    "\n",
    "\n",
    "def format_optimization_params(params_config):\n",
    "    \"\"\"\n",
    "    Convert parameter config to API format (plain dictionaries)\n",
    "    \n",
    "    Args:\n",
    "        params_config: Dict with {name: {min, max, step}}\n",
    "    \n",
    "    Returns:\n",
    "        List of parameter dictionaries\n",
    "    \"\"\"\n",
    "    opt_params = []\n",
    "    \n",
    "    for name, config in params_config.items():\n",
    "        # Create parameter dictionary (plain dict for API)\n",
    "        param = {\n",
    "            'key': name,\n",
    "            'min': config['min'],\n",
    "            'max': config['max'],\n",
    "            'step': config['step']\n",
    "        }\n",
    "        opt_params.append(param)\n",
    "    \n",
    "    return opt_params\n",
    "\n",
    "\n",
    "def generate_synthetic_backtest(config, run_number):\n",
    "    \"\"\"\n",
    "    Generate synthetic backtest results (replaces real optimization/backtest)\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        run_number: Current Monte Carlo run number\n",
    "    \n",
    "    Returns:\n",
    "        dict: Synthetic results\n",
    "    \"\"\"\n",
    "    syn = config['synthetic']\n",
    "    \n",
    "    # Generate training Sharpe\n",
    "    train_sharpe = np.random.normal(syn['train_sharpe_mean'], syn['train_sharpe_std'])\n",
    "    train_sharpe = max(0.1, train_sharpe)\n",
    "    \n",
    "    # Generate degradation\n",
    "    degradation = np.random.normal(syn['degradation_mean'], syn['degradation_std'])\n",
    "    degradation = max(0.0, min(0.9, degradation))\n",
    "    \n",
    "    # Calculate test Sharpe\n",
    "    test_sharpe = train_sharpe * (1 - degradation)\n",
    "    \n",
    "    # Generate random \"best\" parameters\n",
    "    best_params = {}\n",
    "    for param_name, param_range in config['parameters'].items():\n",
    "        values = np.arange(param_range['min'], param_range['max'] + param_range['step'], param_range['step'])\n",
    "        best_params[param_name] = float(np.random.choice(values))\n",
    "    \n",
    "    # Generate synthetic metrics\n",
    "    test_metrics = {\n",
    "        'sharpe_ratio': float(test_sharpe),\n",
    "        'total_trades': int(np.random.randint(50, 200)),\n",
    "        'win_rate': float(np.random.uniform(0.45, 0.65))\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'train_sharpe': float(train_sharpe),\n",
    "        'test_sharpe': float(test_sharpe),\n",
    "        'degradation': float(degradation),\n",
    "        'best_params': best_params,\n",
    "        'test_metrics': test_metrics\n",
    "    }\n",
    "\n",
    "print(\"[OK] Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MONTE CARLO WALK-FORWARD ====================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MONTE CARLO WALK-FORWARD ANALYSIS (LOCAL TEST)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "for run in range(config['monte_carlo_runs']):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Monte Carlo Run {run + 1}/{config['monte_carlo_runs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Generate random train/test split\n",
    "        train_start, train_end, test_start, test_end = generate_random_split(\n",
    "            config['total_period']['start'],\n",
    "            config['total_period']['end'],\n",
    "            config['train_test_split'],\n",
    "            seed=run if config['random_seed'] else None\n",
    "        )\n",
    "        \n",
    "        print(f\"Training:  {train_start.date()} to {train_end.date()} ({(train_end - train_start).days} days)\")\n",
    "        print(f\"Testing:   {test_start.date()} to {test_end.date()} ({(test_end - test_start).days} days)\")\n",
    "        \n",
    "        # 2. Generate synthetic results (replaces real optimization/backtest)\n",
    "        print(f\"Generating synthetic backtest results...\")\n",
    "        synthetic_results = generate_synthetic_backtest(config, run)\n",
    "        \n",
    "        train_sharpe = synthetic_results['train_sharpe']\n",
    "        test_sharpe = synthetic_results['test_sharpe']\n",
    "        degradation = synthetic_results['degradation']\n",
    "        best_params = synthetic_results['best_params']\n",
    "        test_metrics = synthetic_results['test_metrics']\n",
    "        \n",
    "        print(f\"  Train Sharpe:  {train_sharpe:.3f}\")\n",
    "        print(f\"  Test Sharpe:   {test_sharpe:.3f}\")\n",
    "        print(f\"  Degradation:   {degradation*100:.1f}%\")\n",
    "        print(f\"  Best Params:   {best_params}\")\n",
    "        \n",
    "        # 3. Store results\n",
    "        results.append({\n",
    "            'run': run + 1,\n",
    "            'train_start': train_start,\n",
    "            'train_end': train_end,\n",
    "            'test_start': test_start,\n",
    "            'test_end': test_end,\n",
    "            'train_sharpe': train_sharpe,\n",
    "            'test_sharpe': test_sharpe,\n",
    "            'degradation': degradation,\n",
    "            'best_params': best_params,\n",
    "            'test_metrics': test_metrics\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"  [ERROR] Error in run {run + 1}: {error_msg}\")\n",
    "        errors.append({\n",
    "            'run': run + 1,\n",
    "            'error': error_msg\n",
    "        })\n",
    "        continue\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Monte Carlo Walk-Forward Complete\")\n",
    "print(f\"  Successful runs: {len(results)}/{config['monte_carlo_runs']}\")\n",
    "print(f\"  Failed runs: {len(errors)}/{config['monte_carlo_runs']}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ANALYSIS ====================\n",
    "\n",
    "if len(results) == 0:\n",
    "    print(\"[ERROR] No successful runs to analyze\")\n",
    "else:\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGGREGATE RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Summary statistics\n",
    "    mean_train = df_results['train_sharpe'].mean()\n",
    "    std_train = df_results['train_sharpe'].std()\n",
    "    mean_test = df_results['test_sharpe'].mean()\n",
    "    std_test = df_results['test_sharpe'].std()\n",
    "    mean_deg = df_results['degradation'].mean()\n",
    "    std_deg = df_results['degradation'].std()\n",
    "    \n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  Mean Training Sharpe:  {mean_train:.3f} ± {std_train:.3f}\")\n",
    "    print(f\"  Mean Testing Sharpe:   {mean_test:.3f} ± {std_test:.3f}\")\n",
    "    print(f\"  Mean Degradation:      {mean_deg*100:.1f}% ± {std_deg*100:.1f}%\")\n",
    "    \n",
    "    # Robustness analysis\n",
    "    overfit_count = (df_results['degradation'] > 0.30).sum()\n",
    "    good_count = (df_results['degradation'] < 0.15).sum()\n",
    "    overfit_pct = overfit_count / len(df_results)\n",
    "    good_pct = good_count / len(df_results)\n",
    "    \n",
    "    print(f\"\\nRobustness Analysis:\")\n",
    "    print(f\"  Runs with >30% degradation: {overfit_count}/{len(df_results)} ({overfit_pct*100:.0f}%)\")\n",
    "    print(f\"  Runs with <15% degradation: {good_count}/{len(df_results)} ({good_pct*100:.0f}%)\")\n",
    "    \n",
    "    # Parameter stability\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"PARAMETER STABILITY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for param_name in config['parameters'].keys():\n",
    "        values = [r['best_params'][param_name] for r in results]\n",
    "        counter = Counter(values)\n",
    "        most_common = counter.most_common(1)[0]\n",
    "        \n",
    "        print(f\"\\n{param_name}:\")\n",
    "        for value, count in counter.most_common():\n",
    "            pct = count / len(results) * 100\n",
    "            print(f\"  {value}: {count}/{len(results)} ({pct:.0f}%)\")\n",
    "        \n",
    "        if most_common[1] / len(results) >= 0.70:\n",
    "            print(f\"  [OK] STABLE: {most_common[0]} appears in {most_common[1]/len(results)*100:.0f}% of runs\")\n",
    "        else:\n",
    "            print(f\"  [WARNING] UNSTABLE: No clear consensus (max {most_common[1]/len(results)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ROBUSTNESS DECISION ====================\n",
    "\n",
    "if len(results) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ROBUSTNESS DECISION FRAMEWORK\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Apply decision rules\n",
    "    if overfit_pct > 0.50:\n",
    "        decision = \"ABANDON_STRATEGY\"\n",
    "        reason = f\"Overfitting in {overfit_pct*100:.0f}% of Monte Carlo runs\"\n",
    "        recommendation = \"Strategy does not generalize well. Consider new hypothesis.\"\n",
    "        \n",
    "    elif mean_deg > 0.40:\n",
    "        decision = \"HIGH_RISK\"\n",
    "        reason = f\"Average degradation {mean_deg*100:.1f}% indicates poor generalization\"\n",
    "        recommendation = \"Strategy shows high out-of-sample degradation. Use with extreme caution.\"\n",
    "        \n",
    "    elif std_deg > 0.25:\n",
    "        decision = \"UNSTABLE_PARAMETERS\"\n",
    "        reason = f\"High variance ({std_deg*100:.1f}%) suggests parameter instability\"\n",
    "        recommendation = \"Parameters not stable across periods. Consider narrowing search space.\"\n",
    "        \n",
    "    elif mean_deg < 0.15 and std_deg < 0.10:\n",
    "        decision = \"ROBUST_STRATEGY\"\n",
    "        reason = f\"Low degradation ({mean_deg*100:.1f}%) with low variance ({std_deg*100:.1f}%)\"\n",
    "        recommendation = \"Strategy shows excellent generalization. Ready for paper trading.\"\n",
    "        \n",
    "    else:\n",
    "        decision = \"PROCEED_WITH_CAUTION\"\n",
    "        reason = f\"Moderate degradation ({mean_deg*100:.1f}%), acceptable stability\"\n",
    "        recommendation = \"Strategy shows reasonable generalization. Additional validation recommended.\"\n",
    "    \n",
    "    print(f\"Decision: {decision}\")\n",
    "    print(f\"\\nReason: {reason}\")\n",
    "    print(f\"\\nRecommendation: {recommendation}\")\n",
    "    \n",
    "    # Recommended parameters\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDED PARAMETERS FOR LIVE TRADING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    recommended_params = {}\n",
    "    for param_name in config['parameters'].keys():\n",
    "        values = [r['best_params'][param_name] for r in results]\n",
    "        most_common = Counter(values).most_common(1)[0]\n",
    "        recommended_params[param_name] = most_common[0]\n",
    "        print(f\"  {param_name}: {most_common[0]} (chosen {most_common[1]/len(results)*100:.0f}% of the time)\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION ====================\n",
    "\n",
    "if len(results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Degradation distribution\n",
    "    axes[0, 0].hist(df_results['degradation'] * 100, bins=10, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[0, 0].axvline(x=30, color='r', linestyle='--', linewidth=2, label='Overfitting threshold (30%)')\n",
    "    axes[0, 0].axvline(x=mean_deg * 100, color='g', linestyle='--', linewidth=2, label=f'Mean ({mean_deg*100:.1f}%)')\n",
    "    axes[0, 0].set_xlabel('Degradation (%)', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0, 0].set_title('Distribution of Performance Degradation', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Training vs Testing Sharpe\n",
    "    axes[0, 1].scatter(df_results['train_sharpe'], df_results['test_sharpe'], \n",
    "                       alpha=0.6, s=100, c='steelblue', edgecolors='black', linewidth=1)\n",
    "    max_sharpe = max(df_results['train_sharpe'].max(), df_results['test_sharpe'].max())\n",
    "    axes[0, 1].plot([0, max_sharpe], [0, max_sharpe], 'r--', linewidth=2, label='Perfect generalization')\n",
    "    axes[0, 1].set_xlabel('Training Sharpe Ratio', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Testing Sharpe Ratio', fontsize=12)\n",
    "    axes[0, 1].set_title('Training vs Testing Performance', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Performance across runs\n",
    "    axes[1, 0].plot(df_results['run'], df_results['train_sharpe'], \n",
    "                    marker='o', label='Training', linewidth=2, markersize=8)\n",
    "    axes[1, 0].plot(df_results['run'], df_results['test_sharpe'], \n",
    "                    marker='s', label='Testing', linewidth=2, markersize=8)\n",
    "    axes[1, 0].fill_between(df_results['run'], df_results['train_sharpe'], \n",
    "                            df_results['test_sharpe'], alpha=0.2, color='gray')\n",
    "    axes[1, 0].set_xlabel('Monte Carlo Run', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Sharpe Ratio', fontsize=12)\n",
    "    axes[1, 0].set_title('Performance Across Monte Carlo Runs', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Degradation by run\n",
    "    colors = ['green' if d < 0.15 else 'orange' if d < 0.30 else 'red' \n",
    "              for d in df_results['degradation']]\n",
    "    axes[1, 1].bar(df_results['run'], df_results['degradation'] * 100, \n",
    "                   color=colors, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].axhline(y=30, color='r', linestyle='--', linewidth=2, label='Overfitting (30%)')\n",
    "    axes[1, 1].axhline(y=15, color='g', linestyle='--', linewidth=2, label='Good (15%)')\n",
    "    axes[1, 1].set_xlabel('Monte Carlo Run', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Degradation (%)', fontsize=12)\n",
    "    axes[1, 1].set_title('Degradation by Run', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n[OK] Visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE RESULTS ====================\n",
    "\n",
    "if len(results) > 0:\n",
    "    output_data = {\n",
    "        'configuration': {\n",
    "            'project_id': config['project_id'],\n",
    "            'period': f\"{config['total_period']['start'].date()} to {config['total_period']['end'].date()}\",\n",
    "            'train_test_split': config['train_test_split'],\n",
    "            'monte_carlo_runs': config['monte_carlo_runs'],\n",
    "            'parameters': config['parameters'],\n",
    "            'synthetic': config['synthetic']\n",
    "        },\n",
    "        'summary': {\n",
    "            'successful_runs': len(results),\n",
    "            'failed_runs': len(errors),\n",
    "            'mean_train_sharpe': float(mean_train),\n",
    "            'mean_test_sharpe': float(mean_test),\n",
    "            'mean_degradation': float(mean_deg),\n",
    "            'std_degradation': float(std_deg),\n",
    "            'pct_overfit': float(overfit_pct),\n",
    "            'decision': decision,\n",
    "            'reason': reason,\n",
    "            'recommendation': recommendation\n",
    "        },\n",
    "        'recommended_parameters': recommended_params,\n",
    "        'detailed_results': [\n",
    "            {\n",
    "                'run': r['run'],\n",
    "                'train_period': f\"{r['train_start'].date()} to {r['train_end'].date()}\",\n",
    "                'test_period': f\"{r['test_start'].date()} to {r['test_end'].date()}\",\n",
    "                'train_sharpe': r['train_sharpe'],\n",
    "                'test_sharpe': r['test_sharpe'],\n",
    "                'degradation': r['degradation'],\n",
    "                'best_params': r['best_params'],\n",
    "                'test_metrics': r['test_metrics']\n",
    "            }\n",
    "            for r in results\n",
    "        ],\n",
    "        'errors': errors\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    output_filename = f\"walkforward_results_local_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n[OK] Results saved to: {output_filename}\")\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"MONTE CARLO WALK-FORWARD ANALYSIS COMPLETE (LOCAL TEST)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nThis was a LOCAL TEST with SYNTHETIC data.\")\n",
    "    print(\"The logic and structure have been validated.\")\n",
    "    print(\"Next: Adapt this for QuantConnect Research with real backtests.\")\n",
    "else:\n",
    "    print(\"\\n[ERROR] No results to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
