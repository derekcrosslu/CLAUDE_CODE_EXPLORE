{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monte Carlo Walk-Forward Optimization\n",
        "\n",
        "This notebook implements Monte Carlo walk-forward analysis to validate strategy robustness.\n",
        "\n",
        "**Method:**\n",
        "1. Randomly sample training/testing periods (Monte Carlo)\n",
        "2. Optimize parameters on training data\n",
        "3. Validate on out-of-sample testing data\n",
        "4. Repeat N times and analyze distribution of results\n",
        "\n",
        "**Advantages over traditional walk-forward:**\n",
        "- More robust to period selection bias\n",
        "- Tests strategy across diverse market conditions\n",
        "- Provides distribution of expected degradation\n",
        "- Identifies parameter stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# QC API (available in QC Research environment)\n",
        "from QuantConnect import *\n",
        "from QuantConnect.Api import *\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    'project_id': 26120873,  # Your QC project ID\n",
        "    'total_period': {\n",
        "        'start': datetime(2020, 1, 1),\n",
        "        'end': datetime(2023, 12, 31)\n",
        "    },\n",
        "    'train_test_split': 0.60,  # 60% training, 40% testing\n",
        "    'monte_carlo_runs': 10,\n",
        "    'parameters': {\n",
        "        'rsi_oversold': {'min': 30, 'max': 45, 'step': 5},\n",
        "        'bb_distance_pct': {'min': 1.02, 'max': 1.10, 'step': 0.04},\n",
        "        'use_trend_filter': {'min': 0, 'max': 1, 'step': 1}\n",
        "    },\n",
        "    'validation_metric': 'sharpe_ratio'\n",
        "}\n",
        "\n",
        "print(f\"Configuration loaded:\")\n",
        "print(f\"  Period: {config['total_period']['start']} to {config['total_period']['end']}\")\n",
        "print(f\"  Train/Test: {config['train_test_split']*100:.0f}%/{(1-config['train_test_split'])*100:.0f}%\")\n",
        "print(f\"  Monte Carlo runs: {config['monte_carlo_runs']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function: Generate random train/test split\n",
        "def generate_random_split(start_date, end_date, train_pct):\n",
        "    \"\"\"\n",
        "    Generate random training and testing periods\n",
        "    \n",
        "    Args:\n",
        "        start_date: Overall start date\n",
        "        end_date: Overall end date\n",
        "        train_pct: Percentage of data for training (0.0-1.0)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (train_start, train_end, test_start, test_end)\n",
        "    \"\"\"\n",
        "    total_days = (end_date - start_date).days\n",
        "    train_days = int(total_days * train_pct)\n",
        "    test_days = total_days - train_days\n",
        "    \n",
        "    # Ensure we have at least 3 months for testing\n",
        "    min_test_days = 90\n",
        "    if test_days < min_test_days:\n",
        "        raise ValueError(f\"Test period too short ({test_days} days). Need at least {min_test_days} days.\")\n",
        "    \n",
        "    # Random start point for training window\n",
        "    # Ensure we leave enough room for the test period\n",
        "    max_start_offset = total_days - train_days - test_days\n",
        "    start_offset = random.randint(0, max(0, max_start_offset))\n",
        "    \n",
        "    train_start = start_date + timedelta(days=start_offset)\n",
        "    train_end = train_start + timedelta(days=train_days)\n",
        "    test_start = train_end + timedelta(days=1)\n",
        "    test_end = test_start + timedelta(days=test_days)\n",
        "    \n",
        "    return train_start, train_end, test_start, test_end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function: Run optimization on training period\n",
        "def run_optimization(api, project_id, start_date, end_date, parameters):\n",
        "    \"\"\"\n",
        "    Run parameter optimization on specified period using QC API\n",
        "    \n",
        "    Args:\n",
        "        api: QuantConnect API instance\n",
        "        project_id: QC project ID\n",
        "        start_date: Training start date\n",
        "        end_date: Training end date\n",
        "        parameters: Dict of parameters to optimize\n",
        "    \n",
        "    Returns:\n",
        "        dict: Optimization results with best_parameters and best_sharpe\n",
        "    \"\"\"\n",
        "    # Note: This would use QC's optimization API\n",
        "    # For now, we'll simulate with grid search backtests\n",
        "    \n",
        "    print(f\"  Optimizing: {start_date.date()} to {end_date.date()}\")\n",
        "    \n",
        "    # TODO: Implement actual QC optimization API call\n",
        "    # optimization = api.CreateOptimization(\n",
        "    #     project_id,\n",
        "    #     name=f\"MC_Train_{start_date.date()}\",\n",
        "    #     target=\"TotalPerformance.PortfolioStatistics.SharpeRatio\",\n",
        "    #     parameters=parameters\n",
        "    # )\n",
        "    \n",
        "    # Simulate optimization result for demo\n",
        "    best_params = {\n",
        "        'rsi_oversold': random.choice([30, 35, 40, 45]),\n",
        "        'bb_distance_pct': random.choice([1.02, 1.06, 1.10]),\n",
        "        'use_trend_filter': random.choice([0, 1])\n",
        "    }\n",
        "    \n",
        "    best_sharpe = np.random.uniform(0.5, 1.5)  # Simulated\n",
        "    \n",
        "    return {\n",
        "        'best_parameters': best_params,\n",
        "        'best_sharpe': best_sharpe,\n",
        "        'period': [start_date, end_date]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function: Run backtest with specific parameters\n",
        "def run_backtest(api, project_id, start_date, end_date, parameters):\n",
        "    \"\"\"\n",
        "    Run backtest on specified period with given parameters\n",
        "    \n",
        "    Args:\n",
        "        api: QuantConnect API instance\n",
        "        project_id: QC project ID\n",
        "        start_date: Test start date\n",
        "        end_date: Test end date\n",
        "        parameters: Dict of parameters to use\n",
        "    \n",
        "    Returns:\n",
        "        dict: Backtest results with sharpe_ratio\n",
        "    \"\"\"\n",
        "    print(f\"  Testing: {start_date.date()} to {end_date.date()}\")\n",
        "    \n",
        "    # TODO: Implement actual QC backtest API call with parameters\n",
        "    # backtest = api.CreateBacktest(\n",
        "    #     project_id,\n",
        "    #     name=f\"MC_Test_{start_date.date()}\",\n",
        "    #     parameters=parameters\n",
        "    # )\n",
        "    \n",
        "    # Simulate backtest result with some degradation\n",
        "    # Typically OOS performance degrades by 10-30%\n",
        "    degradation_factor = np.random.uniform(0.70, 0.95)\n",
        "    \n",
        "    # This would come from the optimization result passed in\n",
        "    # For demo, generate correlated performance\n",
        "    test_sharpe = np.random.uniform(0.3, 1.2) * degradation_factor\n",
        "    \n",
        "    return {\n",
        "        'sharpe_ratio': test_sharpe,\n",
        "        'total_return': np.random.uniform(0.05, 0.25),\n",
        "        'max_drawdown': np.random.uniform(0.05, 0.20),\n",
        "        'total_trades': np.random.randint(10, 100),\n",
        "        'period': [start_date, end_date]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main Monte Carlo Walk-Forward Loop\n",
        "print(\"Starting Monte Carlo Walk-Forward Analysis...\\n\")\n",
        "\n",
        "results = []\n",
        "api = None  # Would initialize QC API here: Api()\n",
        "\n",
        "for run in range(config['monte_carlo_runs']):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Monte Carlo Run {run + 1}/{config['monte_carlo_runs']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Generate random train/test split\n",
        "    train_start, train_end, test_start, test_end = generate_random_split(\n",
        "        config['total_period']['start'],\n",
        "        config['total_period']['end'],\n",
        "        config['train_test_split']\n",
        "    )\n",
        "    \n",
        "    print(f\"Training:  {train_start.date()} to {train_end.date()} ({(train_end - train_start).days} days)\")\n",
        "    print(f\"Testing:   {test_start.date()} to {test_end.date()} ({(test_end - test_start).days} days)\")\n",
        "    \n",
        "    # Run optimization on training period\n",
        "    opt_result = run_optimization(\n",
        "        api,\n",
        "        config['project_id'],\n",
        "        train_start,\n",
        "        train_end,\n",
        "        config['parameters']\n",
        "    )\n",
        "    \n",
        "    print(f\"  Best parameters: {opt_result['best_parameters']}\")\n",
        "    print(f\"  Training Sharpe: {opt_result['best_sharpe']:.3f}\")\n",
        "    \n",
        "    # Run backtest on testing period with optimized parameters\n",
        "    test_result = run_backtest(\n",
        "        api,\n",
        "        config['project_id'],\n",
        "        test_start,\n",
        "        test_end,\n",
        "        opt_result['best_parameters']\n",
        "    )\n",
        "    \n",
        "    print(f\"  Testing Sharpe: {test_result['sharpe_ratio']:.3f}\")\n",
        "    \n",
        "    # Calculate performance degradation\n",
        "    if opt_result['best_sharpe'] > 0:\n",
        "        degradation = (opt_result['best_sharpe'] - test_result['sharpe_ratio']) / opt_result['best_sharpe']\n",
        "    else:\n",
        "        degradation = 0\n",
        "    \n",
        "    print(f\"  Degradation: {degradation*100:.1f}%\")\n",
        "    \n",
        "    # Store results\n",
        "    results.append({\n",
        "        'run': run + 1,\n",
        "        'train_start': train_start,\n",
        "        'train_end': train_end,\n",
        "        'test_start': test_start,\n",
        "        'test_end': test_end,\n",
        "        'train_sharpe': opt_result['best_sharpe'],\n",
        "        'test_sharpe': test_result['sharpe_ratio'],\n",
        "        'degradation': degradation,\n",
        "        'best_params': opt_result['best_parameters'],\n",
        "        'test_trades': test_result['total_trades'],\n",
        "        'test_return': test_result['total_return'],\n",
        "        'test_drawdown': test_result['max_drawdown']\n",
        "    })\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Monte Carlo Walk-Forward Analysis Complete\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert results to DataFrame for analysis\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\n=\" * 60)\n",
        "print(\"AGGREGATE RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  Mean Training Sharpe:  {df_results['train_sharpe'].mean():.3f} ± {df_results['train_sharpe'].std():.3f}\")\n",
        "print(f\"  Mean Testing Sharpe:   {df_results['test_sharpe'].mean():.3f} ± {df_results['test_sharpe'].std():.3f}\")\n",
        "print(f\"  Mean Degradation:      {df_results['degradation'].mean()*100:.1f}% ± {df_results['degradation'].std()*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nRobustness Analysis:\")\n",
        "overfit_pct = (df_results['degradation'] > 0.30).sum() / len(df_results) * 100\n",
        "good_pct = (df_results['degradation'] < 0.15).sum() / len(df_results) * 100\n",
        "\n",
        "print(f\"  Runs with >30% degradation: {overfit_pct:.0f}% (overfitting indicator)\")\n",
        "print(f\"  Runs with <15% degradation: {good_pct:.0f}% (good generalization)\")\n",
        "\n",
        "# Display full results table\n",
        "print(f\"\\n\\nDetailed Results:\")\n",
        "display_df = df_results[['run', 'train_sharpe', 'test_sharpe', 'degradation', 'test_trades']].copy()\n",
        "display_df['degradation'] = display_df['degradation'] * 100\n",
        "display_df = display_df.round(3)\n",
        "print(display_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze parameter stability\n",
        "print(\"\\n=\" * 60)\n",
        "print(\"PARAMETER STABILITY ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Count frequency of each parameter value\n",
        "param_names = ['rsi_oversold', 'bb_distance_pct', 'use_trend_filter']\n",
        "\n",
        "for param in param_names:\n",
        "    values = [r['best_params'][param] for r in results]\n",
        "    counter = Counter(values)\n",
        "    most_common = counter.most_common(1)[0]\n",
        "    \n",
        "    print(f\"\\n{param}:\")\n",
        "    for value, count in counter.most_common():\n",
        "        pct = count / len(results) * 100\n",
        "        print(f\"  {value}: {count}/{len(results)} ({pct:.0f}%)\")\n",
        "    \n",
        "    if most_common[1] / len(results) >= 0.70:\n",
        "        print(f\"  ✅ STABLE: {most_common[0]} chosen in {most_common[1]/len(results)*100:.0f}% of runs\")\n",
        "    else:\n",
        "        print(f\"  ⚠️  UNSTABLE: No clear consensus (max {most_common[1]/len(results)*100:.0f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Degradation Distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Degradation distribution\n",
        "axes[0, 0].hist(df_results['degradation'] * 100, bins=10, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(x=30, color='r', linestyle='--', label='Overfitting threshold (30%)')\n",
        "axes[0, 0].axvline(x=df_results['degradation'].mean() * 100, color='g', linestyle='--', label=f'Mean ({df_results[\"degradation\"].mean()*100:.1f}%)')\n",
        "axes[0, 0].set_xlabel('Degradation (%)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Distribution of Performance Degradation')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Training vs Testing Sharpe\n",
        "axes[0, 1].scatter(df_results['train_sharpe'], df_results['test_sharpe'], alpha=0.6, s=100)\n",
        "max_sharpe = max(df_results['train_sharpe'].max(), df_results['test_sharpe'].max())\n",
        "axes[0, 1].plot([0, max_sharpe], [0, max_sharpe], 'r--', label='Perfect generalization')\n",
        "axes[0, 1].set_xlabel('Training Sharpe Ratio')\n",
        "axes[0, 1].set_ylabel('Testing Sharpe Ratio')\n",
        "axes[0, 1].set_title('Training vs Testing Performance')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Sharpe ratio over runs\n",
        "axes[1, 0].plot(df_results['run'], df_results['train_sharpe'], marker='o', label='Training', linewidth=2)\n",
        "axes[1, 0].plot(df_results['run'], df_results['test_sharpe'], marker='s', label='Testing', linewidth=2)\n",
        "axes[1, 0].fill_between(df_results['run'], df_results['train_sharpe'], df_results['test_sharpe'], alpha=0.2, color='gray')\n",
        "axes[1, 0].set_xlabel('Monte Carlo Run')\n",
        "axes[1, 0].set_ylabel('Sharpe Ratio')\n",
        "axes[1, 0].set_title('Performance Across Monte Carlo Runs')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Parameter stability heatmap\n",
        "param_matrix = []\n",
        "for param in param_names:\n",
        "    values = [r['best_params'][param] for r in results]\n",
        "    param_matrix.append(values)\n",
        "\n",
        "im = axes[1, 1].imshow([param_matrix], aspect='auto', cmap='YlOrRd')\n",
        "axes[1, 1].set_yticks(range(len(param_names)))\n",
        "axes[1, 1].set_yticklabels(param_names)\n",
        "axes[1, 1].set_xticks(range(len(results)))\n",
        "axes[1, 1].set_xticklabels(range(1, len(results) + 1))\n",
        "axes[1, 1].set_xlabel('Monte Carlo Run')\n",
        "axes[1, 1].set_title('Parameter Values Across Runs')\n",
        "plt.colorbar(im, ax=axes[1, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('monte_carlo_walkforward_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nVisualization saved to: monte_carlo_walkforward_results.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Decision Framework\n",
        "mean_deg = df_results['degradation'].mean()\n",
        "std_deg = df_results['degradation'].std()\n",
        "pct_overfit = (df_results['degradation'] > 0.30).sum() / len(df_results)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ROBUSTNESS DECISION\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "if pct_overfit > 0.50:\n",
        "    decision = \"ABANDON_STRATEGY\"\n",
        "    reason = f\"Overfitting in {pct_overfit*100:.0f}% of Monte Carlo runs\"\n",
        "    recommendation = \"Strategy does not generalize well. Consider new hypothesis.\"\n",
        "    \n",
        "elif mean_deg > 0.40:\n",
        "    decision = \"HIGH_RISK\"\n",
        "    reason = f\"Average degradation {mean_deg*100:.1f}% indicates poor generalization\"\n",
        "    recommendation = \"Strategy shows high out-of-sample degradation. Use with caution.\"\n",
        "    \n",
        "elif std_deg > 0.25:\n",
        "    decision = \"UNSTABLE_PARAMETERS\"\n",
        "    reason = f\"High variance ({std_deg*100:.1f}%) suggests parameter instability\"\n",
        "    recommendation = \"Parameters not stable. Consider narrowing search space.\"\n",
        "    \n",
        "elif mean_deg < 0.15 and std_deg < 0.10:\n",
        "    decision = \"ROBUST_STRATEGY\"\n",
        "    reason = f\"Low degradation ({mean_deg*100:.1f}%) with low variance ({std_deg*100:.1f}%)\"\n",
        "    recommendation = \"Strategy shows excellent generalization. Ready for live testing.\"\n",
        "    \n",
        "else:\n",
        "    decision = \"PROCEED_WITH_CAUTION\"\n",
        "    reason = f\"Moderate degradation ({mean_deg*100:.1f}%), acceptable stability\"\n",
        "    recommendation = \"Strategy shows reasonable generalization. Additional validation recommended.\"\n",
        "\n",
        "print(f\"Decision: {decision}\")\n",
        "print(f\"\\nReason: {reason}\")\n",
        "print(f\"\\nRecommendation: {recommendation}\")\n",
        "\n",
        "# Recommended parameters (most frequently chosen)\n",
        "print(f\"\\nRecommended Parameters for Live Trading:\")\n",
        "for param in param_names:\n",
        "    values = [r['best_params'][param] for r in results]\n",
        "    most_common = Counter(values).most_common(1)[0]\n",
        "    print(f\"  {param}: {most_common[0]} (chosen {most_common[1]/len(results)*100:.0f}% of the time)\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to JSON\n",
        "output_data = {\n",
        "    'configuration': config,\n",
        "    'summary': {\n",
        "        'mean_train_sharpe': float(df_results['train_sharpe'].mean()),\n",
        "        'mean_test_sharpe': float(df_results['test_sharpe'].mean()),\n",
        "        'mean_degradation': float(mean_deg),\n",
        "        'std_degradation': float(std_deg),\n",
        "        'pct_overfit': float(pct_overfit),\n",
        "        'decision': decision,\n",
        "        'reason': reason,\n",
        "        'recommendation': recommendation\n",
        "    },\n",
        "    'recommended_parameters': {\n",
        "        param: Counter([r['best_params'][param] for r in results]).most_common(1)[0][0]\n",
        "        for param in param_names\n",
        "    },\n",
        "    'detailed_results': results\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "output_filename = f\"walkforward_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "with open(output_filename, 'w') as f:\n",
        "    json.dump(output_data, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\nResults saved to: {output_filename}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
