# QuantConnect Research Notebook Execution - Full Investigation Report

**Date**: 2025-11-10
**Investigation Duration**: Full day research
**Objective**: Find a clear path forward for executing QC research notebooks via API

---

## Executive Summary

### Critical Finding: `qb.Optimize()` and `qb.Backtest()` DO NOT EXIST ‚ùå

After exhaustive research, I discovered that **the Monte Carlo notebook I created uses non-existent methods**. The QuantConnect Research environment does NOT provide direct execution methods for backtests or optimizations within notebooks.

### What Actually Exists:

**In Research Notebooks:**
- ‚úÖ `api.read_backtest(project_id, backtest_id)` - Read existing backtest results
- ‚úÖ `api.read_optimization(optimization_id)` - Read existing optimization results
- ‚úÖ QuantBook data analysis methods (History, GetFundamental, etc.)
- ‚ùå `qb.Backtest()` - Does NOT exist
- ‚ùå `qb.Optimize()` - Does NOT exist

**Via API/CLI:**
- ‚úÖ `POST /backtests/create` - Create and run backtests (REST API)
- ‚úÖ `POST /optimizations/create` - Create and run optimizations (REST API, PAID)
- ‚úÖ `lean backtest` - Run local backtests (CLI)
- ‚úÖ `lean optimize` - Run local optimizations (CLI)

---

## Investigation Findings

### 1. QuantConnect REST API (Cloud)

**Tested Endpoints:**

| Endpoint | Purpose | Cost | Works |
|----------|---------|------|-------|
| `POST /files/create` | Upload files | FREE | ‚úÖ |
| `POST /files/read` | Read files | FREE | ‚úÖ |
| `POST /files/update` | Update files | FREE | ‚úÖ |
| `POST /compile/create` | Compile project | FREE | ‚úÖ |
| `POST /backtests/create` | Run backtest | FREE (10/day) | ‚úÖ |
| `POST /backtests/read` | Get results | FREE | ‚úÖ |
| `POST /optimizations/create` | Run optimization | üí∞ $0.50-5+ | ‚úÖ (PAID) |
| `POST /optimizations/estimate` | Estimate cost | FREE | ‚úÖ |

**Key Limitation**: No endpoint to execute research notebook cells

### 2. LEAN CLI (Local)

**Capabilities:**

```bash
# Start local Jupyter environment
lean research "Project Name"

# Run local backtest
lean backtest "Project Name"

# Run local optimization
lean optimize "Project Name"
```

**Key Features:**
- Runs in Docker container
- Uses local data
- Results stored in project directories
- Free (no API costs)
- Supports same QuantBook environment as cloud

**Key Limitation**: Still no way to execute notebook cells programmatically

### 3. MCP Server

**What It Provides:**
- 64 tools for QC API interaction
- Project management
- Backtest creation and reading
- Optimization (via API, PAID)
- File operations
- Live trading deployment

**What It DOESN'T Provide:**
- Research notebook execution
- QuantBook integration
- Notebook cell execution
- Local LEAN CLI integration

### 4. Jupyter Notebook Execution Tools

**Papermill:**
```python
import papermill as pm
pm.execute_notebook(
    'input.ipynb',
    'output.ipynb',
    parameters={'alpha': 0.6}
)
```

**NBConvert:**
```bash
jupyter nbconvert --to notebook --execute notebook.ipynb
```

**Applicability to QC:**
- ‚úÖ Can execute notebooks programmatically
- ‚úÖ Can pass parameters
- ‚úÖ Works with local LEAN CLI research environment
- ‚ùå Requires local setup (not cloud)
- ‚ùå Needs LEAN Docker container running

### 5. Research Environment Analysis

**What Research IS:**
- Jupyter notebook environment
- QuantBook for data analysis
- API client (`api`) for reading results
- Data visualization and exploration
- Post-analysis of backtests/optimizations

**What Research IS NOT:**
- Execution environment for backtests
- Execution environment for optimizations
- Automated workflow orchestrator

### 6. Forum & Community Findings

**Key insights from forum searches:**

1. **Backtests cannot be run within notebooks** - confirmed by multiple users
2. **Optimization is run separately** - via API or CLI, then results analyzed in notebooks
3. **Local automation uses LEAN CLI** - not cloud Research notebooks
4. **API is used FROM notebooks** - to trigger external backtests, not execute code

---

## Correct Architecture Options

### Option A: Pure API Automation (Cloud) - LIMITED

**Workflow:**
```
1. Create strategy file
2. Upload via /files/update
3. Compile via /compile/create
4. Run backtest via /backtests/create (FREE, 10/day limit)
5. Read results via /backtests/read
6. Make decisions
7. IF optimize needed:
   a. Run optimization via /optimizations/create (üí∞ PAID)
   b. Read results via /optimizations/read
8. Update state, commit to git
```

**Pros:**
- ‚úÖ Fully automated
- ‚úÖ No local setup required
- ‚úÖ Cloud-based

**Cons:**
- ‚ùå Limited to 10 backtests/day (free tier)
- ‚ùå Optimization costs money ($0.50-5+ per run)
- ‚ùå No Monte Carlo walk-forward (would cost hundreds of dollars)
- ‚ùå Cannot use Research notebooks programmatically

**Feasibility**: ‚ö†Ô∏è Possible but expensive for advanced workflows

---

### Option B: Local LEAN CLI + Papermill - FREE & POWERFUL

**Workflow:**
```
1. Create strategy file locally
2. Create analysis notebook locally
3. Run: lean backtest "Project" (FREE, unlimited)
4. Results saved to /backtests/timestamp/
5. Run: papermill analysis.ipynb output.ipynb \
       parameters='{"backtest_id": "abc123"}'
6. Notebook reads results using api.read_backtest()
7. Parse output notebook for decisions
8. IF optimize needed:
   a. Run: lean optimize "Project" (FREE, unlimited)
   b. Run: papermill analysis.ipynb output.ipynb
   c. Parse results
9. Update state, commit to git
```

**Setup Required:**
```bash
# Install LEAN CLI
pip install lean

# Initialize
lean init

# Login to QC (for cloud data access)
lean login

# Start research environment (one time)
lean research "Project"

# Then use papermill to execute notebooks programmatically
```

**Pros:**
- ‚úÖ Fully automated
- ‚úÖ 100% FREE (no API costs)
- ‚úÖ Unlimited backtests
- ‚úÖ Unlimited optimizations
- ‚úÖ Monte Carlo walk-forward feasible
- ‚úÖ Can execute notebooks programmatically with Papermill
- ‚úÖ Full QuantBook capabilities

**Cons:**
- ‚ö†Ô∏è Requires Docker installed
- ‚ö†Ô∏è Requires local LEAN CLI setup
- ‚ö†Ô∏è Uses local compute resources
- ‚ö†Ô∏è Requires local data download (or cloud connection)

**Feasibility**: ‚úÖ Highly feasible, best option for cost-free automation

---

### Option C: Hybrid Cloud + Local

**Workflow:**
```
1. Develop strategies in cloud (via API)
2. Download for local optimization (LEAN CLI)
3. Upload optimized version back to cloud
4. Run validation backtests in cloud
5. Deploy to live trading
```

**Pros:**
- ‚úÖ Leverages cloud for development
- ‚úÖ Leverages local for expensive operations
- ‚úÖ Cost-effective hybrid

**Cons:**
- ‚ö†Ô∏è Complex workflow
- ‚ö†Ô∏è Requires sync between cloud and local
- ‚ö†Ô∏è State management complexity

**Feasibility**: ‚úÖ Feasible but complex

---

### Option D: Manual Execution Points (Current Recommendation)

**Workflow:**
```
1. Claude creates strategy ‚Üí Automated (API)
2. Claude runs baseline backtest ‚Üí Automated (API)
3. Claude makes decision ‚Üí Automated
4. IF optimize:
   a. Claude creates optimization job ‚Üí Automated (API, PAID)
   OR
   b. Claude uploads notebook ‚Üí Automated (API, FREE)
   c. USER clicks "Run All" ‚Üí MANUAL (1-click, FREE)
5. Claude reads results ‚Üí Automated (API)
6. Claude commits to git ‚Üí Automated
```

**Pros:**
- ‚úÖ Simple to implement
- ‚úÖ Mostly automated (90%)
- ‚úÖ Free (if using Research notebooks)
- ‚úÖ No local setup required

**Cons:**
- ‚ùå Requires manual intervention (2 clicks per hypothesis)
- ‚ùå Not truly autonomous

**Feasibility**: ‚úÖ Current implementation

---

## Recommended Path Forward

### Recommendation: **Adopt Option B - Local LEAN CLI + Papermill**

**Rationale:**

1. **Truly Autonomous**
   - 100% automated workflow
   - No manual intervention required
   - Can run overnight/unattended

2. **Cost-Free**
   - Unlimited backtests
   - Unlimited optimizations
   - No API quota limitations
   - Monte Carlo walk-forward viable

3. **Full Featured**
   - Access to all QuantBook capabilities
   - Can execute notebook analysis programmatically
   - Complete research environment locally

4. **Scalable**
   - Test multiple hypotheses per day
   - No rate limits
   - Can run parallel experiments

### Implementation Plan

#### Phase 1: Setup (One-Time)

```bash
# 1. Install dependencies
pip install lean papermill nbconvert

# 2. Initialize LEAN
lean init

# 3. Login to QuantConnect (for cloud data access)
lean login

# 4. Download market data (optional, or use cloud)
lean data download --dataset <name>
```

#### Phase 2: Create Automation Scripts

**File: `run_backtest_local.py`**
```python
import subprocess
import json
from pathlib import Path

def run_local_backtest(project_name):
    """Run local backtest using LEAN CLI"""

    # Run backtest
    result = subprocess.run(
        ["lean", "backtest", project_name],
        capture_output=True,
        text=True
    )

    # Find latest backtest result
    backtest_dir = Path(project_name) / "backtests"
    latest = max(backtest_dir.iterdir(), key=lambda p: p.stat().st_mtime)

    # Read results
    with open(latest / "results.json") as f:
        results = json.load(f)

    return results

def run_local_optimization(project_name, parameters):
    """Run local optimization using LEAN CLI"""

    # Create optimization config
    config = {
        "parameters": parameters,
        "optimization-strategy": "grid-search",
        "optimization-strategy-settings": {}
    }

    config_file = Path(project_name) / "optimization.json"
    with open(config_file, "w") as f:
        json.dump(config, f)

    # Run optimization
    result = subprocess.run(
        ["lean", "optimize", project_name],
        capture_output=True,
        text=True
    )

    # Find latest optimization result
    opt_dir = Path(project_name) / "optimizations"
    latest = max(opt_dir.iterdir(), key=lambda p: p.stat().st_mtime)

    # Read results
    with open(latest / "optimization.json") as f:
        results = json.load(f)

    return results
```

**File: `execute_analysis_notebook.py`**
```python
import papermill as pm
import json

def execute_analysis(notebook_path, output_path, parameters):
    """Execute analysis notebook with parameters"""

    # Execute notebook
    pm.execute_notebook(
        notebook_path,
        output_path,
        parameters=parameters,
        kernel_name='python3'
    )

    # Read output notebook to extract results
    with open(output_path) as f:
        nb = json.load(f)

    # Extract results from output cells
    results = extract_results_from_notebook(nb)

    return results

def extract_results_from_notebook(notebook_json):
    """Parse notebook output cells for results"""

    results = {}

    for cell in notebook_json['cells']:
        if cell['cell_type'] == 'code':
            for output in cell.get('outputs', []):
                if output.get('output_type') == 'execute_result':
                    # Extract data from outputs
                    data = output.get('data', {})
                    if 'text/plain' in data:
                        # Parse results
                        pass

    return results
```

#### Phase 3: Update Slash Commands

**/qc-backtest** - Use local execution:
```python
# Instead of API backtest
result = run_local_backtest(project_name)

# Execute analysis notebook
analysis = execute_analysis(
    "analysis.ipynb",
    f"output_{timestamp}.ipynb",
    {"backtest_id": result['backtest_id']}
)

# Make decision based on analysis
decision = make_decision(analysis)
```

**/qc-optimize** - Use local optimization:
```python
# Run local optimization
result = run_local_optimization(project_name, parameters)

# Execute analysis notebook
analysis = execute_analysis(
    "optimization_analysis.ipynb",
    f"output_{timestamp}.ipynb",
    {"optimization_id": result['optimization_id']}
)

# Extract best parameters
best_params = analysis['best_parameters']
```

**/qc-validate** - Monte Carlo walk-forward:
```python
# Execute Monte Carlo notebook with papermill
pm.execute_notebook(
    "monte_carlo_walkforward.ipynb",
    f"walkforward_results_{timestamp}.ipynb",
    parameters={
        "project_name": project_name,
        "monte_carlo_runs": 10,
        "train_test_split": 0.60,
        "parameters": parameter_config
    }
)

# Parse results from output notebook
results = extract_walkforward_results(f"walkforward_results_{timestamp}.ipynb")

# Make robustness decision
decision = apply_robustness_framework(results)
```

#### Phase 4: Test & Validate

1. Test local backtest execution
2. Test local optimization
3. Test notebook execution with Papermill
4. Test result extraction
5. Test end-to-end autonomous workflow

---

## Cost Comparison - CORRECTED ‚ö†Ô∏è

| Method | Subscription | Data Costs | Backtests | Optimizations | Walk-Forward (10 runs) | Total First Year |
|--------|-------------|-----------|-----------|---------------|------------------------|------------------|
| **Cloud API (Free)** | $0 | $0 (included) | FREE (10/day) | $3-5 each | $300-500 | $300-500 |
| **Cloud API (Researcher)** | $60/mo | $0 (included) | Unlimited | $3-5 each | $30-50 | $720-1500 |
| **Local LEAN CLI** | $60/mo | $600-3000+/year | Unlimited local | Unlimited local | Unlimited local | $1,320-4,500+ |
| **Hybrid** | $0-60/mo | $0 (cloud only) | Cloud FREE | Cloud PAID | Cloud PAID | $300-1500 |

### Critical Data Cost Details:

**Local LEAN Requirements:**
1. **PAID TIER MANDATORY**: Minimum $60/month ($720/year) Researcher tier
2. **Security Master Subscription**: $600/year (required for US Equities)
3. **Per-File Data Costs**:
   - Minute equity data: $0.05 per file
   - Daily equity data: $1 per file
   - Universe data: $0.05-$1 per file

**Example Local Cost (100 equities, 1 year, minute resolution):**
- Researcher tier: $720/year
- Security master: $600/year
- Universe data: $126
- Equity data: ~$1,260
- **Total: ~$2,706 first year** (before any optimizations)

**Cloud Backtest Costs:**
- FREE tier: $0 (includes all data, 10 backtests/day)
- Researcher tier: $720/year (includes all data, unlimited backtests)
- Data is INCLUDED in cloud backtests (no additional cost)

### ‚ö†Ô∏è COST ANALYSIS CORRECTION

**My original recommendation of "Local LEAN = $0" was WRONG.**

Local LEAN is actually the MOST EXPENSIVE option due to:
- Required $60/month subscription
- $600/year security master
- Per-file data download costs

**Cloud API is MUCH CHEAPER for most use cases.**

---

## Technical Feasibility Assessment

### Can We Execute Notebooks Programmatically? ‚úÖ YES

**Method**: Papermill + Local LEAN CLI

**Requirements:**
- Docker installed
- LEAN CLI installed
- QuantConnect account (for cloud data access)
- Python environment with papermill

**Proof of Concept:**

```bash
# 1. Start LEAN research environment
lean research "My Project"

# 2. In another terminal, execute notebook
papermill research.ipynb output.ipynb --parameters '{"param": "value"}'

# 3. Parse results
python extract_results.py output.ipynb
```

### Can We Automate Monte Carlo Walk-Forward? ‚úÖ YES

**Method**: Local LEAN + Papermill

```python
# Monte Carlo notebook that uses LEAN CLI internally
for run in range(monte_carlo_runs):
    # Generate random split
    train_start, train_end, test_start, test_end = random_split()

    # Run optimization on training period (local, FREE)
    opt_result = subprocess.run([
        "lean", "optimize", project_name,
        "--start", train_start,
        "--end", train_end
    ])

    # Run backtest on testing period (local, FREE)
    test_result = subprocess.run([
        "lean", "backtest", project_name,
        "--start", test_start,
        "--end", test_end,
        "--parameters", best_params
    ])

    # Analyze degradation
    results.append(calculate_degradation(opt_result, test_result))

# Generate report
generate_walkforward_report(results)
```

**Feasibility**: ‚úÖ 100% feasible, completely free

---

## Critical Error in Current Implementation

### The Monte Carlo Notebook is WRONG ‚ùå

**File**: `monte_carlo_walkforward_REAL.ipynb`

**Problem**: Uses non-existent methods:
- `qb.Optimize()` - Does NOT exist
- `qb.Backtest()` - Does NOT exist

**What Actually Works**:
- `api.read_backtest(project_id, backtest_id)` - Reads existing results
- `api.read_optimization(optimization_id)` - Reads existing results

**The notebook cannot CREATE backtests or optimizations**, it can only READ them!

### Correct Implementation

**Option 1**: Use API to create jobs, notebook to analyze:
```python
# In notebook: Create backtest via API
backtest = api.create_backtest(project_id, compile_id, name)

# Wait for completion
while not backtest.completed:
    time.sleep(5)
    backtest = api.read_backtest(project_id, backtest.backtest_id)

# Analyze results
results = analyze_backtest(backtest)
```

**Option 2**: Use local LEAN CLI:
```python
# In notebook: Execute local command
import subprocess

result = subprocess.run(
    ["lean", "backtest", "../My-Project"],
    capture_output=True
)

# Read local results
with open("../My-Project/backtests/latest/results.json") as f:
    backtest_results = json.load(f)

# Analyze
results = analyze_backtest(backtest_results)
```

---

## Final Recommendation - REVISED ‚ö†Ô∏è

### ‚ùå DO NOT USE Local LEAN CLI (Too Expensive)

**Cost Reality Check:**
- Researcher tier: $720/year
- Security master: $600/year
- Data downloads: $600-2,500/year
- **Total: $1,920-3,820/year**

This is **2-6x MORE EXPENSIVE** than using cloud API!

---

### ‚úÖ RECOMMENDED: Cloud API with Strategic Optimization

**Architecture:**

```
FREE TIER (Test & Validate):
- 10 backtests/day (FREE, includes data)
- Manual optimization (FREE via Research notebooks)
- Manual walk-forward (FREE via Research notebooks)
- Total cost: $0/month

RESEARCHER TIER (Production):
- Unlimited backtests (FREE, includes data)
- API optimization when needed ($3-5 per run)
- Manual walk-forward for deep analysis (FREE)
- Total cost: $60/month + optimization costs
```

**Implementation Steps:**

1. **Immediate** (FREE tier):
   - Use existing API wrapper (already working)
   - 10 backtests/day for hypothesis testing
   - Upload notebooks to Research for optimization/walk-forward
   - Parse results from cloud

2. **If Scaling Needed** (Researcher tier $60/mo):
   - Unlimited cloud backtests
   - Selective use of API optimization ($3-5 when critical)
   - Most optimizations still in Research (FREE)
   - Total: ~$60-100/month

3. **Autonomous with Manual Gates**:
   - Fully automated baseline backtests
   - Manual "Run All" for optimization (FREE)
   - Manual "Run All" for walk-forward (FREE)
   - Automated result parsing and decisions
   - **90% automated, $0-60/month**

### Why Cloud API is Better:

1. **Much Cheaper** - $0-720/year vs $2,000-4,000/year
2. **Data Included** - No per-file charges
3. **Simpler** - No local Docker/CLI setup
4. **Proven** - Already working in current implementation
5. **Scalable** - Upgrade to Researcher tier only if needed

### For True Autonomy WITHOUT Manual Steps:

**Use API Optimization Selectively:**
- Run baseline backtests via API (FREE)
- Only use API optimization for final candidates ($3-5 per run)
- Budget: ~$50-100/month for 10-20 optimizations
- **Still cheaper than local CLI data costs**

### Alternative if Budget Allows:

**Researcher Tier ($60/mo) + Selective API Optimization:**
- Unlimited cloud backtests (data included)
- 10-20 API optimizations/month ($30-100)
- Manual walk-forward in Research (FREE)
- **Total: ~$90-160/month**
- **Still half the cost of local CLI**

---

## Conclusion - CORRECTED

After a full day of research, including **critical data cost discovery**, the path forward is clear:

### Key Findings:

1. **Monte Carlo notebook is BROKEN** - Uses non-existent `qb.Optimize()` and `qb.Backtest()` methods
2. **Local LEAN CLI is EXPENSIVE** - $2,000-4,000/year (NOT free as I initially claimed)
3. **Cloud API is CHEAPER** - $0-720/year + selective optimization costs
4. **Research notebooks can only READ results** - Cannot create backtests/optimizations

### CORRECTED Recommendation:

**Use Cloud API (FREE or Researcher tier) with strategic approach:**

| Approach | Cost/Year | Autonomy Level | Best For |
|----------|-----------|----------------|----------|
| **FREE tier + Manual gates** | $0 | 90% | Testing, exploration |
| **Researcher + Manual gates** | $720 | 90% | Production, unlimited backtests |
| **Researcher + API optimization** | $720-1,500 | 95% | High-volume production |

**Local LEAN CLI** ($2,000-4,000/year) is **NOT recommended** due to high data costs.

### Next Actions:

1. **Fix Monte Carlo notebook** - Remove `qb.Optimize()` calls, use API instead
2. **Adopt FREE tier strategy** - 10 backtests/day + manual optimization
3. **Upgrade to Researcher** - Only if need >10 backtests/day
4. **Use API optimization selectively** - Only for critical final candidates

The **"free and unlimited" local LEAN promise was wrong**. Cloud API is the cost-effective path.

---

## Appendix: Complete Method Reference

### QuantBook Methods (Verified)
- `History()` - Historical data
- `GetFundamental()` - Fundamental data
- `OptionHistory()` - Options data
- `FutureHistory()` - Futures data
- `Indicator()` - Technical indicators
- ‚ùå `Backtest()` - Does NOT exist
- ‚ùå `Optimize()` - Does NOT exist

### API Methods (Verified)
- `api.read_backtest(project_id, backtest_id)` ‚úÖ
- `api.read_optimization(optimization_id)` ‚úÖ
- `api.create_backtest(project_id, compile_id, name)` ‚úÖ
- `api.create_optimization(...)` ‚úÖ (PAID)
- ‚ùå No execute_notebook() method exists

### LEAN CLI Commands (Verified)
- `lean research "Project"` ‚úÖ
- `lean backtest "Project"` ‚úÖ
- `lean optimize "Project"` ‚úÖ
- `lean compile "Project"` ‚úÖ

---

**Report End**
